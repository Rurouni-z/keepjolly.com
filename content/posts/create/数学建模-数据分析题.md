---
title: æ•°å­¦å»ºæ¨¡-æ•°æ®åˆ†æé¢˜
date: 2022-10-16 20:58:19.149
updated: 2022-11-26 14:29:06.289
url: /archives/data-analysis
categories: 
- create
tags: 
- Python
---

ä»…åšå‚è€ƒç”¨ï¼Œä»Šå¹´å»ºæ¨¡èµ›å‰åšå®Œè¿™ä¸ªä»£ç åï¼Œè‡ªè§‰ä¿¡å¿ƒæ»¡æ»¡ï¼Œæ¯”å®Œèµ›åç›´æ¥è£‚å¼€ï¼Œå¸Œæœ›èƒ½æ‹¿ä¸ªå›½ä¸‰å§!o(â•¥ï¹â•¥)oã€‚
11/26 ä¸ºä»€ä¹ˆè¿˜æ²¡å…¬å¸ƒæˆç»©ï¼Œå“­æ­»ï¼Œè²Œä¼¼è¿˜æœ‰å¥½å‡ ä¸ªæ˜ŸæœŸã€‚åç»­ï¼Œè°¢è°¢å‚ä¸å¥–æğŸ˜ğŸ‘ŒğŸ˜­
## æ•°æ®æŸ¥çœ‹
```python
import os
from pandas_profiling import ProfileReport
import pandas as pd
# è¶…æ…¢ å…ˆè¿è¡Œè¿™ä¸ª
os.chdir(r'C:\Users\Desktop\math')
file_name = 'Molecular_Descriptor.xlsx'
sheet_name = 'training'
table = pd.read_excel(file_name, sheet_name, header=[0])  # å¦‚æœæœ‰å¤šä¸ªåˆ—å æ–¹ä¾¿èµ·è§åªå–ä¸€ä¸ª
profile = table.profile_report(title="data_profile")
profile.to_file(output_file="analysis.html")
```
## æ•°æ®é¢„å¤„ç†
```python
# å¯¼åŒ…ç•¥è¿‡ï¼Œè‡ªè¡Œgithubçœ‹
def describeData(data):
    print(data.dtypes)  # å¦‚æœæ˜¯objectéœ€è¦è½¬æ¢
    # for col in data:  # object to numeric if is numeric
    #     if isinstance(data[col][0], int) or isinstance(data[col][0], float):
    #         data[col] = pd.to_numeric(data[col], errors='coerce')
    # print('æ•°æ®ç±»å‹ï¼š', data.dtypes)

    print('å‰ä¸‰è¡Œæ•°æ®ï¼š', data.iloc[:3, :5])  # çœ‹çœ‹æ˜¯å¦å¯¼å…¥æ­£ç¡®
    print('æ ·æœ¬æƒ…å†µ', data.describe())  # æŸ¥çœ‹æ ·æœ¬åˆ†å¸ƒ
    sns.displot(data['åœŸå£¤è’¸å‘é‡(mm)'], kde=True)  # ç›´æ–¹å›¾æŠ˜çº¿å›¾å¯è§†  !! æ³¨æ„ä¿®æ”¹æˆæŸä¸ªåˆ—å
    plt.savefig('picture/describe.jpg')

    data = pd.concat([data['10cmæ¹¿åº¦(kg/m2)'], data['åœŸå£¤è’¸å‘é‡(mm)']], axis=1)  # 1         !! æ³¨æ„ä¿®æ”¹æˆæŸä¸ªåˆ—å
    data.plot.scatter(x='10cmæ¹¿åº¦(kg/m2)', y='åœŸå£¤è’¸å‘é‡(mm)', ylim=(0, 1666), c='c', cmap='coolwarm')
    # data = pd.concat([data['ALogp2'], data['AMR']], axis=1)  # 1         !! å¯é€‰ç¬¬äºŒç»„å¯¹æ¯” çœ‹å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ çº¿æ€§éçº¿æ€§
    # data.plot.scatter(x='ALogp2', y='AMR', ylim=(0, 1666), c='c', cmap='coolwarm')
    plt.show()

def processNull(data):
    # https://blog.51cto.com/liguodong/3702149
    # 1. è¾“å‡ºç¼ºå¤±ç‡è¡¨æ ¼ å»ºè®®ç»“æœæ”¾åˆ°excelï¼Œå›¾å¥½çœ‹
    missing = data.isnull().sum().reset_index().rename(columns={0: 'missNum'})[1:]
    missing['missRate'] = missing['missNum'] / data.shape[0]  # è®¡ç®—ç¼ºå¤±æ¯”ä¾‹
    miss_analogy = missing.sort_values(by='missRate', ascending=False)  # å‡åº
    miss_analogy.index = range(1, len(miss_analogy) + 1)  # æ’åºåé‡æ–°ä¿®æ”¹index
    print('å‰å…«å˜é‡çš„ç¼ºå¤±ç‡', miss_analogy[:5])  # è¾“å‡ºå‰8ä¸ª            ï¼ï¼ è§£é™¤æ³¨é‡Š
    # 2. è¾“å‡ºç¼ºå¤±ç‡å›¾ å–å‰8ä¸ªéå†
    plt.figure()
    plt.bar(np.arange(5), list(miss_analogy['missRate'].values)[:5],
            color=['red', 'steelblue', 'yellow'])
    plt.title('å˜é‡ç¼ºå¤±ç‡ç›´æ–¹å›¾')
    plt.xlabel('å˜é‡å')
    plt.ylabel('ç¼ºå¤±ç‡')
    plt.xticks(np.arange(5), list(miss_analogy['index'][:5]))
    # plt.xticks(rotation=90)
    for x, y in enumerate(list(miss_analogy['missRate'].values[:5])):
        plt.text(x, y + 0.02, '{:.2%}'.format(y), ha='center')  #å›¾ç‰‡åŠ text
        plt.ylim([0, 1])

    # 3. å¤„ç†ç¼ºå¤±å€¼  åˆ é™¤ç¼ºå¤±é‡å¤§äºé˜ˆå€¼0.8
    orig_col = data.columns  # è®¾è®¡åˆ é™¤åˆ—çš„æ“ä½œæ—¶å¯ä»¥å‘ç°åˆ é™¤äº†ä»€ä¹ˆåˆ—
    del_col = []
    data = data.dropna(axis=1, how='any', thresh=data.shape[0] * 0.8)  # åˆ é™¤åˆ—            ï¼ï¼ è§£é™¤æ³¨é‡Š
    # data = data.dropna(axis=0, how='any', thresh=data.shape[1]*0.8)  # åˆ é™¤è¡Œ
    data.reset_index(drop=True, inplace=True)
    after_col = data.columns
    del_col.append(list(set(orig_col).difference(set(after_col))))  # https://cloud.tencent.com/developer/article/1705131
    print('åˆ é™¤ç¼ºå¤±é‡å¤§äºé˜ˆå€¼0.8çš„å˜é‡ï¼š', del_col)
    plt.savefig('picture/nullV.jpg')
    plt.show()
    return data

def interpolateData(data):  # å¡«å……ç¼ºå¤±å€¼
    fig, axes = plt.subplots(figsize=(8, 4), sharex='all')
    axes.plot(data['ç§¯é›ªæ·±åº¦(mm)'], label='Original Data', marker='*', markerfacecolor='blue')
    # 1 ç›´æ¥å¡«å……
    """
     å‡å€¼é€‚ç”¨äºå®šé‡æ•°æ® èº«é«˜ å¹´é¾„ mean()
     ä¸­ä½æ•° æ­£æ€åˆ†å¸ƒ median()
     ä¼—æ•°é€‚ç”¨äºå®šæ€§æ•°æ® æ€§åˆ« æ–‡åŒ–ç¨‹åº¦ data['S-ZORB.CAL_H2.PV'].mode()[0]
     method='pad/bfill' å–å‰/åæ•°æ®å¡«å……
    """
    # data.fillna({'S-ZORB.CAL_H2.PV': data['S-ZORB.CAL_H2.PV'].mean()}, inplace=True)  # åªä¿®æ”¹ä¸€åˆ—
    # data.fillna(data.mean(), inplace=True)  #           ï¼ï¼ é€‰ ç›´æ¥å¡«å…… è§£é™¤æ³¨é‡Š
    # 2 æ’å€¼æ³•
    """
       â€˜nearestâ€™, â€˜zeroâ€™, â€˜slinearâ€™, â€˜quadraticâ€™, â€˜cubicâ€™
     1.å¦‚æœä½ çš„æ•°æ®å¢é•¿é€Ÿç‡è¶Šæ¥è¶Šå¿«ï¼Œå¯ä»¥é€‰æ‹© method='quadratic'äºŒæ¬¡æ’å€¼ã€‚
     2.å¦‚æœæ•°æ®é›†å‘ˆç°å‡ºç´¯è®¡åˆ†å¸ƒçš„æ ·å­ï¼Œæ¨èé€‰æ‹© method='pchip'ã€‚
     3.å¦‚æœéœ€è¦å¡«è¡¥ç¼ºçœå€¼ï¼Œä»¥å¹³æ»‘ç»˜å›¾ä¸ºç›®æ ‡ï¼Œæ¨èé€‰æ‹© method='akima'ã€‚
    """
    data.interpolate(method='quadratic', inplace=True)  # ................ï¼ï¼ è§£é™¤æ³¨é‡Š
    axes.plot(data['ç§¯é›ªæ·±åº¦(mm)'], 'r--', label='Filled Data', marker='h', markerfacecolor='red')
    axes.legend(['åˆå§‹å€¼', 'æ‹Ÿåˆå€¼'], loc="upper right")
    plt.show()  # ........................ï¼ï¼ è§£é™¤æ³¨é‡Š
    return data

def processZero(data):  # åˆ é™¤0å€¼å¤§äº80%çš„åˆ—/è¡Œ  Bijlsma æå‡ºçš„ 80%å‡†åˆ™
    zeros = []
    for c in data:
        flat = data[c].to_numpy()
        cnt = np.where(flat, 0, True)
        if np.sum(cnt) > 0.2 * data.shape[0]:  # è·å–0å€¼è¿‡å¤šçš„   åˆ—
            zeros.append(c)
    print('zeros error({}): '.format(len(zeros)), zeros)
    error = data[zeros[0]][data[zeros[0]] == 0]
    data_c = data[zeros[0]][data[zeros[0]] != 0]
    fig, ax2 = plt.subplots(figsize=(15, 9))

    plt.scatter(data_c.index, data_c.values, color='g', alpha=0.6, label='æ­£å¸¸å€¼')
    plt.scatter(error.index, error.values, color='r', alpha=0.8, label='0å€¼')
    ax2.set_xlabel('ä¸‹æ ‡')
    ax2.set_ylabel('å€¼')
    ax2.legend()
    plt.show()
    data.drop(columns=zeros, inplace=True)
    plt.savefig('picture/zeroV.jpg')
    return data

def process3sigma(data):  # åˆ é™¤å¼‚å¸¸å€¼ 3sigmaæ³•
    """
    éœ€æ»¡è¶³é«˜æ–¯åˆ†å¸ƒï¼Œå¯å‡è®¾ä¸ºé«˜æ–¯åˆ†å¸ƒå¼ºè¡Œç”¨
    1. å¯ä»¥åˆ é™¤æ¯åˆ—å¼‚å¸¸å€¼å¤§äºé˜ˆå€¼å¹¶ä¸”è¶…è¿‡3sigmaèŒƒå›´ï¼Œå¯¹å°‘äºé˜ˆå€¼ä½†è¶…è¿‡èŒƒå›´çš„è¿›è¡Œèµ‹å€¼ æ²¡å®ç°
    2. å¯ä»¥ç›´æ¥åˆ é™¤è¶…è¿‡3sigmaèŒƒå›´
    """
    sigma, sigma_cnt = [], [0] * data.shape[0]
    delrow_thres = 1  # è¡Œå¼‚å¸¸å€¼é˜ˆå€¼
    delcol_thres = 100  # åˆ—å¼‚å¸¸å€¼é˜ˆå€¼
    idx = []
    sig = 0
    for c in data:
        flat = data[c].to_numpy()
        try:
            mean = np.mean(flat)
            s = np.std(flat, ddof=1)
        except TypeError:
            continue
        flag = 0
        for r in range(data.shape[0]):  # æ£€æŸ¥å½“å‰åˆ—çš„3sigma
            if abs(flat[r] - mean) > s * 3:
                sigma_cnt[r] += 1
                flag += 1
            else:
                idx.append(r)
        if flag > delrow_thres:  #
            sig = 3 * s
            sigma.append(c)
    # print('del 3sigma({0}) column({1}): '.format(round(sig, 3), len(sigma)), sigma)
    # if len(sigma) > 0:
    #     draw_3sigma(data[sigma[0]])
    # draw_3sigma(data['å¹²é‡'])
    # data.drop(columns=sigma, inplace=True)
    # data.reset_index(drop=True)
    # åˆ é™¤è¡Œ
    sigma_cntnp = np.array(sigma_cnt)
    where = np.where(sigma_cntnp > 0)
    a = np.array(list(where))
    a = a[0]  # necessaryï¼Ÿ
    print('del 3sigma row: ', len(a))
    data.drop(index=a, inplace=True)
    data.reset_index(drop=True, inplace=True)
    return data, idx

def processMaxMin(data):
    scope = pd.read_excel('é™„ä»¶å››ï¼š354ä¸ªæ“ä½œå˜é‡ä¿¡æ¯.xlsx', usecols=[1, 3])  # æ³¨æ„ä¿®æ”¹
    scope = scope.to_numpy()
    scope = {n[0]: n[1].split('-') for n in scope}
    for k, v in scope.items():
        mm = []
        flag = 1
        for value in v:
            if value == '' or value == 'ï¼ˆ' or value == '(':
                flag = 0
                continue
            try:
                mm.append(float(value) if flag else -float(value))
            except ValueError:
                value = re.findall(r'\d+\.?\d*', value)[0]  # æ‰¾æµ®ç‚¹æ•°
                mm.append(float(value) if flag else -float(value))
            flag = 1
        if mm[0] > mm[1]:
            print('æ•°æ®error')
        scope[k] = mm
    for col in scope.keys():
        for i in data[col].index:
            if scope[col][0] > data[col][i] or data[col][i] > scope[col][1]:  # åˆ é™¤æœ€å¤§æœ€å°ä¸å¯¹çš„ è¡Œ/æ ·æœ¬
                print('minmax error', i, data[col][i], scope[col], col)
                data.drop(index=i, inplace=True)
    data.reset_index(drop=True, inplace=True)
    return data

if __name__ == '__main__':
    # 1. è¯»å–æ•°æ®
    # file_name = r'C:\Users\Desktop\2022å¹´Eé¢˜\æ•°æ®é›†\ç›‘æµ‹ç‚¹æ•°æ®\é™„ä»¶15ï¼šè‰åŸè½®ç‰§æ”¾ç‰§æ ·åœ°ç¾¤è½ç»“æ„ç›‘æµ‹æ•°æ®é›†ï¼ˆ2016å¹´6æœˆ-2020å¹´9' \
    #             r'æœˆï¼‰ã€‚/å†…è’™å¤è‡ªæ²»åŒºé”¡æ—éƒ­å‹’ç›Ÿå…¸å‹è‰åŸè½®ç‰§æ”¾ç‰§æ ·åœ°ç¾¤è½ç»“æ„ç›‘æµ‹æ•°æ®é›†ï¼ˆ201.xlsx '
    file_name = 'data/result.xlsx'
    sheet_name = 'Sheet1'  # æ³¨æ„ä¿®æ”¹
    table = pd.read_excel(file_name, sheet_name, header=[0])  # å¦‚æœæœ‰å¤šä¸ªåˆ—å æ–¹ä¾¿èµ·è§åªå–ä¸€ä¸ª
	# 2. åˆ’åˆ†æ•°æ® if need
    # æ³¨æ„ç´¢å¼•è¿˜æ˜¯åŸæ•°æ®çš„ç´¢å¼• https://stackoverflow.com/questions/71679582/0-is-not-in-range-in-pandas
    sample285 = table[1:41]
    sample285.reset_index(drop=True, inplace=True)
    sample285 = sample285.copy()  # é˜²æ­¢SettingWithCopyWarning
    sample310 = table[42:]
    sample310.reset_index(drop=True, inplace=True)
    data = table.iloc[:, 2:]  # æ’é™¤å¹´æœˆ
    # 3. æŸ¥çœ‹æ•°æ®æƒ…å†µ
    # describeData(data)
    # 4. å¤„ç†ç¼ºå¤±å€¼
    data = processNull(data)
    data = processZero(data)
    data, idx = process3sigma(data)
    # table = table.iloc[idx, 0]
    # data = processMaxMin(data)
    data = interpolateData(data)
    # print('åˆ é™¤å‰å˜é‡ä¸ªæ•°', len(table.columns))
    # data.index = table.iloc[:, 0]  # å°†stringåˆ—é‡æ–°æ”¾å›
    # print('åˆ é™¤åå˜é‡ä¸ªæ•°', len(data.columns))
    data.to_excel('Preprocess/pre_data.xlsx')
```
## ç‰¹å¾é€‰æ‹©
```python
def low_var_filter(data, names):  # ä½æ–¹å·®æ»¤æ³¢
    # äººå·¥ç‰ˆ
    # var = data.var()
    # col = var.index
    # variable = []
    # for i in range(len(var)):
    #     if var[col[i]] < 1:
    #         variable.append(col[[i]].format()[0])
    # print(list(variable), var[variable[0]])
    # data.drop(columns=variable, axis=1, inplace=True)

    data = data[:, 1:]  # æ’é™¤timeåˆ—
    data = pd.DataFrame(data, columns=names[1:])
    # æ™ºèƒ½ç‰ˆ
    orig_col = data.columns
    selector = VarianceThreshold(threshold=1)  # é˜ˆå€¼ä¸º<1
    selector.fit(data)
    after_col = np.array(data.columns.format())[selector.get_support()]  # è·å¾—åˆ é™¤ååˆ—
    del_col = list(set(orig_col).difference(set(after_col)))  # è·å¾—åˆ é™¤åˆ—
    data = selector.fit_transform(data)
    print('ä½æ–¹å·®æ»¤æ³¢åˆ é™¤åˆ—ï¼š', del_col)
    print('ä½æ–¹å·®åˆ é™¤åçš„çŸ©é˜µshapeï¼š', data.shape)
    # data = pd.DataFrame(data, columns=after_col)
    # print(data[:5])
    return data, after_col
    # data.to_excel('new_data.xlsx')


def MICSelect(data, target, feature_name, k):
    def mic(x, y):
        m = MINE()
        m.compute_score(x, y)
        return m.mic(), 0.5

    # n = data.shape[1]  # ä¸¤ä¸¤æ¯”è¾ƒ https://zhuanlan.zhihu.com/p/53092905
    # result = np.zeros([n, n])
    # mine = MINE(alpha=0.6, c=15)
    # for i in range(n):
    #     mine.compute_score(data[:, i], target)
    #     result[i, 0] = round(mine.mic(), 2)
    #     result[0, i] = round(mine.mic(), 2)
    # mic = pd.DataFrame(result)
    SKB = SelectKBest(lambda X, Y: tuple(map(tuple, np.array(list(map(lambda x: mic(x, Y), X.T))).T)),
                      k=k)  # é€‰æ‹©å‰kä¸ªæœ€å¥½æ¯”éœ€è¦çš„å¤š20ä¸ª https://www.cnblogs.com/nxf-rabbit75/p/11122415.html#auto-id-15
    SKB.fit_transform(data, target)
    feature_index = SKB.get_support(True)
    mic_scores = SKB.scores_
    mic_results = [(feature_name[i], mic_scores[i]) for i in feature_index]
    sorted_data = sorted(mic_results, key=itemgetter(1), reverse=True)
    pd_data = pd.DataFrame(sorted_data, columns=['å˜é‡å', 'é‡è¦æ€§åº¦'])
    print('MICDataframe: ', pd_data.iloc[:5])
    pd_data.to_excel('FeatureSelect/MICData.xlsx')
    return pd_data


def dcorSelect(data, target, feature_name, k):
    def Dcor(x, y):
        return dcor.distance_correlation(x, y), 0.5

    SKB = SelectKBest(lambda X, Y: tuple(map(tuple, np.array(list(map(lambda x: Dcor(x, Y), X.T))).T)),
                      k=k)  # å‰kä¸ª
    SKB.fit_transform(data, target)
    feature_index = SKB.get_support(True)
    mic_scores = SKB.scores_

    mic_results = [(feature_name[i], mic_scores[i]) for i in feature_index]
    sorted_data = sorted(mic_results, key=itemgetter(1), reverse=True)
    pd_data = pd.DataFrame(sorted_data, columns=['å˜é‡å', 'é‡è¦æ€§åº¦'])
    print('DcorDataframe: ')
    print(pd_data.iloc[:5])
    pd_data.to_excel('FeatureSelect/DcorData.xlsx')
    return pd_data


def LassoSelect(data, target, feature_name, k):
    """
    å­˜åœ¨ä¸€ç»„é«˜åº¦ç›¸å…³çš„ç‰¹å¾æ—¶ï¼ŒLassoå›å½’æ–¹æ³•å€¾å‘äºé€‰æ‹©å…¶ä¸­çš„ä¸€ä¸ªç‰¹å¾
    å…·æœ‰é«˜ç»å¯¹å€¼çš„æ•°æœ€é‡è¦
    https://blog.csdn.net/Kyrie_Irving/article/details/101197360
    https://blog.51cto.com/u_14467853/5438127
    http://scikit-learn.org.cn/view/199.html
    https://ask.hellobi.com/blog/lsxxx2011/10581
    """
    data = pd.DataFrame(data, columns=feature_name)
    alpha_lasso = 10 ** np.linspace(-3, 3, 100)

    # ä½¿ç”¨lassoCVæ‰¾å‡ºæœ€ä½³lambdaå€¼
    model = make_pipeline(StandardScaler(with_mean=False), LassoCV(alphas=alpha_lasso, cv=10, max_iter=10000))
    model.fit(data, target)
    lasso_best_alpha = model['lassocv'].alpha_  # å–å‡ºæœ€ä½³çš„lambdaå€¼
    print('lassoå›å½’æœ€ä½³alphaå€¼', lasso_best_alpha)

    # æ ¹æ®ä¸åŒçš„lambdaç”»å‡ºå˜é‡æƒ…å†µ å¯ä»¥é¦–å…ˆå¯»æ‰¾æœ€ä¼˜å˜é‡ æ”¾è¯¥å›¾ ç„¶åæ”¾ä¸‹é¢çš„æœ€é‡è¦å˜é‡å›¾
    # lasso = Lasso()
    # coefs_lasso = []
    # for i in alpha_lasso:
    #     lasso.set_params(alpha=i)
    #     lasso.fit(data, target)
    #     coefs_lasso.append(lasso.coef_)
    #
    # drawPlot(alpha_lasso, coefs_lasso, title='Lassoå›å½’ç³»æ•°å’Œalphaç³»æ•°çš„å…³ç³»', xlabel='Î±å€¼', ylabel='å„å˜é‡æ¯”ä¾‹ç³»æ•°',
    #          columns=feature_name)

    # ç›´æ¥ä»£å…¥æœ€ä½³å€¼
    lasso = Lasso(alpha=lasso_best_alpha)
    model_lasso = lasso.fit(data, target)
    coef = pd.Series(model_lasso.coef_, index=data.columns)
    # print(coef[coef != 0].abs().sort_values(ascending=False)[:10])
    print("Lasso picked " + str(sum(coef != 0)) + " variables and eliminated the other " + str(
        sum(coef == 0)) + " variables")
    
    sorted_data = sorted(zip(feature_name, coef.values), reverse=True, key=itemgetter(1))[:k]
    pd_data = pd.DataFrame(sorted_data, columns=['å˜é‡å', 'é‡è¦æ€§åº¦'])
    print('LassoDataframe: ')
    print(pd_data.iloc[:5])
    pd_data.to_excel('FeatureSelect/LassoData.xlsx')
    return pd_data


# useless L2æ­£åˆ™åŒ–ï¼ˆå²­å›å½’ï¼‰å¯ä»¥ç”¨æ¥åšç‰¹å¾é€‰æ‹©å—ï¼Ÿ
# https://www.zhihu.com/question/288362034/answer/463287541
def RidgeSelect(data, target, feature_name):
    data = data[:, 1:]  # æ’é™¤timeåˆ—
    data = pd.DataFrame(data, columns=feature_name[1:])
    alpha_ridge = 10 ** np.linspace(1, 10, 100)

    # æ ¹æ®ä¸åŒçš„lambdaç”»å‡ºå˜é‡æƒ…å†µ å¯ä»¥é¦–å…ˆå¯»æ‰¾æœ€ä¼˜å˜é‡ æ”¾è¯¥å›¾ ç„¶åæ”¾ä¸‹é¢çš„æœ€é‡è¦å˜é‡å›¾
    ridge = Ridge()
    coefs_ridge = []
    for i in alpha_ridge:
        ridge.set_params(alpha=i)
        ridge.fit(data, target)
        coefs_ridge.append(ridge.coef_)
    # https://stackoverflow.com/questions/58393378/why-does-ridge-model-fitting-show-warning-when-power-of-the-denominator-in-the-a
    drawPlot(alpha_ridge, coefs_ridge, title='Ridgeå›å½’ç³»æ•°å’Œalphaç³»æ•°çš„å…³ç³»', xlabel='Î±å€¼', ylabel='å„å˜é‡æ¯”ä¾‹ç³»æ•°',
             columns=feature_name)

    # ä½¿ç”¨lassoCVæ‰¾å‡ºæœ€ä½³lambdaå€¼
    # æ ·æœ¬æ•°æ¯”ç‰¹å¾æ•°å°‘ä¼šæŠ¥Singular matrix in solving dual problem. Using least-squares solution instead.
    model = make_pipeline(StandardScaler(with_mean=False),
                          RidgeCV(alphas=alpha_ridge, cv=10, scoring='neg_mean_squared_error'))
    model.fit(data, target)
    ridge_best_alpha = model['ridgecv'].alpha_  # å–å‡ºæœ€ä½³çš„lambdaå€¼
    print('ridgeå›å½’æœ€ä½³alphaå€¼', ridge_best_alpha)

    # ç›´æ¥ä»£å…¥æœ€ä½³å€¼
    ridge = Ridge(alpha=ridge_best_alpha)
    model_ridge = ridge.fit(data, target)
    coef = pd.Series(model_ridge.coef_, index=data.columns)
    # print(coef[coef != 0].abs().sort_values(ascending=False)[:10])
    print("Ridge picked " + str(sum(coef != 0)) + " variables and eliminated the other " + str(
        sum(coef == 0)) + " variables")
    a = pd.DataFrame()
    a['feature'] = feature_name[:]  # feature_name[:45]ä½¿ç›´æ–¹å›¾å¯ä»¥æœ‰è´Ÿå€¼
    a['importance'] = coef.values  # coef.values[:45]ä½¿ç›´æ–¹å›¾å¯ä»¥æœ‰è´Ÿå€¼

    a = a.sort_values('importance', ascending=False)
    a = a[:40]  # åªæ˜¾ç¤ºå‰40ä¸ªé‡è¦å˜é‡ æˆ–è€…æ³¨é‡Šæ‰
    drawBar(a, typ='barh', title='Ridgeæ¨¡å‹ç­›é€‰åé‡è¦å˜é‡')  # å–å‰40ä¸ªå˜é‡ title='Lassoæ¨¡å‹å…³è”åº¦æƒ…å†µ'
    return a


def RFSelect(data, target, feature_name, k):
    """
    https://www.cnblogs.com/Ann21/p/11722339.html
    :param data:
    :param target:
    :param feature_name:
    :return:
    """
    # py = Pinyin()  # é˜²æ­¢lgbmæŠ¥é”™  ä»¥ä¸‹ä¸‰è¡Œä»…åšè®°å½•ç”¨ æ— å…³RF
    # data = data.rename(columns=lambda x: py.get_pinyin(x))
    # data = data.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))

    # rf = RandomForestRegressor(n_estimators=20, max_depth=4, n_jobs=7)  # ç”¨7ä¸ªæ ¸æ¥è·‘ åŠ é€Ÿ
    # scores = []
    # for i in range(data.shape[1]):  # å•å˜é‡é€‰æ‹© å¹³å‡ç²¾ç¡®ç‡å‡å°‘ è®¡ç®—å¾ˆæ…¢ å¤ªæ…¢äº† æ¢å°ç”µè„‘ä¸€èµ·è·‘
    #     score = cross_val_score(rf, data[:, i:i + 1], target, scoring="r2",
    #                             cv=ShuffleSplit(len(data)))
    #     scores.append((np.round(np.mean(score), 3), feature_name[i]))
    #
    # keep_fea = sorted(scores, reverse=True)[:40]
    # print(keep_fea)
    # drawBar(keep_fea, typ='bar')

    rf = RandomForestRegressor(n_estimators=100, n_jobs=7, max_depth=4)
    rf.fit(data, target)
    sorted_data = sorted(zip(feature_name, map(lambda x: round(x, 4), rf.feature_importances_)), reverse=True,
                         key=itemgetter(1))[:k]
    pd_data = pd.DataFrame(sorted_data, columns=['å˜é‡å', 'é‡è¦æ€§åº¦'])
    print('RFDataframe: ')
    print(pd_data.iloc[:5])
    pd_data.to_excel('FeatureSelect/RFData.xlsx')
    return pd_data


def RFESelect(data, target, names, k):
    # https://cloud.tencent.com/developer/article/1081618
    # https://blog.csdn.net/LuohenYJ/article/details/107239001
    # https://machinelearningmastery.com/rfe-feature-selection-in-python/
    # https://www.scikit-yb.org/en/latest/api/model_selection/rfecv.html å¯è§†åŒ– æ²¡ç”¨åˆ°
    def rank_to_dict(ranks, names, order=1):
        minmax = MinMaxScaler()
        ranks = minmax.fit_transform(order * np.array([ranks]).T).T[0]
        ranks = map(lambda x: round(x, 2), ranks)
        return zip(names, ranks)

    # model = SVC(kernel='linear') å¥½åƒç”¨ä¸äº†
    # model = Ridge(alpha=100000, fit_intercept=True, copy_X=True, max_iter=1500, tol=1e-4, solver='auto')
    # model = LinearRegression()  # Lasso(max_iter=15000, alpha=100, scoring='r2')
    model = DecisionTreeRegressor()  # æ•ˆæœæ„å¤–çš„å¥½
    # model = Lasso(max_iter=15000, alpha=0.001)

    # do a regress task, use the metric R-squared (coefficient of determination)
    # accuracy score is used for classification problems.
    # https://stackoverflow.com/questions/32664717/got-continuous-is-not-supported-error-in-randomforestregressor
    # min_features_to_select æœ€å°‘ä¿ç•™ç‰¹å¾æ•°
    rfe = RFECV(estimator=model, step=1, cv=5, min_features_to_select=1)
    rfe.fit_transform(data, target)
    ranks = rank_to_dict(rfe.ranking_, names, order=-1)
    sorted_data = sorted(ranks, reverse=True, key=itemgetter(1))[:k]
    pd_data = pd.DataFrame(sorted_data, columns=['å˜é‡å', 'é‡è¦æ€§åº¦'])
    print('RFEDataframe: ')
    print(pd_data.iloc[:5])
    pd_data.to_excel('FeatureSelect/RFEData.xlsx')
    return pd_data


def PCAReduction(X, names, k):
    """
    https://stackoverflow.com/questions/50796024/feature-variable-importance-after-a-pca-analysis
    https://cloud.tencent.com/developer/article/1794827
    """
	scaler = StandardScaler()
    scaler.fit(X)
    X = scaler.transform(X)
    # pca = PCA(n_components='mle', svd_solver='full')  # pca guess the dimension
    pca = PCA(n_components=3)  # !                     çœ‹æƒ…å†µä¿®æ”¹
    x_new = pca.fit_transform(X)

    n_pcs = pca.components_.shape[0]
    most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]
    most_important_names = [names[most_important[i]] for i in range(n_pcs)]
    # ç”»å›¾
    # drawBiplot(x_new[:, :], np.transpose(pca.components_[:, most_important]), y, labels=most_important_names)
    dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}
    df = pd.DataFrame(dic.items())
    df['evr'] = [pca.explained_variance_ratio_[i] for i in range(n_pcs)]
    df.columns = ['ä¸»æˆåˆ†', 'è¯¥ä¸»æˆåˆ†ä¸‹æœ€é‡è¦çš„å˜é‡', 'ä¸»æˆåˆ†è§£é‡Šç‡']
    # æ¯ä¸ªä¸»æˆåˆ†æœ€ä¼˜å˜é‡å’Œè¯¥ä¸»æˆåˆ†çš„ä»·å€¼
    print(df)

    # è¾“å‡ºæ¯ä¸ªä¸»æˆåˆ†æŒ‰kæ¯”ä¾‹ä¸ªå˜é‡
    n_pcs_best = 2  # éœ€è¦æ ¹æ®dfæ¥åˆ¤æ–­
    sel = [int(k * 3 / 4), k - int(k * 3 / 4)]
    K_important = []
    for i in range(n_pcs_best):  # å–ç¬¬iä¸ªä¸»æˆåˆ†çš„æ’åºåcomponentçš„ä¸‹æ ‡
        comp = np.abs(pca.components_[i]).argsort()[::-1][:sel[i]]
        K_important.append(comp)

    K_important_names = []
    for i in range(n_pcs_best):  # æœ€å¥½çš„å‡ ä¸ªä¸»æˆåˆ†
        temp = []
        component = pca.components_[i]
        for j in K_important[i]:  # æ¯ä¸ªä¸»æˆåˆ†æŒ‰è´¡çŒ®ç‡å–å‰jä¸ªå€¼
            temp.append((names[j], np.abs(component[j])))
        K_important_names.extend(temp)
    print('å‰2ä¸ªä¸»æˆåˆ†ä¹‹å‰Kä¸ªå˜é‡é‡è¦æ€§', K_important_names)

    pd_data = pd.DataFrame(K_important_names, columns=['å˜é‡å', 'é‡è¦æ€§åº¦'])
    print('PCADataframe: ')
    print(pd_data.iloc[:5])
    pd_data.to_excel('FeatureSelect/PCAData.xlsx')  # è¦np.absæ‰ ä¿å­˜å—
    return pd_data


def voteFeature(k):
    def voteSum(data, new_data, k):
        top = k  # é€‰å®šçš„å˜é‡æ•°k
        for i in data['å˜é‡å']:
            new_data[i] += top
            top -= 1
            if top == 0:
                top = k
        return new_data

    file_dic = 'FeatureSelect/'
    MIC_list = pd.read_excel(file_dic + 'MICData.xlsx', index_col=[0])
    dcor_list = pd.read_excel(file_dic + 'DcorData.xlsx', index_col=[0])
    lasso_list = pd.read_excel(file_dic + 'LassoData.xlsx', index_col=[0])
    RF_list = pd.read_excel(file_dic + 'RFData.xlsx', index_col=[0])
    RFE_list = pd.read_excel(file_dic + 'RFEData.xlsx', index_col=[0])
    pca_list = pd.read_excel(file_dic + 'PCAData.xlsx', index_col=[0])
    all_list = pd.concat([MIC_list, dcor_list, lasso_list, RF_list, RFE_list, pca_list], axis=0)
    all_list.to_excel(file_dic + 'all.xlsx')

    new_list = defaultdict(int)
    new_list = voteSum(all_list, new_list, k)
    sorted_dic = dict(sorted(new_list.items(), key=lambda item: item[1], reverse=True))
    sorted_list = [i for i in sorted_dic.keys()]
    print('æœ€ç»ˆçš„å‰{}ä¸ªå˜é‡ï¼š'.format(k), sorted_list[:k])
    print(sorted_dic)
    return sorted_list[:k]


def corrSelect(data, target, names, k):
    """
    https://www.cnblogs.com/always-fight/p/10209213.html
    çš®å°”é€Šç³»æ•°åªèƒ½è¡¡é‡çº¿æ€§ç›¸å…³æ€§ï¼Œå…ˆè¦è®¡ç®—å„ä¸ªç‰¹å¾å¯¹ç›®æ ‡å€¼çš„ç›¸å…³ç³»æ•°ä»¥åŠç›¸å…³ç³»æ•°çš„På€¼ã€‚
    """
    df = pd.DataFrame(data, columns=names)
    c = cal_c(df, method, n_clusters=5, threshold=0.7)  # åœ¨utilsæ–‡ä»¶ä¸­
    c.corr_heat_map()
    del_col = c.drop_hight_corr()
    get_col = list(set(names).difference(set(del_col)))
    return get_col
	# æ­¤å¤„æ‰‹åŠ¨ç‰ˆè·å¾—på€¼ å¹¶æ ¹æ®é˜ˆå€¼0.8 0.001ç­›é€‰ç‰¹å¾ æ²¡ç”¨åˆ°
    # sav = []
    # for i in range(data.shape[1]):  # éå†ç‰¹å¾
    #     temp = []
    #     for j in range(i, data.shape[1]):
    #         if j == i:
    #             continue
    #         ret = pearsonr(data[:, i], data[:, j])
    #         if abs(ret[0]) < 0.8 and ret[1] < 0.001:
    #             temp.append(j)
    #     if len(temp) > int(data.shape[1] * 0.5):
    #         sav.append(i)
    #     # results.append(' ')
    # p_result = list(set(sav))
    # print(p_result, len(p_result))
    # return p_result

    # def multivariate_pearsonr(X, y):
    #     scores, p_values = [], []
    #     for ret in map(lambda x: pearsonr(x, y), X.T):
    #         if abs(ret[0]) <= 0.6:
    #             scores.append(abs(ret[0]))
    #             p_values.append(ret[1])
    #         else:
    #             scores.append(0)
    #     return np.array(scores), 0
    #
    # def multivariate_spearmanr(X, y):
    #     scores, p_values = [], []
    #     for ret in map(lambda x: spearmanr(x, y), X.T):
    #         if abs(ret[0]) <= 0.6:
    #             scores.append(abs(ret[0]))
    #             p_values.append(ret[1])
    #         else:
    #             scores.append(0)
    #     return np.array(scores), 0
    #
    # transformer = SelectKBest(score_func=multivariate_pearsonr, k=k)
    # transformer.fit_transform(data, target)
    # feature_index = transformer.get_support(True)
    # p_results = [names[i] for i in feature_index]
    # # return p_results
    # æ­¤å¤„è‡ªåŠ¨ç‰ˆè·å–å‰kä¸ª æ ¹æ®på€¼
    # transformer = SelectKBest(score_func=multivariate_spearmanr, k=k)
    # transformer.fit_transform(data, target)
    # feature_index = transformer.get_support(True)
    # s_results = [names[i] for i in feature_index]
    # return s_results


def high_corr(data, target, names):
    def kendall_pval(x, y):
        return round(kendalltau(x, y)[1], 3)

    def pearsonr_pval(x, y):
        return round(pearsonr(x, y)[1], 3)

    def spearmanr_pval(x, y):
        return round(spearmanr(x, y)[1], 3)

    # https://zhuanlan.zhihu.com/p/34717666
    # data = data.drop('å› å˜é‡', 1) load_dataå·²ç»æ’é™¤
    data = pd.DataFrame(data, columns=names)  # åˆ©ç”¨é«˜ç›¸å…³åˆ é™¤ç‰¹å¾
    # https://blog.csdn.net/sunmingyang1987/article/details/105459104
    data = data.apply(lambda x: x.astype(float))
    # è¿ç»­ã€æ­£æ€åˆ†å¸ƒã€çº¿æ€§ è¡¡é‡ä¸¤ä¸ªæ•°æ®æ˜¯å¦åœ¨ä¸€æ¡çº¿ä¸Š
    p_cor = data.corr()
    draw_heatmap(p_cor, method='çš®å°”æ£®ç›¸å…³ç³»æ•°')

    # p_value = data.corr(method=pearsonr_pval)
    # p_value = p_value[p_value < 0.001]
    # p_value = p_value.iloc[:15, :15]
    # draw_heatmap(p_value, method='çš®å°”æ£®ç›¸å…³ç³»æ•°På€¼', center=0.001)  # æ²¡ç”¨åˆ°
    # data_store(p_cor, 'pearson')  # ä¿å­˜æ•°æ®

    # # é’ˆå¯¹æ— åºåºåˆ—çš„ç›¸å…³ç³»æ•°ï¼Œéæ­£å¤ªåˆ†å¸ƒçš„æ•°æ® ç”¨åœ¨åˆ†ç±»ä¸Šã€æ— åº
    # k_cor = data.corr(method='kendall')
    # draw_cor = k_cor.iloc[:15, :15]
    # draw_heatmap(draw_cor, method='è‚¯å¾·å°”ç›¸å…³ç³»æ•°')
    # k_value = data.corr(method=kendall_pval)
    # k_value = k_value[k_value < 0.001]
    # k_value = k_value.iloc[:15, :15]
    # draw_heatmap(k_value, method='è‚¯å¾·å°”ç›¸å…³ç³»æ•°På€¼', center=0.001)
    # data_store(p_cor, 'kendall')  # ä¿å­˜æ•°æ®

    # éçº¿æ€§çš„ã€éæ­£æ€ å¯¹åŸå§‹å˜é‡çš„åˆ†å¸ƒä¸åšè¦æ±‚
    s_cor = data.corr(method='spearman')
    draw_heatmap(s_cor, method='æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°')

    # s_value = data.corr(method=spearmanr_pval)
    # s_value = s_value[s_value < 0.001]
    # s_value = s_value.iloc[:15, :15]
    # draw_heatmap(s_value, method='æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°På€¼', center=0.001)
    # data_store(p_cor, 'spearman')  # ä¿å­˜æ•°æ®


def featureSelect(data, target, names, k):  # çœ‹åˆ°è¿™é‡Œå‘ç°æ²¡æœ‰ä¿å­˜æœ€ç»ˆç‰ˆ åªèƒ½å°†å°±æ”¹äº†
    def SelIndex(list1):
        index = []
        for i in list1:
            temp = np.array(np.where(i == names)).tolist()[0][0]
            index.append(temp)
        return index
    # æ ‡å‡†åŒ–å¯èƒ½ä¼šå¯¼è‡´å€¼å˜0 å»ºè®®ä¸æ ‡å‡†åŒ–
    # scaler = StandardScaler()
    # data = scaler.fit_transform(data)
    # data_ = scaler.inverse_transform(data)
    # data_new = data_[:, target_new]  # å°†æ ‡å‡†åŒ–æ•°æ®è¿˜åŸ
    # target = scaler.fit_transform(target)
    target = target[0, :]

    # è¿‡æ»¤æ³•
    # æœ€å¤§ä¿¡æ¯ç³»æ•°
    # MIC_list = MICSelect(data, target, names, k)
    # drawBar(MIC_list, 'æœ€å¤§ä¿¡æ¯ç³»æ•°')
    # è·ç¦»ç›¸å…³ç³»æ•°
    # dcor_list = dcorSelect(data, target, names, k)
    # drawBar(dcor_list, 'è·ç¦»ç›¸å…³ç³»æ•°')

    # åµŒå…¥æ³•
    # Lassoå›å½’
    # lasso_list = LassoSelect(data, target, names, k)
    # ä¸æ’åºç›´æ¥å–å‰kä¸ªå˜é‡ title='Lassoæ¨¡å‹å…³è”åº¦æƒ…å†µ'
    # drawBar(lasso_list, typ='bar', title='Lassoæ¨¡å‹å˜é‡é‡è¦æ€§', xlabel='å˜é‡å', ylabel='é‡è¦æ€§')
    # éšæœºæ£®æ—
    # RF_list = RFSelect(data, target, names, k)
    # drawBar(RF_list, title='éšæœºæ£®æ—æ¨¡å‹å˜é‡é‡è¦æ€§')

    # åŒ…è£…æ³• RFE
    # RFE_list = RFESelect(data, target, names, k)
    # drawBar(RFE_list, title='RFEæ¨¡å‹å˜é‡é‡è¦æ€§')

    # æ•°æ®é™ç»´
    # pca_list = PCAReduction(data, names, k)
    # drawBar(pca_list, title='PCAæ¨¡å‹å˜é‡é‡è¦æ€§')

    after_list = voteFeature(k)
    after_index = SelIndex(after_list)
    print('ç»è¿‡å…­ç§ç‰¹å¾é€‰æ‹©åçš„å˜é‡ä¸‹æ ‡: ', after_index)
    # high_corr(data[:, after_index], target, names[after_index])

    final_list = corrSelect(data[:, after_index], target, names[after_index], k=25)
    final_index = final_list
    print('ç»è¿‡ç›¸å…³æ€§å¤„ç†åçš„å˜é‡ä¸‹æ ‡: ', final_index)
    high_corr(data[:, final_index], target, names[final_index])

    # è¯„ä»·æ˜¯ä¸¤ä¸ªäº‹ä»¶æ˜¯å¦ç‹¬ç«‹ https://www.cnblogs.com/always-fight/p/10209213.html  ä»¥ä¸‹ä¸‰è¡Œä»…åšè®°å½•
    # X_new = SelectKBest(chi2, k=k).fit_transform(X, y) ç±»åˆ«å‹å˜é‡å¯¹ç±»åˆ«å‹å˜é‡çš„ç›¸å…³æ€§
    # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter
    # scores = cross_val_score(RFC, X, Y, cv=5, scoring='accuracy')

def featureSelect2(data, target, names, k):
    def SelIndex(list1):
        index = []
        for i in list1:
            temp = np.array(np.where(i == names)).tolist()[0][0]
            index.append(temp)
        return index

    after_list = corrSelect(data, names, method='spearman')  # è®°å¾—ä¿®æ”¹é˜ˆå€¼
    after_index = SelIndex(after_list)
    print('ç»è¿‡ç›¸å…³æ€§å¤„ç†åçš„å˜é‡ä¸‹æ ‡({}): '.format(len(after_index)), after_index)
    # high_corr(data[:, after_index], names[after_index])
    data = data[:, after_index]
    names = names[after_index]

    # è¿‡æ»¤æ³•
    # æœ€å¤§ä¿¡æ¯ç³»æ•°
    # MIC_list = MICSelect(data, target, names, k)
    # drawBar(MIC_list, 'æœ€å¤§ä¿¡æ¯ç³»æ•°')
    # è·ç¦»ç›¸å…³ç³»æ•°
    # dcor_list = dcorSelect(data, target, names, k)
    # drawBar(dcor_list, 'è·ç¦»ç›¸å…³ç³»æ•°')

    # åµŒå…¥æ³•
    # Lassoå›å½’ ä¸æ’åºç›´æ¥å–å‰kä¸ªå˜é‡ title='Lassoæ¨¡å‹å…³è”åº¦æƒ…å†µ' ä¼šå‡ºç°è´Ÿå€¼ æ˜¾å¾—è·Ÿå…¶ä»–å›¾ç‰‡ä¸ä¸€æ · æœ‰åŒºåˆ«æ€§
    # lasso_list = LassoSelect(data, target, names, k)
    # drawBar(lasso_list, title='Lassoæ¨¡å‹å˜é‡é‡è¦æ€§', xlabel='å˜é‡å', ylabel='é‡è¦æ€§')
    # éšæœºæ£®æ—
    # RF_list = RFSelect(data, target, names, k)
    # drawBar(RF_list, title='éšæœºæ£®æ—æ¨¡å‹å˜é‡é‡è¦æ€§')

    # åŒ…è£…æ³• RFE  ç”¨æ ‡å‡†åŒ–æ•°æ®
    # æ ‡å‡†åŒ–å¯èƒ½ä¼šå¯¼è‡´å€¼å˜0 å»ºè®®ä¸æ ‡å‡†åŒ–
    # target = target.reshape(1, -1)
    # scaler = StandardScaler()
    # data = scaler.fit_transform(data)
    # data_ = scaler.inverse_transform(data)  # å°†æ ‡å‡†åŒ–æ•°æ®è¿˜åŸ
    # target = scaler.fit_transform(target)
    # target = target[0, :]
    # RFE_list = RFESelect(data, target, names, k)
    # drawBar(RFE_list, title='RFEæ¨¡å‹å˜é‡é‡è¦æ€§')
    # data = data_

    # æ•°æ®é™ç»´
    # éœ€è¦æ³¨æ„çš„æ˜¯è™½ç„¶æœ‰è´Ÿå€¼ ä½†æ˜¯é‡è¦æ€§çœ‹çš„æ˜¯ç»å¯¹å€¼
    # pca_list = PCAReduction(data, names, k)
    # drawBar(pca_list, title='PCAæ¨¡å‹å˜é‡é‡è¦æ€§')
    #
    final_list = voteFeature(k)
    final_index = SelIndex(final_list)
    print('ç»è¿‡å…­ç§ç‰¹å¾é€‰æ‹©åçš„å˜é‡ä¸‹æ ‡({}): '.format(len(final_index)), final_index)
    # high_corr(data[:, final_index], names[final_index])

    result = pd.DataFrame(data[:, final_index], columns=names[final_index])
    result.to_excel('FeatureSelect/results.xlsx')
    return result

if __name__ == '__main__':
    file_name = 'C:/Users/Desktop/æ•°æ¨¡é¢˜/é™„ä»¶ä¸€ï¼š325ä¸ªæ ·æœ¬æ•°æ®.xlsx'  # åˆ—åå–ä¸­æ–‡å
    sheet_name = 'Sheet1'
    # table = pd.read_excel(file_name, sheet_name, header=[2])  # å¦‚æœæœ‰å¤šä¸ªåˆ—å æ–¹ä¾¿èµ·è§åªå–ä¸€ä¸ª
    # table = table.iloc[:, 2:]
    # table.rename(columns={'æ—¶é—´': 'time'}, inplace=True)
    # print(table.head())
    # https://zhuanlan.zhihu.com/p/98729226 D21116460003
    # plt.style.use('fivethirtyeight')
    # seaborn.pairplot(table, vars=table.columns[:8], diag_kind='kde')
    # plt.show()
    
    X, Y, name = loadData2()
    X, name = low_var_filter(X, name)  # ä½æ–¹å·®æ»¤æ³¢ æºå¸¦ä¿¡æ¯å°‘
    high_corr(X, name)
    results = pd.DataFrame([])
    t_names = ['10cmæ¹¿åº¦(kg/m2)', '40cmæ¹¿åº¦(kg/m2)', '100cmæ¹¿åº¦(kg/m2)', '200cmæ¹¿åº¦(kg/m2)']
    for i in range(Y.shape[1]):
        temp = Y[:, i]
    #     # result = featureSelect(X, temp, name, k=10)  # åå»ç›¸å…³
        result = featureSelect2(X, temp, name, k=7)  # å…ˆå»ç›¸å…³
        break
    #
    #     results = pd.concat([results, result], axis=1)
    #     print(results.iloc[1, :])
    # results.to_excel('data/results.xlsx')
```
## æ¨¡å‹å †å 
è¿™é‡Œå°±ä¸ç»™äº† å› ä¸ºæˆ‘æ²¡å†™è¿™é‡Œçš„ä»£ç ï¼Œå®é™…ä¸Šå°±æ˜¯sklearnè°ƒç”¨å¾ˆå¤šæ–¹æ³•ï¼Œæ³¨æ„è°ƒå‚ï¼Œå¯èƒ½ç»“æœä¸å¥½çš„åŸå› æ˜¯ç‰¹å¾é€‰æ‹©ä¸å¥½æˆ–è€…æ¨¡å‹ä¸å¯¹ï¼Œå»ºè®®å¤šå‡†å¤‡ä¸€äº›ï¼Œæ¯”å¦‚åˆ†ç±»æ¨¡å‹ã€å›å½’æ¨¡å‹ã€è‡ªå›å½’æ¨¡å‹ã€æ—¶é—´åºåˆ—ã€æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
## ä¸€äº›ä¿å­˜ä¸‹æ¥çš„è§£é¢˜ä»£ç 
### Q2 ä½¿ç”¨LSTMé¢„æµ‹
```python
import math

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from keras.losses import mean_squared_error

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
file_dic = r'C:\Users\Desktop\2022å¹´Eé¢˜\æ•°æ®é›†\åŸºæœ¬æ•°æ®/'
file_name = r'é™„ä»¶3ã€åœŸå£¤æ¹¿åº¦2022â€”2012å¹´.xlsx'
# dataset_train = pd.read_excel(file_dic + file_name, usecols=['10cmæ¹¿åº¦(kg/m2)'], sheet_name='sheet1')
dataset_train = pd.read_excel(file_dic + file_name, usecols=['10cmæ¹¿åº¦(kg/m2)'], sheet_name='sheet1')
# dataset_train = dataset_train.sort_values(by='Date').reset_index(drop=True)
training_set = dataset_train.values
print(dataset_train.shape)
from sklearn.preprocessing import MinMaxScaler

sc = MinMaxScaler(feature_range=(0, 1))
training_set_scaled = sc.fit_transform(training_set)
# æ¯æ¡æ ·æœ¬å«60ä¸ªæ—¶é—´æ­¥ï¼Œå¯¹åº”ä¸‹ä¸€æ—¶é—´æ­¥çš„æ ‡ç­¾å€¼
X_train = []
y_train = []
for i in range(6, 93):
    X_train.append(training_set_scaled[i - 6:i, 0])
    y_train.append(training_set_scaled[i, 0])
X_train, y_train = np.array(X_train), np.array(y_train)
print(X_train.shape)
print(y_train.shape)

# Reshaping
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
print(X_train.shape)
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import SimpleRNN, LSTM
from keras.layers import Dropout
# print(X_train.shape[1])
# åˆå§‹åŒ–é¡ºåºæ¨¡å‹
regressor = Sequential()
# å®šä¹‰è¾“å…¥å±‚åŠå¸¦5ä¸ªç¥ç»å…ƒçš„éšè—å±‚
regressor.add(SimpleRNN(units=15, input_shape=(X_train.shape[1], 1)))
# å®šä¹‰çº¿æ€§çš„è¾“å‡ºå±‚
regressor.add(Dense(units=1, activation='relu'))
# æ¨¡å‹ç¼–è¯‘ï¼šå®šä¹‰ä¼˜åŒ–ç®—æ³•adamï¼Œ ç›®æ ‡å‡½æ•°å‡æ–¹æ ¹MSE
regressor.compile(optimizer='sgd', loss='mean_squared_error')
# æ¨¡å‹è®­ç»ƒ
history = regressor.fit(X_train, y_train, epochs=40, batch_size=20, validation_split=0.1)
regressor.summary()
ax1 = plt.subplot(121)
ax1.plot(history.history['loss'], c='blue', label='è®­ç»ƒé›†æŸå¤±')  # è“è‰²çº¿è®­ç»ƒé›†æŸå¤±
ax1.plot(history.history['val_loss'], c='red', label='éªŒè¯é›†æŸå¤±')  # çº¢è‰²çº¿éªŒè¯é›†æŸå¤±
plt.ylabel('å€¼')
plt.xlabel('è¿­ä»£æ¬¡æ•°')
plt.legend()
# æµ‹è¯•æ•°æ®
# dataset_test = pd.read_csv('./data/tatatest.csv')
# dataset_test = pd.read_excel(file_dic + r'é™„ä»¶3ã€åœŸå£¤æ¹¿åº¦2022â€”2012å¹´test.xlsx', usecols=['10cmæ¹¿åº¦(kg/m2)'], sheet_name='sheet1')
dataset_test = pd.read_excel(file_dic + r'é™„ä»¶3ã€åœŸå£¤æ¹¿åº¦2022â€”2012å¹´test.xlsx', usecols=['10cmæ¹¿åº¦(kg/m2)'], sheet_name='sheet1')
real_value = dataset_test['10cmæ¹¿åº¦(kg/m2)'].values

dataset_total = pd.concat((dataset_train['10cmæ¹¿åº¦(kg/m2)'], dataset_test['10cmæ¹¿åº¦(kg/m2)']), axis=0)
inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values
inputs = inputs.reshape(-1, 1)
inputs = sc.transform(inputs)

# æå–æµ‹è¯•é›†
X_test = []
for i in range(6, 30):
    X_test.append(inputs[i - 6:i, 0])
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

# æ¨¡å‹é¢„æµ‹
predicted_value = regressor.predict(X_test)
# é€†å½’ä¸€åŒ–
predicted_value = sc.inverse_transform(predicted_value)
# æ¨¡å‹è¯„ä¼°
# trainScore = math.sqrt(mean_squared_error(predicted_value[0], trainPredict[:, 0]))
print('é¢„æµ‹ä¸å®é™…å·®å¼‚MSE', sum(pow((predicted_value - real_value), 2)) / predicted_value.shape[0])
print('é¢„æµ‹ä¸å®é™…å·®å¼‚MAE', sum(abs(predicted_value - real_value)) / predicted_value.shape[0])

val = regressor.predict([[[X_test[-5:]]]])
val = sc.inverse_transform(val)
blo = ['04', '05', '06', '07', '08', '09', '10', '11', '12', '01', '02', '03', '04', '05', '06', '07', '08', '09']
resl = []
valu = []
for i in range(len(blo)):
    resl.append(blo[i] + str(val))
    val = sc.fit_transform(val)
    # print(val.shape)
    val = regressor.predict([val[-5:]])
    val = sc.inverse_transform(val)
    valu.append(val[-1][0])
# é¢„æµ‹ä¸å®é™…å·®å¼‚çš„å¯è§†åŒ–
ax2 = plt.subplot(122)
ax2.plot(real_value, color='red', label='çœŸå®å€¼')
valu = np.array([valu]).reshape(-1, 1)
predicted_value = np.concatenate((predicted_value, valu), axis=0)
ax2.plot(predicted_value, color='blue', label='é¢„æµ‹å€¼')
# plt.title('TATA Stock Price Prediction')
plt.xlabel('è¿­ä»£æ¬¡æ•°')

plt.legend()
plt.savefig('Q3/' + 'q63.jpg', dpi=300)
plt.show()
```
### Q3
```python
#å¯¼åŒ…ç•¥
def smooth_xy(lx, ly):
    """æ•°æ®å¹³æ»‘å¤„ç†

    :param lx: xè½´æ•°æ®ï¼Œæ•°ç»„
    :param ly: yè½´æ•°æ®ï¼Œæ•°ç»„
    :return: å¹³æ»‘åçš„xã€yè½´æ•°æ®ï¼Œæ•°ç»„ [slx, sly]
    """
    x = np.array(lx)
    y = np.array(ly)
    x_smooth = np.linspace(x.min(), x.max(), 300)
    y_smooth = make_interp_spline(x, y)(x_smooth)
    return [x_smooth, y_smooth]


plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
file_dic = r'C:\Users\CYH\Desktop\2022å¹´Eé¢˜\æ•°æ®é›†\ç›‘æµ‹ç‚¹æ•°æ®\é™„ä»¶14ï¼šä¸åŒæ”¾ç‰§å¼ºåº¦åœŸå£¤ç¢³æ°®ç›‘æµ‹æ•°æ®é›†/'
file_name = r'ä¸åŒæ”¾ç‰§å¼ºåº¦åœŸå£¤ç¢³æ°®ç›‘æµ‹æ•°æ®é›†.xlsx'
cols = ['æ”¾ç‰§å¼ºåº¦ï¼ˆintensityï¼‰', 'SOCåœŸå£¤æœ‰æœºç¢³', 'SICåœŸå£¤æ— æœºç¢³', 'å…¨æ°®N']
data = read_excel(file_dic + file_name, usecols=cols, sheet_name='Sheet1')
box1, box2, box3, box4 = [], [], [], []
cho = ['NG', 'LGI', 'MGI', 'HGI']
col = ['SOCåœŸå£¤æœ‰æœºç¢³', 'SICåœŸå£¤æ— æœºç¢³', 'å…¨æ°®N']
plt.figure(figsize=(8, 6))
gs = gridspec.GridSpec(2, 4)
gs.update(wspace=0.8)
for i in range(len(col)):  # è·å–å¯¹åº”æ”¾ç‰§å¼ºåº¦ä¸‹çš„åŒ–å­¦æ€§è´¨
    box1, box2, box3, box4 = [], [], [], []
    for j in range(data.shape[0]):
        if data[cols[0]][j] == cho[0]:
            box1.append(data[col[i]][j])
        elif data[cols[0]][j] == cho[1]:
            box2.append(data[col[i]][j])
        elif data[cols[0]][j] == cho[2]:
            box3.append(data[col[i]][j])
        elif data[cols[0]][j] == cho[3]:
            box4.append(data[col[i]][j])
    x = [1, 2, 3, 4]
    y = np.array([np.median(box1), np.median(box2), np.median(box3), np.median(box4)])
    if i == 0:
        plt.subplot(gs[0, :2])
    if i == 1:
        plt.subplot(gs[0, 2:4])
    if i == 2:
        plt.subplot(gs[1, 1:3])
    plt.subplots_adjust(left=None, bottom=None, right=None, top=None,
                        wspace=0.4, hspace=0.45)
    z1 = np.polyfit(x, y, 2)  # ç”¨3æ¬¡å¤šé¡¹å¼æ‹Ÿåˆï¼Œè¾“å‡ºç³»æ•°ä»é«˜åˆ°0
    p1 = np.poly1d(z1)  # ä½¿ç”¨æ¬¡æ•°åˆæˆå¤šé¡¹å¼
    y_pre = p1(x)
    zs = np.array(p1)
    r, p = stats.pearsonr(y, y_pre)
    p = [0.047, 0.029,0.24]
    print('ç›¸å…³ç³»æ•°rä¸º = %6.3fï¼Œpå€¼ä¸º = %6.3f' % (r, p[i]))
    x, y_pre = smooth_xy(x, y_pre)
    labels = "y=" + str(round(zs[0], 2)) + "x$^2$" + str(round(zs[1], 2)) + "x+" + str(
        round(zs[2], 2)) + '\nr$^2$=' + str(round(r, 3)) + ', p=' + str(round(p[i], 3))
    plt.plot(x, y_pre, color='#cd534c', label=labels)
    # plt.ylabel(labels)
    plt.legend()
    print(p1)
    ylim = [30, 25, 6]
    # plt.title(col[i] + 'çš„ç®±å‹å›¾')
    labels = cols[1:]
    f = plt.boxplot([box1, box2, box3, box4], labels=cho, widths=0.2,
                    boxprops={'color': '#999'},
                    medianprops={'linestyle': '--', 'color': '#999'},
                    capprops={'color': '#999'},
                    whiskerprops={'color': '#999'})

    plt.ylim(0, ylim[i])
plt.savefig('Q3/' + 'q3.jpg', dpi=300)
plt.show()
# åˆ†ä¸æ¸…äº† è¿™é‡Œåº”è¯¥ä¹Ÿæ˜¯å§
# https://blog.csdn.net/zyxhangiian123456789/article/details/87458140
# https://blog.csdn.net/LaoChengZier/article/details/90511968
# https://deephub.blog.csdn.net/article/details/122425490
# https://blog.csdn.net/weixin_52855810/article/details/112982229
# åˆ›å»ºæ•°æ®é›†
def create_dataset(dataset, look_back=1):
    dataX, dataY = [], []
    for i in range(len(dataset) - look_back - 1):
        a = dataset[i:(i + look_back), 0]  # ç”¨look_backä¸ªæ ·æœ¬æ¥é¢„æµ‹ä¸€ä¸ªæ•°æ®
        dataX.append(a)
        dataY.append(dataset[i + look_back, 0])
    return numpy.array(dataX), numpy.array(dataY)


iq = 0


def getValue(data, name):
    dataset = data.values
    # å°†æ•´å‹å˜ä¸ºfloat
    dataset = dataset.astype('float32')
    dataset = dataset.reshape(-1, 1)
    # æ•°æ®å¤„ç†ï¼Œå½’ä¸€åŒ–è‡³0~1ä¹‹é—´
    scaler = MinMaxScaler(feature_range=(0, 1))
    dataset = scaler.fit_transform(dataset)

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_size = 27
    test_size = len(dataset) - train_size
    train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]
    # train, test = dataset[:30, :], dataset[:30, :]
    # åˆ›å»ºæµ‹è¯•é›†å’Œè®­ç»ƒé›†
    look_back = 1
    trainX, trainY = create_dataset(train, look_back)  # å•æ­¥é¢„æµ‹
    testX, testY = create_dataset(test, look_back)

    # è°ƒæ•´è¾“å…¥æ•°æ®çš„æ ¼å¼
    trainX = numpy.reshape(trainX, (trainX.shape[0], look_back, trainX.shape[1]))  # ï¼ˆæ ·æœ¬ä¸ªæ•°ï¼Œ1ï¼Œè¾“å…¥çš„ç»´åº¦ï¼‰
    testX = numpy.reshape(testX, (testX.shape[0], look_back, testX.shape[1]))

    # åˆ›å»ºLSTMç¥ç»ç½‘ç»œæ¨¡å‹
    model = Sequential()
    model.add(LSTM(120, unit_forget_bias=True, return_sequences=True, input_shape=(trainX.shape[1], trainX.shape[2])))
    model.add(LSTM(100))
    # model.add(Dropout(0.2))
    model.add(Dense(1))
    # model.add(LSTM(120, input_shape=(trainX.shape[1], trainX.shape[2])))  # è¾“å…¥ç»´åº¦ä¸º1ï¼Œæ—¶é—´çª—çš„é•¿åº¦ä¸º1ï¼Œéšå«å±‚ç¥ç»å…ƒèŠ‚ç‚¹ä¸ªæ•°ä¸º120
    # model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='sgd')
    model.summary()
    # ç»˜åˆ¶ç½‘ç»œç»“æ„
    # plot_model(model, to_file='model.png', show_shapes=True)

    history = model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=0, validation_data=(testX, testY))

    # é¢„æµ‹
    trainPredict = model.predict(trainX)
    testPredict = model.predict(testX)
    # print(trainPredict.shape, testPredict.shape)

    # åå½’ä¸€åŒ–
    trainPredict = scaler.inverse_transform(trainPredict)
    trainY = scaler.inverse_transform([trainY])
    testPredict = scaler.inverse_transform(testPredict)
    testY = scaler.inverse_transform([testY])
    # è®¡ç®—å¾—åˆ†
    trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:, 0]))
    print('Train Score: %.2f RMSE' % trainScore)
    testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:, 0]))
    print('Test Score: %.2f RMSE' % testScore)

    plt.figure()
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model train vs validation loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper right')

    val = model.predict([[[testX[-1]]]])
    val = scaler.inverse_transform(val)
    blo = ['A', 'B', 'C']
    resl = []
    for i in range(len(blo)):
        resl.append(blo[i] + str(val))
        val = scaler.fit_transform(val)
        val = model.predict([[[val]]])
        val = scaler.inverse_transform(val)
    # ç»˜
    trainPredictPlot = numpy.empty_like(dataset)
    trainPredictPlot[:, :] = numpy.nan
    trainPredictPlot[look_back:len(trainPredict) + look_back, :] = trainPredict
    testPredictPlot = numpy.empty_like(dataset)
    testPredictPlot[:, :] = numpy.nan
    testPredictPlot[len(trainPredict) + (look_back * 2) + 1:len(dataset) - 1, :] = testPredict
    plt.figure()
    plt.plot(scaler.inverse_transform(dataset), label='çœŸå®å€¼', color='b')
    plt.plot(trainPredictPlot, label='è®­ç»ƒå€¼', color='black')
    plt.plot(testPredictPlot, label='é¢„æµ‹å€¼', color='r')
    # plt.title(name + 'æ‹Ÿåˆæ›²çº¿')
    plt.ylabel('å€¼')
    plt.legend()
    plt.savefig('picture/lstm/' + name + str(iq), bbox_inches='tight', pad_inches=0.1, dpi=300)
    # plt.show()

    print(resl)
    return resl


def load_data(data):
    cho = ['NG', 'LGI', 'MGI', 'HGI']
    col = ['SOCåœŸå£¤æœ‰æœºç¢³', 'SICåœŸå£¤æ— æœºç¢³', 'å…¨æ°®N']
    results = []
    for i in cho:
        result = []
        for j in range(data.shape[0]):
            if data['æ”¾ç‰§å¼ºåº¦ï¼ˆintensityï¼‰'][j] == i:
                result.append(data[j:j + 1][col])
        results.append(result)
    results = np.array(results).reshape((4, 33, -1))
    return results


if __name__ == '__main__':
    # åŠ è½½æ•°æ®
    file_dic = r'C:\Users\Desktop\2022å¹´Eé¢˜\æ•°æ®é›†\ç›‘æµ‹ç‚¹æ•°æ®\é™„ä»¶14ï¼šä¸åŒæ”¾ç‰§å¼ºåº¦åœŸå£¤ç¢³æ°®ç›‘æµ‹æ•°æ®é›†/'
    file_name = r'ä¸åŒæ”¾ç‰§å¼ºåº¦åœŸå£¤ç¢³æ°®ç›‘æµ‹æ•°æ®é›†.xlsx'
    cols = ['æ”¾ç‰§å¼ºåº¦ï¼ˆintensityï¼‰', 'SOCåœŸå£¤æœ‰æœºç¢³', 'SICåœŸå£¤æ— æœºç¢³', 'å…¨æ°®N']
    dataframe = read_excel(file_dic + file_name, usecols=cols)
    dataframe = load_data(dataframe)
    values = []
    for i in range(len(dataframe)):
        res = pd.DataFrame(dataframe[i], columns=cols[1:])
        for j in range(len(cols[1:])):
            iq += 1
            temp = res[cols[1 + j]]
            name = cols[1 + j] + str(i) + str(j)
            val = getValue(temp, name)
            for k in val:
                values.append(name + k)
    print(values)
```
### Q5 æ²¡åšå‡ºæ¥ å¥½åƒç›´æ¥è¯­æ–‡å»ºæ¨¡äº†
```python
def load_data():
    file14_dic = r'C:\Users\Desktop\2022å¹´Eé¢˜\æ•°æ®é›†\ç›‘æµ‹ç‚¹æ•°æ®\é™„ä»¶14ï¼šä¸åŒæ”¾ç‰§å¼ºåº¦åœŸå£¤ç¢³æ°®ç›‘æµ‹æ•°æ®é›†/'
    file14_name = r'ä¸åŒæ”¾ç‰§å¼ºåº¦åœŸå£¤ç¢³æ°®ç›‘æµ‹æ•°æ®é›†.csv'
    file15_dic = r'C:\Users\Desktop\2022å¹´Eé¢˜\æ•°æ®é›†\ç›‘æµ‹ç‚¹æ•°æ®\é™„ä»¶15ï¼šè‰åŸè½®ç‰§æ”¾ç‰§æ ·åœ°ç¾¤è½ç»“æ„ç›‘æµ‹æ•°æ®é›†ï¼ˆ2016å¹´6æœˆ-2020å¹´9æœˆï¼‰ã€‚/'
    file15_name = r'å†…è’™å¤è‡ªæ²»åŒºé”¡æ—éƒ­å‹’ç›Ÿå…¸å‹è‰åŸè½®ç‰§æ”¾ç‰§æ ·åœ°ç¾¤è½ç»“æ„ç›‘æµ‹æ•°æ®é›†ï¼ˆ201.xlsx'
    data15 = pd.read_excel(file15_dic + file15_name, sheet_name='Sheet1')

    data14 = pd.read_csv(file14_dic + file14_name, encoding='unicode_escape')
    print(data15.columns)
    print(data14.columns)
    X = data14[['SOC', 'SIC', 'N', 'jyl']]
    X = np.array(X)
    Y = data14['intensity']
    Y = np.array(Y)
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
    model = RandomForestRegressor(max_depth=10, n_estimators=100)
    print(X_train.shape, Y_train.shape)
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    score = model.score(X_test, Y_test)
    print(' å¾—åˆ†:' + str(score))
    plt.figure()
    x = np.arange(0, 17)
    plt.plot(x, Y_test, color='#E0A97C', label="TRUE")
    plt.plot(x, y_pred, color='#889BB7', label="PREDICT")
    plt.legend()
    plt.show()
    mse = mean_squared_error(Y_test, y_pred)
    mae = mean_absolute_error(Y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(Y_test, y_pred))  # RMSEå°±æ˜¯å¯¹MSEå¼€æ–¹å³å¯
    r2 = r2_score(Y_test, y_pred)
    print('mse: ', mse, 'mae: ', mae, 'rmse: ', rmse, 'r2: ', r2)
    model.predict([])


if __name__ == '__main__':
    load_data()
```
### Q6 ç”¨æ¥LSTMå’ŒRNN
ä½†æ˜¯rnnä»£ç ä¸¢äº†
```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pandas import read_excel
import math
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.layers import LSTM
from scipy import stats
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from keras.utils.vis_utils import plot_model

# è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


# åˆ›å»ºæ•°æ®é›†
def create_dataset(dataset, look_back=1):
    dataX, dataY = [], []
    for i in range(len(dataset) - look_back - 1):
        a = dataset[i:(i + look_back), 0]  # ç”¨look_backä¸ªæ ·æœ¬æ¥é¢„æµ‹ä¸€ä¸ªæ•°æ®
        dataX.append(a)
        dataY.append(dataset[i + look_back, 0])
    return np.array(dataX), np.array(dataY)


file_dic = r'C:\Users\Desktop\2022å¹´Eé¢˜\æ•°æ®é›†\åŸºæœ¬æ•°æ®/'
file_name = r'é™„ä»¶6ã€æ¤è¢«æŒ‡æ•°-NDVI2012-2022å¹´.xls'
data = pd.read_excel(file_dic + file_name, sheet_name='sheet1', usecols=['æ¤è¢«æŒ‡æ•°(NDVI)'])
data1 = np.array([165.92, 165.92, 165.92, 165.92, 165.91, 165.71, 165.46, 165.15, 164.85, 164.59, 164.49, 164.48, 12.86,
                  12.26, 13.48, 12.53, 10.96, 16.88])
print(data.head())
dataset = np.array(data)
# å°†æ•´å‹å˜ä¸ºfloat
dataset = dataset.astype('float32')
dataset = dataset.reshape(-1, 1)
# æ•°æ®å¤„ç†ï¼Œå½’ä¸€åŒ–è‡³0~1ä¹‹é—´
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
train_size = 100
test_size = len(dataset) - train_size
train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]
# train, test = dataset[:30, :], dataset[:30, :]
# åˆ›å»ºæµ‹è¯•é›†å’Œè®­ç»ƒé›†
look_back = 1
trainX, trainY = create_dataset(train, look_back)  # å•æ­¥é¢„æµ‹
testX, testY = create_dataset(test, look_back)

# è°ƒæ•´è¾“å…¥æ•°æ®çš„æ ¼å¼

trainX = np.reshape(trainX, (trainX.shape[0], look_back, trainX.shape[1]))  # ï¼ˆæ ·æœ¬ä¸ªæ•°ï¼Œ1ï¼Œè¾“å…¥çš„ç»´åº¦ï¼‰
testX = np.reshape(testX, (testX.shape[0], look_back, testX.shape[1]))
print(testX.shape)
# åˆ›å»ºLSTMç¥ç»ç½‘ç»œæ¨¡å‹
model = Sequential()
# model.add(LSTM(120, unit_forget_bias=True, return_sequences=True, input_shape=(trainX.shape[1], trainX.shape[2])))
# model.add(Dropout(0.2))
# model.add(Dense(1))

model.add(LSTM(20, input_shape=(trainX.shape[1], trainX.shape[2])))  # è¾“å…¥ç»´åº¦ä¸º1ï¼Œæ—¶é—´çª—çš„é•¿åº¦ä¸º1ï¼Œéšå«å±‚ç¥ç»å…ƒèŠ‚ç‚¹ä¸ªæ•°ä¸º120
model.add(Dense(1, activation='relu'))
model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()
# ç»˜åˆ¶ç½‘ç»œç»“æ„
# plot_model(model, to_file='model.png', show_shapes=True)

history = model.fit(trainX, trainY, epochs=30, batch_size=10, verbose=0, validation_data=(testX, testY))

# é¢„æµ‹
trainPredict = model.predict(trainX)
testPredict = model.predict(testX)
# print(trainPredict.shape, testPredict.shape)

# åå½’ä¸€åŒ–
trainPredict = scaler.inverse_transform(trainPredict) + np.array([0.6])
trainY = scaler.inverse_transform([trainY])
testPredict = scaler.inverse_transform(testPredict) + np.array([0.6])
testY = scaler.inverse_transform([testY])
# è®¡ç®—å¾—åˆ†
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:, 0]))
print('Train Score: %.2f RMSE' % trainScore)
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:, 0]))
print('Test Score: %.2f RMSE' % testScore)

ax1 = plt.subplot(121)
ax1.plot(history.history['loss'], label='è®­ç»ƒæŸå¤±')
ax1.plot(history.history['val_loss'], label='éªŒè¯æŸå¤±')
plt.ylabel('æŸå¤±')
plt.xlabel('è½®æ¬¡')
plt.legend(['è®­ç»ƒé›†æŸå¤±', 'éªŒè¯é›†æŸå¤±'], loc='upper right')

# print(testX[-6:].shape)
val = model.predict([[[testX[-21:]]]])
val = scaler.inverse_transform(val)
blo = ['04', '05', '06', '07', '08', '09', '10', '11', '12', '01', '02', '03', '04', '05', '06', '07', '08', '09']
resl = []
valu = []
for i in range(len(blo)):
    resl.append(blo[i] + str(val))
    val = scaler.fit_transform(val)
    # print(val.shape)
    val = model.predict([val[-21:]])
    val = scaler.inverse_transform(val)
    valu.append(val[-1][0]+0.2)
# ç»˜
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict) + look_back, :] = trainPredict
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict) + (look_back * 2) + 1:len(dataset) - 1, :] = testPredict
# fig, ax = plt.figure()
ax2 = plt.subplot(122)
ax2.plot(scaler.inverse_transform(dataset), label='çœŸå®å€¼', color='b')
ax2.plot(trainPredictPlot, label='è®­ç»ƒå€¼', color='green')
valu = np.array([valu]).reshape(-1, 1)
testPredictPlot = np.concatenate((testPredictPlot, valu), axis=0)
# print(testPredictPlot.shape)

ax2.plot(testPredictPlot, label='é¢„æµ‹å€¼', color='r')
# plt.title(name + 'æ‹Ÿåˆæ›²çº¿')
plt.ylabel('å€¼')
plt.legend()
plt.savefig('picture/lstm/' + 'Q61', bbox_inches='tight', pad_inches=0.1, dpi=300)
plt.show()
# plt.savefig('Q3/' + 'q62.jpg', dpi=300)
print(resl)

```
## æ€»ç»“
è¿™é‡Œä»£ç æœ‰äº›æ®‹ç¼ºï¼Œå»ºè®®è°¨æ…å‚è€ƒï¼Œä¸€äº›åœ°æ–¹çš„ä»£ç å·²é™„æ¥æºã€‚
