<!doctype html><html lang=zh-cn><head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#"><meta charset=utf-8><meta name=generator content="Hugo 0.111.3"><meta name=theme-color content="#fff"><meta name=color-scheme content="light dark"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no, date=no, address=no, email=no"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><title>Delving into Deep Imbalanced Regressionç¿»è¯‘ | æ‚ é—²ã®å°å±‹</title><link rel=stylesheet href=/css/meme.min.css><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@2/dist/instantsearch.min.js defer></script><script src=/js/meme.min.js></script>
<link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto+Serif+SC:wght@400;500;700&amp;family=Source+Code+Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap" media=print onload='this.media="all"'><noscript><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,700;1,400;1,700&amp;family=Noto+Serif+SC:wght@400;500;700&amp;family=Source+Code+Pro:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap"></noscript><meta name=author content="Rurouni"><meta name=description content="éå¯¹ç…§ç¿»è¯‘ï¼Œæœ‰æ‰€ç®€ç•¥ã€‚ç¿»è¯‘ä¸å¯¹ï¼Œå°½æƒ…è°…è§£ï¼Œå¯ç•™è¨€ å› ä¸ºhaloæ¸²æŸ“çš„åŸå› ï¼Œæœ‰äº›åœ°æ–¹å¯èƒ½â€¦â€¦"><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=mask-icon href=/icons/safari-pinned-tab.svg color=#2a6df4><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-title content="æ‚ é—²ã®å°å±‹"><meta name=apple-mobile-web-app-status-bar-style content="black"><meta name=mobile-web-app-capable content="yes"><meta name=application-name content="æ‚ é—²ã®å°å±‹"><meta name=msapplication-starturl content="../../"><meta name=msapplication-TileColor content="#fff"><meta name=msapplication-TileImage content="../../icons/mstile-150x150.png"><link rel=manifest href=/manifest.json><link rel=canonical href=https://keepjolly.com/archives/delving-into-dir-translation/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","datePublished":"2022-08-16T20:19:04+00:00","dateModified":"2023-05-06T13:51:51+00:00","url":"https://keepjolly.com/archives/delving-into-dir-translation/","headline":"Delving into Deep Imbalanced Regressionç¿»è¯‘","description":"éå¯¹ç…§ç¿»è¯‘ï¼Œæœ‰æ‰€ç®€ç•¥ã€‚ç¿»è¯‘ä¸å¯¹ï¼Œå°½æƒ…è°…è§£ï¼Œå¯ç•™è¨€ å› ä¸ºhaloæ¸²æŸ“çš„åŸå› ï¼Œæœ‰äº›åœ°æ–¹å¯èƒ½â€¦â€¦","inLanguage":"zh-CN","articleSection":"posts","wordCount":16482,"image":["https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-1.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-2.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-3.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-4.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-5.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-6.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-7.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-8.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-9.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-10.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-11.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-12.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-13.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-14.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-15.png","https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-16.png"],"author":{"@type":"Person","description":"çŸ¥è¡Œåˆä¸€","email":"1366475809@qq.com","image":"https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/profile%20photo_1647067705220.jpeg","url":"https://keepjolly.com/","name":"Rurouni"},"license":"åœ¨ä¿ç•™æœ¬æ–‡ä½œè€…åŠæœ¬æ–‡é“¾æ¥çš„å‰æä¸‹ï¼Œéå•†ä¸šç”¨é€”éšæ„è½¬è½½åˆ†äº«(æˆ‘ä¼šé«˜å¼ºåº¦è‡ªæœçš„å–”ğŸ‘Š)ã€‚","publisher":{"@type":"Organization","name":"æ‚ é—²ã®å°å±‹","logo":{"@type":"ImageObject","url":"https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo%2Fweb_logo.png"},"url":"https://keepjolly.com/"},"mainEntityOfPage":{"@type":"WebSite","@id":"https://keepjolly.com/"}}</script><meta name=twitter:card content="summary_large_image"><meta property="og:title" content="Delving into Deep Imbalanced Regressionç¿»è¯‘"><meta property="og:description" content="éå¯¹ç…§ç¿»è¯‘ï¼Œæœ‰æ‰€ç®€ç•¥ã€‚ç¿»è¯‘ä¸å¯¹ï¼Œå°½æƒ…è°…è§£ï¼Œå¯ç•™è¨€ å› ä¸ºhaloæ¸²æŸ“çš„åŸå› ï¼Œæœ‰äº›åœ°æ–¹å¯èƒ½â€¦â€¦"><meta property="og:url" content="https://keepjolly.com/archives/delving-into-dir-translation/"><meta property="og:site_name" content="æ‚ é—²ã®å°å±‹"><meta property="og:locale" content="zh"><meta property="og:image" content="https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758.png"><meta property="og:type" content="article"><meta property="article:published_time" content="2022-08-16T20:19:04+00:00"><meta property="article:modified_time" content="2023-05-06T13:51:51+00:00"><meta property="article:section" content="posts"><meta name=google-site-verification content="mBjLYgXaXR8EAfTNbi4DTVC5KOJuBHpZDsmtgbgC6Rs"></head><body><div class=container><header class=header><div class=header-wrapper><div class="header-inner single"><div class=site-brand><a href=/ class=brand>æ‚ é—²ã®å°å±‹</a></div><nav class=nav><ul class=menu id=menu><li class="menu-item active"><a href=/posts/><span class=menu-item-name>æ–‡ç« </span></a></li><li class=menu-item><a href=/tags/><span class=menu-item-name>æ ‡ç­¾</span></a></li><li class=menu-item><a href=/about/><span class=menu-item-name>å…³äº</span></a></li><li class=menu-item><a id=theme-switcher href=#><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-light"><path d="M193.2 104.5 242 7a18 18 0 0128 0l48.8 97.5L422.2 70A18 18 0 01442 89.8l-34.5 103.4L505 242a18 18 0 010 28l-97.5 48.8L442 422.2A18 18 0 01422.2 442l-103.4-34.5L270 505a18 18 0 01-28 0l-48.8-97.5L89.8 442A18 18 0 0170 422.2l34.5-103.4-97.5-48.8a18 18 0 010-28l97.5-48.8L70 89.8A18 18 0 0189.8 70zM256 128a128 128 0 10.01.0M256 160a96 96 0 10.01.0"/></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon theme-icon-dark"><path d="M27 412A256 256 0 10181 5a11.5 11.5.0 00-5 20A201.5 201.5.0 0142 399a11.5 11.5.0 00-15 13"/></svg></a></li><li class="menu-item search-item"><form id=search class=search role=search><label for=search-input><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon search-icon"><path d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></label><input type=search id=search-input class=search-input></form><template id=search-result hidden><article class="content post"><h2 class=post-title><a class=summary-title-link></a></h2><summary class=summary></summary><div class=read-more-container><a class=read-more-link>é˜…è¯»æ›´å¤š Â»</a></div></article></template></li></ul></nav></div></div><input type=checkbox id=nav-toggle aria-hidden=true>
<label for=nav-toggle class=nav-toggle></label>
<label for=nav-toggle class=nav-curtain></label></header><main class="main single" id=main><div class=main-inner><article class="content post h-entry" data-align=justify data-type=posts data-toc-num=true><h1 class="post-title p-name">Delving into Deep Imbalanced Regressionç¿»è¯‘</h1><nav class=contents><h2 id=contents class=contents-title>ç›®å½•</h2><ol class=toc><li><a id=contents:abstract href=#abstract>Abstract</a></li><li><a id=contents:1-introduction href=#1-introduction>1. Introduction</a></li><li><a id=contents:2-related-work href=#2-related-work>2. Related Work</a></li><li><a id=contents:3-methods href=#3-methods>3. Methods</a><ol><li><a id=contents:31-label-distribution-smoothing href=#31-label-distribution-smoothing>3.1. Label Distribution Smoothing</a></li><li><a id=contents:32-feature-distribution-smoothing href=#32-feature-distribution-smoothing>3.2. Feature Distribution Smoothing</a></li></ol></li><li><a id=contents:4-benchmarking-dir href=#4-benchmarking-dir>4. Benchmarking DIR</a><ol><li><a id=contents:41-main-results href=#41-main-results>4.1. Main Results</a></li><li><a id=contents:42-further-analysis href=#42-further-analysis>4.2. Further Analysis</a></li></ol></li><li><a id=contents:5-conclusion href=#5-conclusion>5. Conclusion</a></li><li><a id=contents:supplementary-material href=#supplementary-material>Supplementary Material</a><ol><li><a id=contents:a-pseudo-code-for-lds--fds href=#a-pseudo-code-for-lds--fds>A. Pseudo Code for LDS & FDS</a></li><li><a id=contents:b-details-of-dir-datasets href=#b-details-of-dir-datasets>B. Details of DIR Datasets</a></li><li><a id=contents:c-experimental-settings href=#c-experimental-settings>C. Experimental Settings</a></li><li><a id=contents:d-additional-results href=#d-additional-results>D. Additional Results</a></li><li><a id=contents:e-further-analysis-and-ablation-studies href=#e-further-analysis-and-ablation-studies>E. Further Analysis and Ablation Studies</a><ol><li><a id=contents:e1-kernel-type-for-lds--fds href=#e1-kernel-type-for-lds--fds>E.1. Kernel Type for LDS & FDS</a></li><li><a id=contents:e2-training-loss-for-lds--fds href=#e2-training-loss-for-lds--fds>E.2. Training Loss for LDS & FDS</a></li><li><a id=contents:e3-hyper-parameters-for-lds--fds href=#e3-hyper-parameters-for-lds--fds>E.3. Hyper-parameters for LDS & FDS</a></li></ol></li></ol></li></ol></nav><div class="post-body e-content"><p>éå¯¹ç…§ç¿»è¯‘ï¼Œæœ‰æ‰€ç®€ç•¥ã€‚ç¿»è¯‘ä¸å¯¹ï¼Œå°½æƒ…è°…è§£ï¼Œå¯ç•™è¨€
å› ä¸ºhaloæ¸²æŸ“çš„åŸå› ï¼Œæœ‰äº›åœ°æ–¹å¯èƒ½ä¸å¯¹ï¼Œè¯·ç•™è¨€
<a href=https://zhuanlan.zhihu.com/p/369627086 target=_blank rel=noopener>ä½œè€…è§£é‡Š</a> and <a href=https://arxiv.org/abs/2102.09554 target=_blank rel=noopener>paper</a></p><p><a href=https://www.keepjolly.com/archives/dir-note target=_blank rel=noopener>è‡ªå·±åšçš„ç¬”è®°</a>
<a name=U6BAc></a></p><h2 id=abstract><a href=#abstract class=anchor-link>#</a><a href=#contents:abstract class=headings>Abstract</a></h2><p>â€ƒâ€ƒReal-world data often exhibit imbalanced distributions, where certain target values have significantly fewer observations. Existing techniques for dealing with imbalanced data focus on targets with categorical indices, i.e., different classes. However, many tasks involve continuous targets, where hard boundaries between classes do not exist. We define Deep Imbalanced Regression (DIR) as learning from such imbalanced data with continuous targets, dealing with potential missing data for certain target values, and generalizing to the entire target range. Motivated by the intrinsic difference between categorical and continuous label space, we propose distribution smoothing for both labels and features, which explicitly acknowledges the effects of nearby targets, and calibrates both label and learned feature distributions. We curate and benchmark large-scale DIR datasets from common real-world tasks in computer vision, natural language processing, and healthcare domains. Extensive experiments verify the superior performance of our strategies. Our work fills the gap in benchmarks and techniques for practical imbalanced regression problems. Code and data are available at: <a href=https://github.com/YyzHarry/imbalanced-regression target=_blank rel=noopener>https://github.com/YyzHarry/imbalanced-regression</a>.<br>â€ƒâ€ƒç°å®ä¸–ç•Œçš„æ•°æ®å¾€å¾€æ˜¯ä¸å¹³è¡¡åˆ†å¸ƒï¼Œå…¶ä¸­æŸäº›targetå€¼çš„è§‚æµ‹æ•°æ®å¾ˆå°‘ã€‚å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„ç°å­˜æ–¹æ³•éƒ½ä¾§é‡äºå…·æœ‰åˆ†ç±»ç´¢å¼•çš„targetï¼ˆå·²ç»æ ‡å¥½æ•°æ®çš„æ•°æ®é›†ï¼Ÿæ•°æ®éƒ½æ ‡å‡ºäº†ç±»åˆ«ï¼‰ï¼Œå¦‚ä¸åŒçš„ç±»åˆ«ã€‚ç„¶è€Œï¼Œå¾ˆå¤šä»»åŠ¡æ¶‰åŠåˆ°è¿ç»­ç›®æ ‡ï¼Œå…¶ä¸­ç±»ä¹‹é—´ä¸å­˜åœ¨æ¸…æ™°è¾¹ç•Œã€‚æˆ‘ä»¬ç§°æ·±åº¦ä¸å¹³è¡¡å›å½’ï¼ˆDIRï¼‰ä¸ºä»æ­¤ç±»å«è¿ç»­ç›®æ ‡çš„ä¸å¹³è¡¡æ•°æ®ä¸­å­¦ä¹ ï¼Œå¤„ç†æŸäº›targetå€¼çš„æ½œåœ¨ç¼ºå¤±å€¼ï¼Œå¹¶æ³›åŒ–åˆ°æ•´ä¸ªtargetèŒƒå›´ã€‚å—åˆ†ç±»å’Œè¿ç»­æ ‡ç­¾ç©ºé—´ä¹‹é—´å›ºæœ‰å·®å¼‚çš„æ¿€å‘ï¼Œæˆ‘ä»¬å»ºè®®å¯¹æ ‡ç­¾å’Œç‰¹å¾çš„è¿›è¡Œå¹³æ»‘åˆ†å¸ƒï¼Œè¿™æ‰¿è®¤äº†ä¸´è¿‘ç›®æ ‡çš„å½±å“ï¼Œå¹¶æ ¡å‡†æ ‡ç­¾å’Œå­¦ä¹ åˆ°çš„ç‰¹å¾åˆ†å¸ƒã€‚æˆ‘ä»¬ä»è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’ŒåŒ»ç–—é¢†åŸŸçš„å¸¸è§ç°å®ä¸–ç•Œä»»åŠ¡ä¸­è¯„ä¼°è¯¥å¤§å‹ DIR æ•°æ®é›†ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬ç­–ç•¥çš„å“è¶Šæ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œå¡«è¡¥äº†å®é™…ä¸­çš„ä¸å¹³è¡¡å›å½’é—®é¢˜çš„åŸºå‡†æ•°æ®é›†å’ŒæŠ€æœ¯æ–¹é¢çš„ç©ºç™½ã€‚</p><p><a name=cH3xw></a></p><h2 id=1-introduction><a href=#1-introduction class=anchor-link>#</a><a href=#contents:1-introduction class=headings>1. Introduction</a></h2><p>â€ƒâ€ƒData imbalance is ubiquitous and inherent in the real world. Rather than preserving an ideal uniform distribution over each category, the data often exhibit <a href=https://www.statisticshowto.com/probability-and-statistics/skewed-distribution/ target=_blank rel=noopener>skewed distributions</a> with a long tail (<a href="https://arxiv.org/abs/1710.05381?context=cs.AI" target=_blank rel=noopener>Buda et al., 2018</a>; <a href=https://arxiv.org/abs/1904.05160 target=_blank rel=noopener>Liu et al., 2019</a>), where certain target values have significantly fewer observations. This phenomenon poses great challenges for deep recognition models, and has motivated many prior techniques for addressing data imbalance (<a href=https://arxiv.org/abs/1906.07413 target=_blank rel=noopener>Cao et al., 2019</a>; <a href=https://arxiv.org/abs/1901.05555 target=_blank rel=noopener>Cui et al., 2019</a>; <a href=https://arxiv.org/abs/1806.00194 target=_blank rel=noopener>Huang et al., 2019</a>; <a href=https://arxiv.org/abs/1904.05160 target=_blank rel=noopener>Liu et al., 2019</a>; <a href=https://arxiv.org/abs/2009.12991 target=_blank rel=noopener>Tang et al., 2020</a>).<br>â€ƒâ€ƒåœ¨ç°å®ä¸­ï¼Œæ•°æ®çš„ä¸å¹³è¡¡æ˜¯å¸¸è§ä¸”å›ºæœ‰çš„ã€‚ä¸åœ¨æ¯ä¸ªç±»åˆ«ä¸Šä¿æŒç†æƒ³çš„å‡åŒ€åˆ†å¸ƒä¸åŒï¼Œæ•°æ®ç»å¸¸è¡¨ç°å‡ºå¸¦æœ‰é•¿å°¾çš„åæ€åˆ†å¸ƒï¼Œå…¶ä¸­æŸäº›ç›®æ ‡å€¼çš„è§‚æµ‹å€¼ååˆ†å°‘ã€‚è¯¥ç°è±¡å¯¹æ·±åº¦è¯†åˆ«æ¨¡å‹æå‡ºäº†å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå¹¶æ¿€å‘è®¸å¤šç°æœ‰æŠ€æœ¯å»å¤„ç†æ•°æ®ä¸å¹³è¡¡ã€‚</p><p>â€ƒâ€ƒExisting solutions for learning from imbalanced data, however, focus on targets with categorical indices, i.e., the targets are different classes. However, many real-world tasks involve continuous and even infinite target values. For example, in vision applications, one needs to infer the age of different people based on their visual appearances, where age is a continuous target and can be highly imbalanced. Treating different ages as distinct classes is unlikely to yield the best results because it does not take advantage of the similarity between people with nearby ages. Similar issues happen in medical applications since many health metrics including heart rate, blood pressure, and oxygen saturation, are continuous and often have skewed distributions across patient populations.<br>â€ƒâ€ƒç„¶è€Œï¼Œç°æœ‰ä»ä¸å¹³è¡¡æ•°æ®ä¸­å­¦ä¹ çš„æ–¹æ¡ˆä¾§é‡äºå¸¦åˆ†ç±»ç´¢å¼•çš„ç›®æ ‡ï¼Œå¦‚ç›®æ ‡æ˜¯ä¸åŒçš„ç±»åˆ«ï¼ˆæœ‰å¤šä¸ªç›®æ ‡è¿˜æ˜¯ç›®æ ‡æœ‰å¤šä¸ªlabelå€¼ï¼Ÿåº”è¯¥æ˜¯æ¯ä¸ªç›®æ ‡éƒ½æ ‡å‡ºäº†ç‹¬ç«‹ç±»åˆ«ï¼‰ã€‚ä½†ï¼Œè®¸å¤šç°å®ä»»åŠ¡æ¶‰åŠè¿ç»­ä¸”æ— é™çš„ç›®æ ‡å€¼ã€‚å¦‚ï¼Œåœ¨è§†è§‰åº”ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®ä»–ä»¬çš„å¤–è²Œæ¥æ¨æ–­ä»–ä»¬çš„å¹´é¾„ï¼Œå…¶ä¸­å¹´é¾„æ˜¯è¿ç»­å€¼å¹¶ä¸”å¯èƒ½é«˜åº¦ä¸å¹³è¡¡ã€‚æŠŠä¸åŒå¹´é¾„è§†ä¸ºç‹¬ç«‹ç±»åˆ«ä¸å¤ªå¯èƒ½äº§ç”Ÿæœ€ä½³ç»“æœï¼Œå› ä¸ºå®ƒæ²¡æœ‰åˆ©ç”¨å¹´é¾„ç›¸è¿‘çš„äººçš„ç›¸ä¼¼æ€§ã€‚ç±»ä¼¼çš„é—®é¢˜ä¹Ÿå‘ç”Ÿåœ¨åŒ»ç–—åº”ç”¨ä¸­ï¼Œå› ä¸ºåŒ…æ‹¬å¿ƒç‡ã€è¡€å‹å’Œè¡€æ°§é¥±å’Œåº¦åœ¨å†…çš„è®¸å¤šå¥åº·æŒ‡æ ‡æ˜¯è¿ç»­çš„ï¼Œå¹¶ä¸”åœ¨æ‚£è€…ç¾¤ä½“ä¸­é€šå¸¸æ˜¯åæ€åˆ†å¸ƒã€‚</p><p><img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758.png alt=alt><span class=caption>â— Figure 1. Deep Imbalanced Regression (DIR) aims to learn from imbalanced data with continuous targets, tackle potential missing data for certain regions, and generalize to the entire target range.å›¾ä¸€Â  Â DIRæ—¨åœ¨ä»å¸¦è¿ç»­ç›®æ ‡çš„ä¸å¹³è¡¡æ•°æ®ä¸­å­¦ä¹ ï¼Œè§£å†³æŸäº›åŒºåŸŸçš„æ½œåœ¨ç¼ºå¤±æ•°æ®å¹¶æ³›åŒ–åˆ°æ•´ä¸ªç›®æ ‡èŒƒå›´</span></p><style type=text/css rel=stylesheet>figure{font-size:13px;font-weight:400;color:#8c8c8c;text-align:center}</style><figure>Figure 1. Deep Imbalanced Regression (DIR) aims to learn from imbalanced data with continuous targets, tackle potential missing data for certain regions, and generalize to the entire target range.<p>å›¾ä¸€Â  Â DIRæ—¨åœ¨ä»å¸¦è¿ç»­ç›®æ ‡çš„ä¸å¹³è¡¡æ•°æ®ä¸­å­¦ä¹ ï¼Œè§£å†³æŸäº›åŒºåŸŸçš„æ½œåœ¨ç¼ºå¤±æ•°æ®å¹¶æ³›åŒ–åˆ°æ•´ä¸ªç›®æ ‡èŒƒå›´</figure></p><p>â€ƒâ€ƒIn this work, we systematically investigate Deep Imbalanced Regression (DIR) arising in real-world settings (see Fig. 1). We define DIR as learning continuous targets from natural imbalanced data, dealing with potentially missing data for certain target values, and generalizing to a test set that is balanced over the entire range of continuous target values. This definition is analogous to the class imbalance problem (<a href=https://arxiv.org/abs/1904.05160 target=_blank rel=noopener>Liu et al., 2019</a>), but focuses on the continuous setting.<br>â€ƒâ€ƒåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†åœ¨ç°å®ä¸­å‡ºç°çš„DIRã€‚æˆ‘ä»¬æŠŠDIRå®šä¹‰ä¸ºä»è‡ªç„¶çš„ä¸å¹³è¡¡æ•°æ®ä¸­å­¦ä¹ åˆ°è¿ç»­çš„ç›®æ ‡ï¼Œç„¶åå¤„ç†æŸäº›ç›®æ ‡å€¼æ½œåœ¨ç¼ºå¤±æ•°æ®ï¼Œå¹¶æ³›åŒ–åˆ°æ•´ä¸ªè¿ç»­ç›®æ ‡å€¼èŒƒå›´å†…æ˜¯å¹³è¡¡çš„æµ‹è¯•é›†ã€‚è¿™ä¸ªå®šä¹‰ç±»ä¼¼äºç±»ä¸å¹³è¡¡é—®é¢˜ï¼Œä½†ä¾§é‡äºè¿ç»­ã€‚</p><p>â€ƒâ€ƒDIR brings new challenges distinct from its classification counterpart. First, given continuous (potentially infinite) target values, the hard boundaries between classes no longer exist, causing ambiguity when directly applying traditional imbalanced classification methods such as re-sampling and re-weighting. Moreover, continuous labels inherently possess a meaningful distance between targets, which has implication for how we should interpret data imbalance. For example, say two target labels $t1$ and $t2$ have a small number of observations in training data. However, $t1$ is in a highly represented neighborhood (i.e., there are many samples in the range $\left [ t1- \bigtriangleup , t1+ \bigtriangleup \right ]$), while $t2$ is in a weakly represented neighborhood. In this case,$t1$ does not suffer from the same level of imbalance as $t2$. Finally, unlike classification, certain target values may have no data at all, which motivates the need for target extrapolation & interpolation.
â€ƒâ€ƒDIRå¸¦æ¥äº†ä¸åŒäºå…¶å®ƒåˆ†ç±»ä»»åŠ¡çš„æ–°æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œç»™å®šäº†è¿ç»­ï¼ˆå¯èƒ½æ— é™çš„ï¼‰ç›®æ ‡å€¼ï¼Œç±»é—´çš„è¾¹ç•Œä¸å†å­˜åœ¨ï¼Œä»è€Œç›´æ¥ä½¿ç”¨ä¼ ç»Ÿçš„ä¸å¹³è¡¡åˆ†ç±»æ–¹æ³•ï¼ˆå¦‚é‡é‡‡æ ·å’Œé‡åŠ æƒï¼‰æ—¶ä¼šå¯¼è‡´æ­§ä¹‰ã€‚æ­¤å¤–ï¼Œè¿ç»­æ ‡ç­¾åœ¨ç›®æ ‡ä¹‹é—´æœ¬èº«å…·æœ‰æœ‰æ„ä¹‰çš„distanceï¼Œè¿™å¯¹æˆ‘ä»¬å¦‚ä½•è§£é‡Šæ•°æ®ä¸å¹³è¡¡æœ‰å½±å“ã€‚å¦‚ï¼Œå‡è®¾ä¸¤ä¸ªç›®æ ‡æ ‡ç­¾$t1$å’Œ$t2$åœ¨è®­ç»ƒé›†ä¸­åªæœ‰å°éƒ¨åˆ†çš„è§‚æµ‹å€¼ã€‚ç„¶è€Œ$t1$åœ¨ä¸€ä¸ªç›¸å½“é«˜çš„è¡¨ç¤ºåŸŸå†…ï¼ˆå³ï¼Œåœ¨$\left [ t1- \bigtriangleup , t1+ \bigtriangleup \right ]$ å†…æœ‰è®¸å¤šæ ·æœ¬ï¼Œè€Œ$t2$åœ¨ä¸€ä¸ªä½çš„è¡¨ç¤ºåŸŸå†…ï¼‰åœ¨è¿™ç§ä¾‹å­ä¸‹ï¼Œ$t1$ä¸ä¼šé­å—ä¸$t2$ç›¸åŒç¨‹åº¦çš„ä¸å¹³è¡¡ã€‚æœ€åï¼Œä¸åŒäºåˆ†ç±»ä»»åŠ¡ï¼ŒæŸäº›ç›®æ ‡å€¼å¯èƒ½æ ¹æœ¬æ²¡æœ‰æ•°æ®ï¼Œè¿™æ¿€å‘äº†å¯¹ç›®æ ‡extrapolation å’Œ interpolationçš„éœ€æ±‚ã€‚</p><p>â€ƒâ€ƒIn this paper, we propose two simple yet effective methods for addressing DIR: label distribution smoothing (LDS) and feature distribution smoothing (FDS). A key idea underlying both approaches is to leverage the similarity between nearby targets by employing a kernel distribution to perform explicit distribution smoothing in the label and feature spaces. Both techniques can be easily embedded into existing deep networks and allow optimization in an end-to-end fashion. We verify that our techniques not only successfully calibrate for the intrinsic underlying imbalance, but also provide large and consistent gains when combined with other methods. To support practical evaluation of imbalanced regression, we curate and benchmark large-scale DIR datasets for common real-world tasks in computer vision, natural language processing, and healthcare. They range from single-value prediction such as age, text similarity score, health condition score, to dense-value prediction such as depth. We further set up benchmarks for proper DIR performance evaluation.<br>â€ƒâ€ƒåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸¤ä¸ªç®€å•é«˜æ•ˆçš„æ–¹æ³•æ¥è§£å†³DIRï¼šæ ‡ç­¾åˆ†å¸ƒå¹³æ»‘å’Œç‰¹å¾åˆ†å¸ƒå¹³æ»‘ã€‚ä¸¤ä¸ªæ–¹æ³•çš„å…³é”®æ€æƒ³æ˜¯é€šè¿‡ä½¿ç”¨æ ¸åˆ†å¸ƒåœ¨æ ‡ç­¾å’Œç‰¹å¾ç©ºé—´ä¸­æ‰§è¡Œæ˜¾å¼çš„åˆ†å¸ƒå¹³æ»‘æ¥åˆ©ç”¨ä¸´è¿‘ç›®æ ‡é—´çš„ç›¸ä¼¼æ€§ã€‚æ¯ä¸ªæ–¹æ³•éƒ½èƒ½è½»æ˜“åµŒå…¥åˆ°ç°åœ¨çš„æ·±åº¦ç½‘ç»œä¸­å¹¶å…è®¸ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æˆåŠŸåœ°æ ¡å‡†äº†å›ºæœ‰çš„æ½œåœ¨ä¸å¹³è¡¡ï¼Œè€Œä¸”ä¸å…¶ä»–æ–¹æ³•ç»“åˆæ—¶æä¾›äº†å·¨å¤§è€Œä¸€è‡´çš„æ”¶ç›Šã€‚ä¸ºäº†æ”¯æŒä¸å¹³è¡¡å›å½’çš„å®é™…è¯„ä¼°ï¼Œæˆ‘ä»¬ä¸ºè®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’ŒåŒ»ç–—ä¸­çš„å¸¸è§ç°å®ä»»åŠ¡ä¸­æ•´ç†äº†å’ŒåŸºå‡†æµ‹è¯•äº†å¤§è§„æ¨¡ DIR æ•°æ®é›†ã€‚è¿™äº›æ•°æ®é›†ä»å•å€¼é¢„æµ‹ï¼ˆå¦‚å¹´é¾„ï¼Œæ–‡æœ¬ç›¸ä¼¼åº¦å¾—åˆ†å’Œå¥åº·çŠ¶å†µå¾—åˆ†ï¼‰åˆ°å¯†é›†å€¼çš„é¢„æµ‹ï¼ˆå¦‚æ·±åº¦ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ä¸ºåˆé€‚çš„DIRæ€§èƒ½è¯„ä¼°å»ºç«‹äº†åŸºå‡†ã€‚</p><p>Our contributions are as follows:</p><ul><li>We formally define the DIR task as learning from imbalanced data with continuous targets, and generalizing to the entire target range. DIR provides thorough and unbiased evaluation of learning algorithms in practical settings.</li><li>We develop two simple, effective, and interpretable algorithms for DIR, LDS and FDS, which exploit the similarity between nearby targets in both label and feature space.</li><li>We curate benchmark DIR datasets in different domains: computer vision, natural language processing, and healthcare. We set up strong baselines as well as benchmarks for proper DIR performance evaluation.</li><li>Extensive experiments on large-scale DIR datasets verify the consistent and superior performance of our strategies.</li></ul><p>æˆ‘ä»¬çš„è´¡çŒ®å¦‚ä¸‹ï¼š</p><ul><li>æˆ‘ä»¬å°†DIRä»»åŠ¡å®šä¹‰ä¸ºä»å¸¦è¿ç»­ç›®æ ‡çš„ä¸å¹³è¡¡æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶æ³›åŒ–åˆ°æ•´ä¸ªç›®æ ‡èŒƒå›´å†…ã€‚DIRåœ¨å®é™…ç¯å¢ƒä¸­å¯¹å­¦ä¹ ç®—æ³•è¿›è¡Œå®Œå…¨çš„å…¬æ­£çš„è¯„ä¼°ã€‚</li><li>æˆ‘ä»¬ä¸ºDIRã€LDSå’ŒFDSè®¾è®¡äº†ä¸¤ä¸ªç®€å•é«˜æ•ˆä¸”å¯è§£é‡Šçš„ç®—æ³•ï¼Œç®—æ³•åˆ©ç”¨äº†æ ‡ç­¾å’Œç‰¹å¾ç©ºé—´ä¸­ä¸´è¿‘ç›®æ ‡çš„ç›¸ä¼¼æ€§</li><li>æˆ‘ä»¬åœ¨ä¸åŒé¢†åŸŸç®¡ç†DIRåŸºå‡†æ•°æ®é›†ã€‚æˆ‘ä»¬ä¸ºDIRæ€§èƒ½è¯„ä¼°å»ºç«‹äº†å¼ºå¤§çš„åŸºçº¿å’ŒåŸºå‡†ã€‚</li><li>åœ¨å¤§è§„æ¨¡DIRæ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•çš„ä¸€è‡´æ€§å’Œå“è¶Šçš„æ€§èƒ½ã€‚
<a name=iAF2C></a></li></ul><h2 id=2-related-work><a href=#2-related-work class=anchor-link>#</a><a href=#contents:2-related-work class=headings>2. Related Work</a></h2><p>â€ƒâ€ƒ<strong>Imbalanced Classification.</strong> Much prior work has focused on the imbalanced classification problem (also referred to as long-tailed recognition (<a href=https://arxiv.org/abs/1904.05160 target=_blank rel=noopener>Liu et al., 2019</a>)). Past solutions can be divided into data-based and model-based solutions: Data-based solutions either over-sample the minority class or under-sample the majority (<a href=https://www.jair.org/index.php/jair/article/view/10302 target=_blank rel=noopener>Chawla et al., 2002</a>; <a href=https://ieeexplore.ieee.org/document/6793456 target=_blank rel=noopener>GarcÂ´Ä±a & Herrera, 2009</a>; <a href=https://ieeexplore.ieee.org/document/4633969 target=_blank rel=noopener>He et al., 2008</a>). For example, SMOTE generates synthetic samples for minority classes by linearly interpolating samples in the same class (<a href=https://www.jair.org/index.php/jair/article/view/10302 target=_blank rel=noopener>Chawla et al., 2002</a>). Model-based solutions include re-weighting or adjusting the loss function to compensate for class imbalance (<a href=https://arxiv.org/abs/1906.07413 target=_blank rel=noopener>Cao et al., 2019</a>; <a href=https://arxiv.org/abs/1901.05555 target=_blank rel=noopener>Cui et al., 2019</a>; <a href=https://arxiv.org/abs/1804.10851 target=_blank rel=noopener>Dong et al., 2019</a>; <a href=https://ieeexplore.ieee.org/document/7780949 target=_blank rel=noopener>Huang et al., 2016</a>; <a href=https://arxiv.org/abs/1806.00194 target=_blank rel=noopener>2019</a>), and leveraging relevant learning paradigms, including transfer learning (<a href=https://arxiv.org/abs/1803.09014 target=_blank rel=noopener>Yin et al., 2019</a>), metric learning (<a href=https://arxiv.org/abs/1611.08976 target=_blank rel=noopener>Zhang et al., 2017</a>), meta-learning (<a href=https://arxiv.org/abs/1902.07379 target=_blank rel=noopener>Shu et al., 2019</a>), and two-stage training (<a href=https://arxiv.org/abs/1910.09217 target=_blank rel=noopener>Kang et al., 2020</a>). Recent studies have also discovered that semi-supervised learning and selfsupervised learning lead to better imbalanced classification results (<a href="https://arxiv.org/abs/2006.07529?amp=1" target=_blank rel=noopener>Yang & Xu, 2020</a>). In contrast to these past work, we identify the limitations of applying class imbalance methods to regression problems, and introduce new techniques particularly suitable for learning continuous target values.
â€ƒâ€ƒä¸å¹³è¡¡åˆ†ç±»ã€‚å¤§é‡çš„å…ˆå‰å·¥ä½œéƒ½é›†ä¸­äºä¸å¹³è¡¡åˆ†ç±»é—®é¢˜ï¼ˆä¹Ÿç§°ä¸ºé•¿å°¾è¯†åˆ«é—®é¢˜ï¼‰ä¸Šã€‚è¿‡å»çš„è§£å†³æ–¹æ¡ˆå¯ä»¥åˆ†ä¸ºåŸºäºæ•°æ®å’ŒåŸºäºæ¨¡å‹ï¼šåŸºäºæ•°æ®çš„æ–¹æ¡ˆè¦ä¹ˆåœ¨å°‘æ•°ç±»ä¸Šè¿‡é‡‡æ ·æˆ–åœ¨å¤§å¤šæ•°ä¸Šç¼ºé‡‡æ ·ã€‚å¦‚ï¼ŒSMOTEä¸ºå°‘æ•°ç±»åˆ«ç”Ÿæˆäººé€ æ ·æœ¬é€šè¿‡åœ¨ç›¸åŒç±»åˆ«çš„æ ·æœ¬ä¸­çº¿æ€§æ’å€¼ã€‚åŸºäºæ¨¡å‹çš„æ–¹æ¡ˆåŒ…æ‹¬é‡åŠ æƒæˆ–è°ƒæ•´æŸå¤±å‡½æ•°æ¥å¼¥è¡¥ç±»åˆ«ä¸å¹³è¡¡ï¼ˆyoloçš„å¤§å°ç›®æ ‡çš„è¶…å‚æ•°ï¼‰ï¼Œå¹¶åˆ©ç”¨ç›¸å…³çš„å­¦ä¹ èŒƒå¼ï¼ŒåŒ…æ‹¬è¿ç§»å­¦ä¹ ã€åº¦é‡å­¦ä¹ ã€å…ƒå­¦ä¹ å’ŒäºŒé˜¶æ®µè®­ç»ƒã€‚æœ€è¿‘ç ”ç©¶ä¹Ÿå‘ç°åŠç›‘ç£å­¦ä¹ å’Œè‡ªç›‘ç£å­¦ä¹ ä¼šäº§ç”Ÿä¸å¹³è¡¡åˆ†ç±»é—®é¢˜çš„å¥½ç»“æœã€‚ä¸è¿‡å»å·¥ä½œç›¸æ¯”ï¼Œæˆ‘ä»¬å‘ç°åœ¨å›å½’é—®é¢˜ä¸Šåº”ç”¨ç±»åˆ«ä¸å¹³è¡¡æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶å¼•å…¥ç‰¹åˆ«é€‚åˆå­¦ä¹ è¿ç»­ç›®æ ‡å€¼çš„æ–°æ–¹æ³•ã€‚</p><p>â€ƒâ€ƒ<strong>Imbalanced Regression.</strong> Regression over imbalanced data is not as well explored. Most of the work on this topic is a direct adaptation of the SMOTE algorithm to regression scenarios (Branco et al., 2017; 2018; Torgo et al., 2013). Synthetic samples are created for pre-defined rare target regions by either directly interpolating both inputs and targets (Torgo et al., 2013), or using Gaussian noise augmentation (Branco et al., 2017). A bagging-based ensemble method that incorporates multiple data pre-processing steps has also been introduced (Branco et al., 2018). However, there exist several intrinsic drawbacks for these methods. First, they fail to take the distance between targets into account, and rather heuristically divide the dataset into rare and frequent sets, then plug in classification-based methods. Moreover, modern data is of extremely high dimension (e.g., images and physiological signals); linear interpolation of two samples of such data does not lead to meaningful new synthetic samples. Our methods are intrinsically different from past work in their approach. They can be combined with existing methods to improve their performance, as we show in Sec. 4. Further, our approaches are tested on large-scale real-world datasets in computer vision, NLP, and healthcare.<br>â€ƒâ€ƒä¸å¹³è¡¡å›å½’ã€‚åœ¨ä¸å¹³è¡¡æ•°æ®ä¸Šå›å½’ä¹Ÿæ²¡æœ‰å¾ˆå¥½çš„æ¢ç´¢è¿‡ã€‚å¤§å¤šæ•°çš„å·¥ä½œæ˜¯SMOTEç®—æ³•çš„ç›´æ¥è°ƒæ•´åˆ°å›å½’åœºæ™¯ã€‚é€šè¿‡ç›´æ¥åœ¨è¾“å…¥å’Œç›®æ ‡ä¸Šæ’å€¼æˆ–ä½¿ç”¨é«˜æ–¯å™ªå£°å¢å¼ºæŠ€æœ¯ï¼Œæ¥ä¸ºé¢„å®šä¹‰çš„ç¨€å°‘ç›®æ ‡åŒºåŸŸï¼ˆæ ·æœ¬å¾ˆå°‘çš„åœ°æ–¹ï¼‰åˆ›é€ äººé€ æ ·æœ¬ã€‚è¿˜å¼•å…¥ä¸€ä¸ªåŸºäºbaggingçš„é›†æˆæ–¹æ³•ï¼Œå®ƒåŒ…å«å¤šä¸ªæ•°æ®é¢„å¤„ç†æ­¥éª¤ã€‚ä½†æ˜¯ï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨å‡ ä¸ªå›ºæœ‰çš„ç¼ºé™·ã€‚é¦–å…ˆï¼Œå®ƒä»¬æ²¡æœ‰è€ƒè™‘ç›®æ ‡é—´çš„distanceï¼Œè€Œæ˜¯å¯å‘å¼åœ°æŠŠæ•°æ®é›†åˆ†æˆrareé›†å’Œfrequenté›†ï¼Œç„¶åæ’å…¥åŸºäºåˆ†ç±»çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæé«˜ç»´åº¦çš„ç°ä»£æ•°æ®ï¼ˆå¦‚ï¼Œå›¾ç‰‡å’Œç”Ÿç†ä¿¡å·ï¼‰å¯¹æ­¤ç±»æ•°æ®çš„ä¸¤ä¸ªæ ·æœ¬è¿›è¡Œçº¿æ€§æ’å€¼ä¸ä¼šäº§ç”Ÿæœ‰æ„ä¹‰çš„æ–°åˆæˆæ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ¬è´¨ä¸ŠåŒå…ˆå‰æ–¹æ³•ä¸åŒã€‚å®ƒä»¬å¯ä»¥ç»“åˆç°æœ‰æ–¹æ³•æ¥æé«˜æ€§èƒ½ï¼Œå¦‚Sec.4æ‰€ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰ã€NLPå’ŒåŒ»ç–—çš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šæµ‹è¯•è¿‡ã€‚
<a name=pchzD></a></p><h2 id=3-methods><a href=#3-methods class=anchor-link>#</a><a href=#contents:3-methods class=headings>3. Methods</a></h2><p>â€ƒâ€ƒ<strong>Problem Setting.</strong> $<code> \left\{\left(x_i,y_i\right)\right\}_{i=1}^N</code>$ be a training set, where$<code>x_i\in \mathbb{R} ^d </code>$ denotes the input and$y_i\in \mathbb{R}$ is the label, which is a continuous target. We introduce an additional structure for the label space $\mathcal{Y}$ , where we divide $\mathcal{Y}$ into $B$ <a href=https://datavizguru.com/tableau/groups-vs-sets-vs-bins-vs-parameters/ target=_blank rel=noopener>groups (bins)</a> with equal intervals, i.e., $\left [ y_0, y_1\right ), \left [ y_1, y_2\right ), \cdots , \left [ y_{B-1}, y_B\right )$. Throughout the paper, we use $b \in \mathcal{B}$ to denote the group index of the target value, where $\mathcal{B} = \left { 1, \dots , B\right } \subset \mathbb{Z}^+$ is the index space. In practice, the defined bins reflect a minimum resolution we care for grouping data in a regression task. For instance, in age estimation, we could define $\delta y \triangleq y_{b+1}-y_{b}=1$, showing a minimum age difference of 1 is of interest. Finally, we denote $\mathrm {z} =f\left ( x;\theta \right )$as the feature for $\mathrm {x}$, where $f\left ( x;\theta \right )$ is parameterized by a deep neural network model with parameter $\theta$. The final prediction $\hat{y}$ is given by a regression function $g(\cdot)$ that operates over $\mathbf{ \mathrm {z}}$.<br>â€ƒâ€ƒé—®é¢˜è®¾ç½®ã€‚è®© $\left { \left ( x_i, y_i \right ) \right }^N_{i=1}$ä½œä¸ºè®­ç»ƒé›†ï¼Œå…¶ä¸­$x_i\in \mathbb{R} ^d$è¡¨ç¤ºè¾“å…¥ï¼Œ$y_i\in \mathbb{R}$è¡¨ç¤ºæ ‡ç­¾ï¼Œæ˜¯è¿ç»­ç›®æ ‡å€¼ã€‚æˆ‘ä»¬ä¸ºæ ‡ç­¾ç©ºé—´$\mathcal{Y}$å¼•å…¥é¢å¤–çš„ç»“æ„ï¼Œå…¶ä¸­æˆ‘ä»¬æŠŠ$\mathcal{Y}$åˆ†æˆç­‰é—´éš”çš„Bç»„ï¼ˆç®±ï¼‰ï¼Œå³$\left [ y_0, y_1\right ), \left [ y_1, y_2\right ), \cdots , \left [ y_{B-1}, y_B\right )$ã€‚åœ¨æ•´ä¸ªæ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨$b \in \mathcal{B}$è¡¨ç¤ºç›®æ ‡å€¼çš„ç»„ç´¢å¼•ï¼Œå…¶ä¸­ $\mathcal{B} = \left { 1, \dots , B\right } \subset \mathbb{Z}^+$æ˜¯ç´¢å¼•ç©ºé—´ã€‚å®é™…ä¸Šï¼Œå®šä¹‰çš„binsååº”äº†æˆ‘ä»¬åœ¨å›å½’ä»»åŠ¡ä¸­å¯¹æ•°æ®åˆ†ç»„æ—¶å…³å¿ƒçš„æœ€å°åˆ†è¾¨ç‡ã€‚ä¾‹å¦‚ï¼Œåœ¨å¹´é¾„è¯„ä¼°ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰$\delta y \triangleq y_{b+1}-y_{b}=1$ï¼Œè¡¨æ˜æœ€å°å¹´é¾„å·®ä¸º1æ˜¯æœ‰ç”¨çš„ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æŠŠ $\mathrm {z} =f\left ( x;\theta \right )$è¡¨ç¤ºxçš„ç‰¹å¾ï¼Œå…¶ä¸­$f\left ( x;\theta \right )$æ˜¯é€šè¿‡å…·æœ‰å‚æ•°$\theta$çš„æ·±åº¦ç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„ã€‚æœ€ç»ˆçš„é¢„æµ‹ $\hat{y}$æ˜¯é€šè¿‡ ä¸€ä¸ªåœ¨$\mathbf{ \mathrm {z}}$ä¸Šè¿è¡Œçš„å›å½’å‡½æ•°$g(\cdot)$ç»™å‡ºã€‚
<a name=jWbVx></a></p><h3 id=31-label-distribution-smoothing><a href=#31-label-distribution-smoothing class=anchor-link>#</a><a href=#contents:31-label-distribution-smoothing class=headings>3.1. Label Distribution Smoothing</a></h3><p>â€ƒâ€ƒWe start by showing an example to demonstrate the difference between classification and regression when imbalance comes into the picture.<br><img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-1.png alt=image.png><span class=caption>â— Figure 2. Comparison on the test error distribution (bottom) using same training label distribution (top) on two different datasets: (a) CIFAR-100, a classification task with categorical label space. (b) IMDB-WIKI, a regression task with continuous label space.å›¾äºŒã€‚åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæ ‡ç­¾åˆ†å¸ƒæ¥æ¯”è¾ƒæµ‹è¯•erroråˆ†å¸ƒ(bottom)ï¼š(a)CIFAR-100ï¼Œå…·æœ‰åˆ†ç±»çš„æ ‡ç­¾ç©ºé—´çš„åˆ†ç±»ä»»åŠ¡ã€‚(b)IMDB-WIKIï¼Œå…·æœ‰è¿ç»­æ ‡ç­¾ç©ºé—´çš„å›å½’ä»»åŠ¡</span></p><figure>Figure 2. Comparison on the test error distribution (bottom) using same training label distribution (top) on two different datasets: (a) CIFAR-100, a classification task with categorical label space. (b) IMDB-WIKI, a regression task with continuous label space.<p>å›¾äºŒã€‚åœ¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šä½¿ç”¨ç›¸åŒçš„è®­ç»ƒæ ‡ç­¾åˆ†å¸ƒæ¥æ¯”è¾ƒæµ‹è¯•erroråˆ†å¸ƒ(bottom)ï¼š(a)CIFAR-100ï¼Œå…·æœ‰åˆ†ç±»çš„æ ‡ç­¾ç©ºé—´çš„åˆ†ç±»ä»»åŠ¡ã€‚(b)IMDB-WIKIï¼Œå…·æœ‰è¿ç»­æ ‡ç­¾ç©ºé—´çš„å›å½’ä»»åŠ¡</figure></p><p>â€ƒâ€ƒ<strong>Motivating Example.</strong> We employ two datasets: (1) CIFAR100 (Krizhevsky et al., 2009), which is a 100-class classification dataset, and (2) the IMDB-WIKI dataset (Rothe et al., 2018), which is a large-scale image dataset for age estimation from visual appearance. The two datasets have intrinsically different label space: CIFAR-100 exhibits categorical label space where the target is class index, while IMDB-WIKI has a continuous label space where the target is age. We limit the age range to $0 \sim 99$ so that the two datasets have the same label range, and subsample them to simulate data imbalance, while ensuring they have exactly the same label density distribution (Fig. 2). We make both test sets balanced. We then train a plain ResNet-50 model on the two datasets, and plot their test error distributions.<br>â€ƒâ€ƒæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªæ•°æ®é›†ï¼šCIFAR100ï¼Œå®ƒæ˜¯ä¸€ä¸ª100ç±»çš„åˆ†ç±»æ•°æ®é›†ï¼›IMDB-WIKIï¼Œå®ƒæ˜¯ä»å¤–è²Œæ¥ä¼°è®¡å¹´é¾„çš„å¤§è§„æ¨¡å›¾ç‰‡æ•°æ®é›†ã€‚è¿™ä¸¤ä¸ªæ•°æ®é›†å…·æœ‰å®Œå…¨ä¸åŒçš„æ ‡ç­¾ç©ºé—´ï¼šCIFAR100çš„ç±»åˆ«æ ‡ç­¾ç©ºé—´ä¸­ç›®æ ‡å€¼æ˜¯ç±»åˆ«ç´¢å¼•ï¼Œè€ŒIMDB-WIKIæ˜¯ç›®æ ‡å€¼æ˜¯å¹´é¾„çš„è¿ç»­æ ·æœ¬ç©ºé—´æˆ‘ä»¬é™åˆ¶å¹´é¾„åˆ°0~99ä½¿å¾—ä¸¤ä¸ªæ•°æ®é›†æœ‰ç›¸åŒçš„æ ‡ç­¾èŒƒå›´ï¼Œå¹¶ä¸”ä¸‹é‡‡æ ·æ¥æ¨¡æ‹Ÿæ•°æ®ä¸å¹³è¡¡ï¼ŒåŒæ—¶ä¿æŒå®ƒä»¬å…·æœ‰ç›¸åŒçš„æ ‡ç­¾å¯†åº¦åˆ†å¸ƒï¼ˆå›¾2ï¼‰ã€‚æˆ‘ä»¬ä¿æŒä¸¤ä¸ªæµ‹è¯•é›†å¹³æ»‘ã€‚ç„¶ååœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒplain ResNet-50ï¼Œå¹¶ç»˜åˆ¶å‡ºå®ƒä»¬çš„æµ‹è¯•è¯¯å·®åˆ†å¸ƒã€‚</p><p>â€ƒâ€ƒWe observe from Fig. 2(a) that the error distribution correlates with label density distribution. Specifically, the test error as a function of class index has a high negative Pearson correlation with the label density distribution (i.e., -0.76) in the categorical label space. The phenomenon is expected, as majority classes with more samples are better learned than minority classes. Interestingly however, as Fig. 2(b) shows, the error distribution is very different for IMDB-WIKI with continuous label space, even when the label density distribution is the same as CIFAR-100. In particular, the error distribution is much smoother and no longer correlates well with the label density distribution (-0.47).
â€ƒâ€ƒåœ¨å›¾2ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¯¯å·®åˆ†å¸ƒä¸æ ‡ç­¾å¯†åº¦åˆ†å¸ƒç›¸å…³ã€‚ç‰¹åˆ«çš„ï¼Œä½œä¸ºä¸€ä¸ªç±»åˆ«ç´¢å¼•çš„å‡½æ•°ï¼Œæµ‹è¯•è¯¯å·®ä¸åˆ†ç±»æ ‡ç­¾ç©ºé—´çš„æ ‡ç­¾å¯†åº¦åˆ†å¸ƒä¸­æœ‰ç‰¹åˆ«é«˜çš„è´ŸPearsonç›¸å…³æ€§ï¼ˆå³-0.76ï¼‰ã€‚è¿™ç§ç°è±¡æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºå¤šæ•°ç±»æœ‰æ›´å¤šçš„æ ·æœ¬æ¯”å°‘æ•°ç±»æ›´æ˜“è®­ç»ƒã€‚ä½†æœ‰è¶£çš„æ˜¯ï¼Œå¦‚å›¾2(b)æ‰€ç¤ºï¼Œè¯¯å·®åˆ†å¸ƒåœ¨IMDB-WIKIååˆ†ä¸åŒï¼Œå³ä½¿å½“æ ‡ç­¾å¯†åº¦ç©ºé—´ä¸CIFAR-100ç›¸åŒã€‚ç‰¹åˆ«æ˜¯è¯¯å·®åˆ†å¸ƒæ›´å¹³æ»‘å¹¶ä¸”ä¸å†è·Ÿæ ‡ç­¾å¯†åº¦åˆ†å¸ƒç›¸å…³ï¼ˆ-0.47ï¼‰ã€‚</p><p>â€ƒâ€ƒThe reason why this example is interesting is that all imbalanced learning methods, directly or indirectly, operate by compensating for the imbalance in the empirical label density distribution. This works well for class imbalance, but for continuous labels the empirical density does not accurately reflect the imbalance as seen by the neural network. Hence, compensating for data imbalance based on empirical label density is inaccurate for the continuous label space.
è¿™ä¸ªä¾‹å­çš„åŸå› æ˜¯æ‰€æœ‰ä¸å¹³è¡¡å­¦ä¹ æ–¹æ³•éƒ½ç›´æ¥æˆ–é—´æ¥çš„é€šè¿‡æ”¹å–„ç»éªŒæ ‡ç­¾å¯†åº¦åˆ†å¸ƒçš„ä¸å¹³è¡¡æ¥æ“ä½œã€‚è¿™å¯¹äºç±»ä¸å¹³è¡¡æœ‰æ•ˆï¼Œä½†å¯¹äºè¿ç»­æ ‡ç­¾ï¼Œç»éªŒå¯†åº¦ä¸èƒ½å‡†ç¡®ååº”é€šè¿‡ç¥ç»ç½‘ç»œçœ‹åˆ°çš„ä¸å¹³è¡¡ã€‚å› æ­¤ï¼Œå¯¹äºè¿ç»­æ ‡ç­¾ç©ºé—´ï¼ŒåŸºäºç»éªŒæ ‡ç­¾å¯†åº¦æ¥æ”¹å–„æ•°æ®ä¸å¹³è¡¡æ˜¯ä¸æ­£ç¡®çš„</p><blockquote><p>â€ƒâ€ƒAn empirical distribution is <strong>one for which each possible event is assigned a probability derived from experimental observation</strong>. It is assumed that the events are independent and the sum of the probabilities is 1.ä¹Ÿå°±æ˜¯ç›´æ¥è§‚æµ‹åˆ°çš„æ ‡ç­¾å¯†åº¦ã€‚</p></blockquote><p>â€ƒâ€ƒ<strong>LDS for Imbalanced Data Density Estimation.</strong> The above example shows that, in the continuous case, the empirical label distribution does not reflect the real label density distribution. This is because of the dependence between data samples at nearby labels (e.g., images of close ages). In fact, there is a significant literature in statistics on how to estimate the expected density in such cases (Parzen, 1962). Thus, Label Distribution Smoothing (LDS) advocates the use of kernel density estimation to learn the effective imbalance in datasets that corresponds to continuous targets.
<a href=https://www.cnblogs.com/ljwgis/p/15471550.html target=_blank rel=noopener>ç”¨äºä¸å¹³è¡¡æ•°æ®å¯†åº¦ä¼°è®¡çš„LDS</a>ã€‚ä¸Šè¿°ä¾‹å­è¡¨æ˜ï¼Œåœ¨è¿ç»­æƒ…å†µä¸‹ï¼Œç»éªŒæ ‡ç­¾å¯†åº¦ä¸èƒ½åæ˜ çœŸå®æ ‡ç­¾å¯†åº¦åˆ†å¸ƒã€‚è¿™æ˜¯å› ä¸ºç›¸è¿‘æ ‡ç­¾çš„æ•°æ®æ ·æœ¬ä¹‹é—´å­˜åœ¨ä¾èµ–æ€§ï¼ˆå¦‚å¹´é¾„ç›¸è¿‘çš„å›¾ç‰‡ï¼‰ã€‚äº‹å®ä¸Šï¼Œå…³äºå¦‚ä½•ä¼°è®¡åœ¨è¿™ç§æƒ…å†µä¸‹çš„é¢„æœŸå¯†åº¦ï¼Œç»Ÿè®¡å­¦æœ‰å¤§é‡çš„ç»Ÿè®¡æ–‡çŒ®ã€‚å› æ­¤ï¼ŒLDSæå€¡ä½¿ç”¨æ ¸å¯†åº¦ä¼°è®¡æ¥å­¦ä¹ æ•°æ®é›†å¯¹åº”è¿ç»­æ ‡ç­¾å€¼çš„æœ‰æ•ˆåŒºåŸŸçš„ä¸å¹³è¡¡ã€‚</p><p>â€ƒâ€ƒLDS convolves a symmetric kernel with the empirical density distribution to extract a kernel-smoothed version that accounts for the overlap in information of data samples of nearby labels. A symmetric kernel is any kernel that satisfies: $\mathrm{k}\left(y, y^{\prime}\right)=\mathrm{k}\left(y^{\prime}, y\right)$and$\nabla_{y} \mathrm{k}\left(y, y^{\prime}\right)+\nabla_{y^{\prime}} \mathrm{k}\left(y^{\prime}, y\right)=0 , \forall y, y^{\prime} \in \mathcal{Y}$. Note that a Gaussian or a Laplacian kernel is a symmetric kernel, while $\mathrm{k}\left(y, y^{\prime}\right) = yy^{\prime}$ is not. The symmetric kernel characterizes the similarity between target values $y^{\prime}$ and any $y$ w.r.t. their distance in the target space. Thus, LDS computes the effective label density distribution as:<br>â€ƒâ€ƒLDSå°†å¯¹ç§°æ ¸ä¸ç»éªŒå¯†åº¦åˆ†å¸ƒå·ç§¯æ¥æå–æ ¸å¹³æ»‘ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬è´Ÿè´£ç›¸é‚»æ ‡ç­¾çš„æ•°æ®æ ·æœ¬çš„ä¿¡æ¯é‡å ã€‚ä¸€ä¸ªå¯¹ç§°æ ¸åªè¦æ»¡è¶³$\mathrm{k}\left(y, y^{\prime}\right)=\mathrm{k}\left(y^{\prime}, y\right)$å’Œ$\nabla_{y} \mathrm{k}\left(y, y^{\prime}\right)+\nabla_{y^{\prime}} \mathrm{k}\left(y^{\prime}, y\right)=0 , \forall y, y^{\prime} \in \mathcal{Y}$å°±è¡Œã€‚æ³¨æ„Gaussianæˆ–Laplaianæ ¸æ˜¯å¯¹ç§°æ ¸ï¼Œä½†å½“$\mathrm{k}\left(y, y^{\prime}\right) = yy^{\prime}$æ—¶å°±ä¸æ˜¯ã€‚å¯¹ç§°æ ¸è¡¨ç¤ºç›®æ ‡å€¼y'å’Œä»»ä½•yå…³äº(w.r.t=with respect to)ç›®æ ‡ç©ºé—´çš„è·ç¦»ã€‚å› æ­¤ï¼ŒLDSè®¡ç®—æœ‰æ•ˆçš„æ ‡ç­¾å¯†åº¦åˆ†å¸ƒå…¬å¼ä¸ºï¼š</p><p>$$\tilde{p}\left(y^{\prime}\right) \triangleq \int_{\mathcal{Y}} \mathrm{k}\left(y, y^{\prime}\right) p(y) d y \qquad(1) $$
where $p(y)$ is the number of appearances of label of y in the training data, and $\tilde{p}(y^{\prime} )$ is the effective density of label$y^{\prime}$.
å…¶ä¸­p(y)æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸­yæ ‡ç­¾çš„å‡ºç°æ¬¡æ•°ï¼Œå¹¶ä¸” $\tilde{p}(y^{\prime} )$ æ˜¯æ ‡ç­¾y'çš„æœ‰æ•ˆå¯†åº¦
<img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-2.png alt=image.png><span class=caption>â— Figure 3.Â  Label distribution smoothing (LDS) convolves a symmetric kernel with the empirical label density to estimate the effective label density distribution that accounts for the continuity of labels.Â  Â  Â  Â  Â  Â  Â  Â  å›¾3.æ ‡ç­¾åˆ†å¸ƒå¹³æ»‘LDSï¼Œå°†å¯¹ç§°æ ¸ä¸ç»éªŒæ ‡ç­¾å¯†åº¦è¿›è¡Œå·ç§¯ï¼Œä»¥ä¼°è®¡è¯´æ˜æ ‡ç­¾è¿ç»­æ€§çš„æœ‰æ•ˆçš„æ ‡ç­¾å¯†åº¦åˆ†å¸ƒã€‚</span></p><figure>Figure 3.Â  Label distribution smoothing (LDS) convolves a symmetric kernel with the empirical label density to estimate the effective label density distribution that accounts for the continuity of labels.Â  Â  Â  Â  Â  Â  Â  Â 
å›¾3.æ ‡ç­¾åˆ†å¸ƒå¹³æ»‘LDSï¼Œå°†å¯¹ç§°æ ¸ä¸ç»éªŒæ ‡ç­¾å¯†åº¦è¿›è¡Œå·ç§¯ï¼Œä»¥ä¼°è®¡è¯´æ˜æ ‡ç­¾è¿ç»­æ€§çš„æœ‰æ•ˆçš„æ ‡ç­¾å¯†åº¦åˆ†å¸ƒã€‚</figure><p>â€ƒâ€ƒFig. 3 illustrates LDS and how it smooths the label density distribution. Further, it shows that the resulting label density computed by LDS correlates well with the error distribution (-0.83). This demonstrates that LDS captures the real imbalance that affects regression problems.
â€ƒâ€ƒå›¾3è§£é‡Šäº†LDSå’Œå®ƒå¦‚ä½•å¹³æ»‘æ ‡ç­¾å¯†åº¦åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œå®ƒè¡¨æ˜ç”±LDSè®¡ç®—å¾—åˆ°çš„æ ‡ç­¾å¯†åº¦ä¸è¯¯å·®åˆ†å¸ƒæœ‰å¾ˆå¥½çš„ç›¸å…³æ€§(-0.83)ã€‚è¿™è¯´æ˜äº†LDSæ•è·åˆ°äº†å½±å“å›å½’ä»»åŠ¡çš„çœŸæ­£ä¸å¹³è¡¡ã€‚</p><p>â€ƒâ€ƒNow that the effective label density is available, techniques for addressing class imbalance problems can be directly adapted to the DIR context. For example, a straightforward adaptation can be the cost-sensitive re-weighting method, where we re-weight the loss function by multiplying it by the inverse of the LDS estimated label density for each target. We show in Sec. 4 that LDS can be seamlessly incorporated with a wide range of techniques to boost DIR performance.<br>â€ƒâ€ƒæ—¢ç„¶æœ‰æ•ˆçš„æ ‡ç­¾å¯†åº¦æ˜¯å¯ç”¨çš„ï¼Œä¸ºäº†è§£å†³ç±»ä¸å¹³è¡¡é—®é¢˜çš„æŠ€æœ¯å¯ä»¥ç›´æ¥ç”¨äºDIRç¯å¢ƒã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªç®€å•çš„è°ƒæ•´æ˜¯å…³äºæˆæœ¬æ•æ„Ÿçš„æ–¹æ³•ï¼Œå…¶ä¸­æˆ‘ä»¬é€šè¿‡å°†æŸå¤±å‡½æ•°ä¹˜ä¸Šæ¯ä¸ªç›®æ ‡çš„LDSä¼°è®¡æ ‡ç­¾å¯†åº¦çš„å€’æ•°æ¥é‡æ–°åŠ æƒæŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬åœ¨Sec.4å±•ç¤ºäº†LDSå¯ä»¥æ— ç¼ç»“åˆå¤§é‡çš„æ–¹æ³•æ¥æé«˜DIRçš„æ€§èƒ½ã€‚
<a name=yzgyN></a></p><h3 id=32-feature-distribution-smoothing><a href=#32-feature-distribution-smoothing class=anchor-link>#</a><a href=#contents:32-feature-distribution-smoothing class=headings>3.2. Feature Distribution Smoothing</a></h3><p>â€ƒâ€ƒWe are motivated by the intuition that continuity in the target space should create a corresponding continuity in the feature space. That is, if the model works properly and the data is balanced, one expects the feature statistics corresponding to nearby targets to be close to each other.<br>â€ƒâ€ƒæˆ‘ä»¬è®¤ä¸ºç›®æ ‡ç©ºé—´çš„è¿ç»­æ€§åº”è¯¥åˆ›é€ ä¸€ä¸ªåœ¨ç‰¹å¾ç©ºé—´ç›¸å¯¹åº”çš„çš„è¿ç»­æ€§ã€‚å³ï¼Œå¦‚æœæ¨¡å‹å·¥ä½œæ­£å¸¸ï¼Œæ•°æ®å¹³è¡¡ï¼Œåˆ™ä¸ä¸´è¿‘ç›®æ ‡å¯¹åº”çš„ç‰¹å¾æ•°æ®å½¼æ­¤æ¥è¿‘ã€‚</p><p>â€ƒâ€ƒ<strong>Motivating Example.</strong> We use an illustrative example to highlight the impact of data imbalance on feature statistics in DIR. Again, we use a plain model trained on the images in the IMDB-WIKI dataset to infer a person's age from visual appearance. We focus on the learned feature space, i.e., $\mathbf{z}$. We use a minimum bin size of 1, i.e., $y_b+1 âˆ’ y_b = 1$, and group features with the same target value in the same bin. We then compute the feature statistics (i.e., mean and variance) with respect to the data in each bin, which we denote as $\left{\boldsymbol{\mu}<em>{b}, \boldsymbol{\sigma}</em>{b}\right}<em>{b=1}^{B}$. To visualize the similarity between feature statistics, we select an anchor bin $b_0$, and calculate the cosine similarity of the feature statistics between $b_0$ and all other bins. The results are summarized in Fig. 4 for $b_0 = 30$. The figure also shows the regions with different data densities using the colors purple, yellow, and pink.
â€ƒâ€ƒæ¿€åŠ±çš„ä¾‹å­ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªè¯´æ˜æ€§çš„ä¾‹å­æ¥å¼ºè°ƒæ•°æ®ä¸å¹³è¡¡åœ¨DIRä¸­çš„ç‰¹å¾statisticsçš„å½±å“ã€‚å†ä¸€æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªåœ¨IMDB-WIKIæ•°æ®é›†å†…çš„å›¾ç‰‡è®­ç»ƒçš„ç©ºç™½æ¨¡å‹æ¥ä»å¤–è²Œæ¨æ–­å¹´é¾„ã€‚æˆ‘ä»¬ä¾§é‡å­¦ä¹ åˆ°çš„ç‰¹å¾ç©ºé—´ï¼Œå³zã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å°binçš„å¤§å°ä¸º1ï¼Œå³$y_b+1 âˆ’ y_b = 1$ï¼Œå¹¶å°†å…·æœ‰ç›¸åŒç›®æ ‡å€¼çš„ç‰¹å¾ç»„æ”¾åœ¨ç›¸åŒbinä¸­ã€‚ç„¶åè®¡ç®—æ¯ä¸ªbinä¸­çš„æ•°æ®çš„ç‰¹å¾ç»Ÿè®¡æ•°æ®ï¼ˆå³å‡å€¼å’Œæ–¹å·®ï¼‰ï¼Œå°†å…¶è¡¨ç¤ºä¸º$\left{\boldsymbol{\mu}</em>{b}, \boldsymbol{\sigma}<em>{b}\right}</em>{b=1}^{B}$ã€‚ä¸ºäº†å¯è§†åŒ–ç‰¹å¾ç»Ÿè®¡æ•°æ®ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªanchor bin $b_0$ï¼Œå¹¶è®¡ç®—$b_0$ä¸æ‰€æœ‰å…¶ä»–binsçš„ç‰¹å¾ç»Ÿè®¡çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚åœ¨å›¾4æ€»ç»“äº†ç»“æœã€‚è¯¥å›¾è¿˜ä½¿ç”¨ç´«è‰²ã€é»„è‰²å’Œç²‰çº¢è‰²æ˜¾ç¤ºäº†å…·æœ‰ä¸åŒæ•°æ®å¯†åº¦çš„åŒºåŸŸã€‚</p><p><img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-3.png alt=image.png><span class=caption>â— Figure 4. Feature statistics similarity for age 30. Top: Cosine similarity of the feature mean at a particular age w.r.t. its value at the anchor age. Bottom: Cosine similarity of the feature variance at a particular age w.r.t. its value at the anchor age. The color of the background refers to the data density in a particular target range. The figure shows that nearby ages have close similarities; However, it also shows that there is unjustified similarity between images at ages 0 to 6 and age 30, due to data imbalance.å›¾4. 30å²çš„ç‰¹å¾ç»Ÿè®¡å€¼çš„ç›¸ä¼¼åº¦ã€‚ä¸Šå›¾ï¼šåœ¨ç‰¹å®šå¹´é¾„çš„ç‰¹å¾å‡å€¼ç›¸å¯¹äºanchorå¹´é¾„å€¼çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä¸‹å›¾ï¼šç‰¹å®šå¹´é¾„çš„ç‰¹å¾æ–¹å·®ç›¸å¯¹äºanchorå¹´é¾„çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚èƒŒæ™¯é¢œè‰²æ˜¯æŒ‡ç‰¹å®šç›®æ ‡èŒƒå›´çš„æ•°æ®å¯†åº¦ã€‚å›¾ç‰‡è¡¨æ˜ä¸´è¿‘å¹´é¾„æœ‰è¿‘ä¼¼çš„ç›¸ä¼¼åº¦ï¼›ä½†ï¼Œå®ƒä¹Ÿè¡¨æ˜åœ¨0-6å²/30å²çš„å›¾ç‰‡é—´æœ‰ä¸åˆç†çš„ç›¸ä¼¼åº¦ï¼Œå› ä¸ºæ•°æ®ä¸å¹³è¡¡</span></p><figure>Figure 4. Feature statistics similarity for age 30. Top: Cosine similarity of the feature mean at a particular age w.r.t. its value at the anchor age. Bottom: Cosine similarity of the feature variance at a particular age w.r.t. its value at the anchor age. The color of the background refers to the data density in a particular target range. The figure shows that nearby ages have close similarities; However, it also shows that there is unjustified similarity between images at ages 0 to 6 and age 30, due to data imbalance.<p>å›¾4. 30å²çš„ç‰¹å¾ç»Ÿè®¡å€¼çš„ç›¸ä¼¼åº¦ã€‚ä¸Šå›¾ï¼šåœ¨ç‰¹å®šå¹´é¾„çš„ç‰¹å¾å‡å€¼ç›¸å¯¹äºanchorå¹´é¾„å€¼çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä¸‹å›¾ï¼šç‰¹å®šå¹´é¾„çš„ç‰¹å¾æ–¹å·®ç›¸å¯¹äºanchorå¹´é¾„çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚èƒŒæ™¯é¢œè‰²æ˜¯æŒ‡ç‰¹å®šç›®æ ‡èŒƒå›´çš„æ•°æ®å¯†åº¦ã€‚å›¾ç‰‡è¡¨æ˜ä¸´è¿‘å¹´é¾„æœ‰è¿‘ä¼¼çš„ç›¸ä¼¼åº¦ï¼›ä½†ï¼Œå®ƒä¹Ÿè¡¨æ˜åœ¨0-6å²/30å²çš„å›¾ç‰‡é—´æœ‰ä¸åˆç†çš„ç›¸ä¼¼åº¦ï¼Œå› ä¸ºæ•°æ®ä¸å¹³è¡¡</figure></p><p>â€ƒâ€ƒFig. 4 shows that the feature statistics around $b_0 = 30$ are highly similar to their values at$b_0 = 30$. Specifically, the cosine similarity of the feature mean and feature variance for all bins between age 25 and 35 are within a few percent from their values at age 30 (the anchor age). Further, the similarity gets higher for tighter ranges around the anchor. Note that bin 30 falls in the high shot region. In fact, it is among the few bins that have the most samples. So, the figure confirms the intuition that when there is enough data, and for continuous targets, the feature statistics are similar to nearby bins. Interestingly, the figure also shows the problem with regions that have very few data samples, like the age range 0 to 6 years (shown in pink). Note that the mean and variance in this range show unexpectedly high similarity to age 30. In fact, it is shocking that the feature statistics at age 30 are more similar to age 1 than age 17. This unjustified similarity is due to data imbalance. Specifically, since there are not enough images for ages 0 to 6, this range thus inherits its priors from the range with the maximum amount of data, which is the range around age 30.
â€ƒâ€ƒå›¾4è¡¨æ˜$b_0=30$å‘¨å›´çš„ç‰¹å¾å€¼ä¸å®ƒä»¬åœ¨$b_0$å¤„çš„å€¼é«˜åº¦ç›¸ä¼¼ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨25åˆ°35ä¹‹é—´çš„æ‰€æœ‰binsçš„ç‰¹å¾å‡å€¼å’Œæ–¹å·®çš„ä½™å¼¦ç›¸ä¼¼åº¦ä¸å®ƒä»¬åœ¨30å²(anchor age)ç›¸å·®å‡ ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œåœ¨anchoré™„è¿‘æ›´çª„çš„èŒƒå›´æœ‰æ›´é«˜çš„ç›¸ä¼¼åº¦ã€‚æ³¨æ„bin30è½åœ¨é«˜shot rigion(æ ·æœ¬å¾ˆå¤šçš„åŒºåŸŸ)ã€‚äº‹å®ä¸Šï¼Œå®ƒæ˜¯å°‘æ•°å‡ ä¸ªæœ‰æœ€å¤šçš„æ ·æœ¬çš„binsä¹‹ä¸€ã€‚æ‰€ä»¥ï¼Œå›¾ç‰‡è¯æ˜äº†å½“æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œå¯¹äºè¿ç»­ç›®æ ‡å€¼ï¼Œåœ¨ç›¸è¿‘çš„binsæœ‰ç›¸ä¼¼çš„ç‰¹å¾å€¼ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¯¥å›¾ä¹Ÿè¡¨æ˜åªæœ‰å¾ˆå°‘æ•°æ®æ ·æœ¬çš„åŒºåŸŸçš„é—®é¢˜ï¼Œå¦‚1-6å²(ç²‰çº¢è‰²æ˜¾ç¤º)ã€‚æ³¨æ„åœ¨è¿™ä¸ªåŒºåŸŸçš„å‡å€¼å’Œæ–¹å·®ä¸30å²çš„ç›¸ä¼¼åº¦å‡ºä¹æ„æ–™çš„é«˜ã€‚äº‹å®ä¸Šï¼Œ30å²çš„ç‰¹å¾æ•°æ®å±…ç„¶è·Ÿ1å²çš„ç›¸ä¼¼åº¦æ¯”17å²è¦é«˜ã€‚è¿™ç§ä¸åˆç†çš„ç›¸ä¼¼åº¦æ˜¯å› ä¸ºæ•°æ®ä¸å¹³è¡¡ã€‚å…·ä½“æ¥è¯´ï¼Œç”±äº1-6å²æ²¡æœ‰è¶³å¤Ÿçš„å›¾ç‰‡ï¼Œå› æ­¤è¯¥èŒƒå›´ä»æ•°æ®é‡æœ€å¤§çš„èŒƒå›´(å³30å²é™„è¿‘)ç»§æ‰¿å…¶å…ˆéªŒå€¼</p><blockquote><p>â€ƒâ€ƒmany-shot region (bins with over 100 training samples), medium-shot region (bins with 20âˆ¼100 training samples), and few-shot region (bins with under 20 training samples)</p></blockquote><p>â€ƒâ€ƒ<strong>FDS Algorithm.</strong> Inspired by these observations, we propose feature distribution smoothing (FDS), which performs distribution smoothing on the feature space, i.e., transfers the feature statistics between nearby target bins. This procedure aims to calibrate the potentially biased estimates of feature distribution, especially for underrepresented target values (e.g., medium- and few-shot groups) in training data. FDS is performed by first estimating the statistics of each bin. Without loss of generality, we substitute variance with covariance to reflect also the relationship between the various feature elements within z:<br>â€ƒâ€ƒFDSç®—æ³•ã€‚å—è¿™äº›è§‚æµ‹å¯å‘ï¼Œæˆ‘ä»¬æå‡ºç‰¹å¾åˆ†å¸ƒå¹³æ»‘(FDS)ï¼Œå®ƒåœ¨ç‰¹å¾ç©ºé—´è¿›è¡Œåˆ†å¸ƒå¹³æ»‘ï¼Œå³åœ¨ç›¸è¿‘ç‰¹å¾binsä¼ è¾“ç‰¹å¾ç»Ÿè®¡æ•°æ®ã€‚æ­¤è¿‡ç¨‹æ—¨åœ¨æ ¡å‡†ç‰¹å¾åˆ†å¸ƒæ½œåœ¨çš„åå·®ä¼°è®¡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè®­ç»ƒæ•°æ®ä¸­ä½ä»£è¡¨æ€§çš„ç›®æ ‡å€¼(å¦‚ä¸­æ ·æœ¬ç»„å’Œå°‘æ ·æœ¬ç»„)ã€‚é€šè¿‡é¦–æ¬¡ä¼°è®¡æ¯ä¸ªbinçš„ç»Ÿè®¡æ•°æ®æ¥è¿è¡ŒFDSã€‚ä¸å¤±ä¸€èˆ¬æ€§ï¼Œæˆ‘ä»¬ç”¨åæ–¹å·®ä»£æ›¿æ–¹å·®ï¼Œä»¥ååº”zå†…çš„å„ç§ç‰¹å¾å…ƒç´ çš„å…³ç³»ï¼š
$$\boldsymbol{\mu}<em>{b}=\frac{1}{N</em>{b}} \sum_{i=1}^{N_{b}} \mathbf{z}<em>{i} \qquad(2)$$
$$\boldsymbol{\Sigma}</em>{b}=\frac{1}{N_{b}-1} \sum_{i=1}^{N_{b}}\left(\mathbf{z}<em>{i}-\boldsymbol{\mu}</em>{b}\right)\left(\mathbf{z}<em>{i}-\boldsymbol{\mu}</em>{b}\right)^{\top} \qquad(3)$$
where $N_b$ is the total number of samples in b-th bin. Given the feature statistics, we employ again a symmetric kernel $k\left(y_{b}, y_{b^{\prime}}\right)$ to smooth the distribution of the feature mean and covariance over the target bins $\mathcal{B}$. This results in a smoothed version of the statistics:<br>å…¶ä¸­$N_b$æ˜¯åœ¨ç¬¬bä¸ªbinçš„æ€»æ ·æœ¬æ•°ã€‚æ ¹æ®ç‰¹å¾ç»Ÿè®¡æ•°æ®ï¼Œæˆ‘ä»¬å†æ¬¡ä½¿ç”¨å¯¹ç§°æ ¸$k\left(y_{b}, y_{b^{\prime}}\right)$æ¥å¹³è¡¡ç›®æ ‡binsçš„ç‰¹å¾å‡å€¼å’Œåæ–¹å·®åˆ†å¸ƒã€‚ç”±æ­¤äº§ç”Ÿç»Ÿè®¡æ•°æ®çš„å¹³æ»‘ç‰ˆæœ¬ï¼š
$$\tilde{\boldsymbol{\mu}}<em>{b}=\sum</em>{b^{\prime} \in \mathcal{B}} \mathrm{k}\left(y_{b}, y_{b^{\prime}}\right) \boldsymbol{\mu}<em>{b^{\prime}} \qquad(4)$$
$$\tilde{\Sigma}</em>{b}=\sum_{b^{\prime} \in \mathcal{B}} \mathrm{k}\left(y_{b}, y_{b^{\prime}}\right) \Sigma_{b^{\prime}} \qquad(5) $$</p><p>â€ƒâ€ƒWith both$\left{\boldsymbol{\mu}<em>{b}, \boldsymbol{\Sigma}</em>{b}\right}$and$\left{\tilde{\boldsymbol{\mu}}<em>{b}, \widetilde{\boldsymbol{\Sigma}}</em>{b}\right}$, we then follow the standard whitening and re-coloring procedure (Sun et al., 2016) to calibrate the feature representation for each input sample:<br>â€ƒâ€ƒä½¿ç”¨$\left{\boldsymbol{\mu}<em>{b}, \boldsymbol{\Sigma}</em>{b}\right}$and$\left{\tilde{\boldsymbol{\mu}}<em>{b}, \widetilde{\boldsymbol{\Sigma}}</em>{b}\right}$ï¼Œç„¶åæˆ‘ä»¬éµå¾ªæ ‡å‡†çš„ç™½åŒ–å’Œé‡ç€è‰²ç¨‹åºæ¥æ ¡å‡†æ¯ä¸ªè¾“å…¥æ ·æœ¬çš„ç‰¹å¾è¡¨ç¤ºï¼ˆå›¾ç‰‡çš„filtersï¼Ÿï¼‰ï¼š
$$\tilde{\mathbf{z}}=\tilde{\boldsymbol{\Sigma}}<em>{b}^{\frac{1}{2}} \boldsymbol{\Sigma}</em>{b}^{-\frac{1}{2}}\left(\mathbf{z}-\boldsymbol{\mu}<em>{b}\right)+\tilde{\boldsymbol{\mu}}</em>{b} \qquad(6)$$</p><p>â€ƒâ€ƒWe integrate FDS into deep networks by inserting a feature calibration layer after the final feature map. To train the model, we employ a momentum update of the running statistics$\left{\boldsymbol{\mu}<em>{b}, \boldsymbol{\Sigma}</em>{b}\right}$across each epoch. Correspondingly, the smoothed statistics$\left{\tilde{\boldsymbol{\mu}}<em>{b}, \widetilde{\boldsymbol{\Sigma}}</em>{b}\right}$are updated across different epochs but fixed within each training epoch. The momentum update, which performs an exponential moving average (EMA) of running statistics, results in more stable and accurate estimations of the feature statistics during training. The calibrated features $\tilde{\mathbf{z}}$ are then passed to the final regression function and used to compute the loss.
â€ƒâ€ƒæˆ‘ä»¬æŠŠFDSæ•´åˆå…¥æ·±åº¦ç½‘ç»œé€šè¿‡åœ¨æœ€åçš„ç‰¹å¾å›¾åæ’å…¥ä¸€ä¸ªç‰¹å¾æ ¡å‡†å±‚ã€‚ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªepochè¿è¡Œä¸­çš„ç»Ÿè®¡æ•°æ®$\left{\boldsymbol{\mu}<em>{b}, \boldsymbol{\Sigma}</em>{b}\right}$è¿›è¡ŒåŠ¨é‡æ›´æ–°ã€‚å¯¹åº”çš„ï¼Œå¹³æ»‘ç»Ÿè®¡$\left{\tilde{\boldsymbol{\mu}}<em>{b}, \widetilde{\boldsymbol{\Sigma}}</em>{b}\right}$åœ¨ä¸åŒepochæ›´æ–°ï¼Œä½†åœ¨æ¯ä¸ªè®­ç»ƒepochæ—¶å›ºå®šã€‚åŠ¨é‡æ›´æ–°æ‰§è¡Œè¿è¡Œç»Ÿè®¡å€¼çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡(EMA)ï¼Œä½¿å¾—è®­ç»ƒæœŸé—´å¯¹ç‰¹å¾ç»Ÿè®¡å€¼è¿›è¡Œæ›´ç¨³å®šå’Œå‡†ç¡®çš„ä¼°è®¡ã€‚ç„¶åå°†æ ¡å‡†å€¼ $\tilde{\mathbf{z}}$ä¼ é€’ç»™æœ€åçš„å›å½’å‡½æ•°å¹¶ç”¨äºè®¡ç®—lossã€‚</p><p>â€ƒâ€ƒWe note that FDS can be integrated with any neural network model, as well as any past work on improving label imbalance. In Sec. 4, we integrate FDS with a variety of prior techniques for addressing data imbalance, and demonstrate that it consistently improves performance.
â€ƒâ€ƒæˆ‘ä»¬æ³¨æ„åˆ°FDSå¯ä»¥æ•´åˆåˆ°ä»»ä½•ç¥ç»ç½‘ç»œæ¨¡å‹å†…ï¼Œä»¥åŠä»»ä½•è¿‡å»æ”¹å–„æ ‡ç­¾ä¸å¹³è¡¡çš„å·¥ä½œã€‚åœ¨Sec.4ï¼Œæˆ‘ä»¬å°†FDSä¸ä¸€ç³»åˆ—è§£å†³æ•°æ®ä¸å¹³è¡¡çš„ç°æœ‰æŠ€æœ¯ç»“åˆï¼Œå¹¶è¯æ˜å®ƒèƒ½ä¸æ–­åœ°æé«˜æ€§èƒ½</p><p><img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-4.png alt=image.png><span class=caption>â— Figure 6. Overview of training set label distribution for five DIR datasets. They range from single-value prediction such as age, textual similarity score, and health condition score, to dense-value prediction such as depth estimation. More details are provided in Appendix B.å›¾6ã€‚äº”ä¸ª DIR æ•°æ®é›†çš„è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒæ¦‚è¿°ã€‚ å®ƒä»¬çš„èŒƒå›´ä»å•å€¼é¢„æµ‹ï¼ˆä¾‹å¦‚å¹´é¾„ã€æ–‡æœ¬ç›¸ä¼¼åº¦å¾—åˆ†å’Œå¥åº·çŠ¶å†µå¾—åˆ†ï¼‰åˆ°å¯†é›†å€¼é¢„æµ‹ï¼ˆä¾‹å¦‚æ·±åº¦ä¼°è®¡ï¼‰ã€‚ é™„å½• B ä¸­æä¾›äº†æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚</span></p><figure>Figure 6. Overview of training set label distribution for five DIR datasets. They range from single-value prediction such as age, textual similarity score, and health condition score, to dense-value prediction such as depth estimation. More details are provided in Appendix B.<p>å›¾6ã€‚äº”ä¸ª DIR æ•°æ®é›†çš„è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒæ¦‚è¿°ã€‚ å®ƒä»¬çš„èŒƒå›´ä»å•å€¼é¢„æµ‹ï¼ˆä¾‹å¦‚å¹´é¾„ã€æ–‡æœ¬ç›¸ä¼¼åº¦å¾—åˆ†å’Œå¥åº·çŠ¶å†µå¾—åˆ†ï¼‰åˆ°å¯†é›†å€¼é¢„æµ‹ï¼ˆä¾‹å¦‚æ·±åº¦ä¼°è®¡ï¼‰ã€‚ é™„å½• B ä¸­æä¾›äº†æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚</figure></p><p><a name=oPq0c></a></p><h2 id=4-benchmarking-dir><a href=#4-benchmarking-dir class=anchor-link>#</a><a href=#contents:4-benchmarking-dir class=headings>4. Benchmarking DIR</a></h2><p>â€ƒâ€ƒ<strong>Datasets.</strong> We curate five DIR benchmarks that span computer vision, natural language processing, and healthcare.Fig. 6 shows the label density distribution of these datasets,and their level of imbalance.
â€ƒâ€ƒæ•°æ®é›†ã€‚æˆ‘ä»¬æ•´ç†äº†äº”ä¸ªDIRåŸºå‡†é›†ï¼ŒåŒ…æ‹¬CVã€NLPã€HCã€‚å›¾6å±•ç¤ºäº†è¿™äº›æ•°æ®é›†åœ°æ ‡ç­¾å¯†åº¦åˆ†å¸ƒå’Œä¸å¹³è¡¡ç­‰çº§ã€‚</p><ul><li>IMDB-WIKI-DIR (age): We construct IMDB-WIKI-DIR using the IMDB-WIKI dataset (Rothe et al., 2018), which contains 523.0K face images and the corresponding ages. We filter out unqualified images, and manually construct balanced validation and test set over the supported ages. The length of each bin is 1 year, with a minimum age of 0 and a maximum age of 186. The number of images per bin varies between 1 and 7149, exhibiting significant data imbalance. Overall, the curated dataset has 191.5K images for training, 11.0K images for validation and testing.</li><li>IMDB-WIKI-DIRï¼ˆå¹´é¾„ï¼‰ï¼šæˆ‘ä»¬ä½¿ç”¨IMDB-WIKIæ•°æ®é›†æ„å»ºäº†IMDB-WIKI-DIRï¼ŒåŒ…å«523Kå¼ é¢éƒ¨å›¾ç‰‡å’Œå¯¹åº”å¹´é¾„ã€‚æˆ‘ä»¬è¿‡æ»¤æ‰ä¸åˆæ ¼å›¾ç‰‡å¹¶åœ¨æ”¯æŒçš„å¹´é¾„èŒƒå›´å†…æ‰‹åŠ¨æ„é€ å¹³è¡¡çš„éªŒè¯å’Œæµ‹è¯•é›†ã€‚æ¯ä¸ªbinçš„é•¿åº¦æ˜¯ä¸€å¹´ï¼Œæœ€å°å¹´é¾„ä¸º0ï¼Œæœ€å¤§å¹´é¾„ä¸º186.æ¯ä¸ªbinçš„å›¾åƒæ•°é‡ä¸º1-7149ä¹‹é—´ï¼Œè¡¨ç°å‡ºæ˜æ˜¾çš„æ•°æ®ä¸å¹³è¡¡ã€‚æ€»çš„æ¥è¯´ï¼Œæ•´ç†åçš„æ•°æ®é›†æœ‰191.5Kå¼ è®­ç»ƒå›¾ç‰‡ï¼Œ11Kå¼ éªŒè¯å’Œæµ‹è¯•å›¾ç‰‡ã€‚</li><li>AgeDB-DIR (age): AgeDB-DIR is constructed in a similar manner from the AgeDB dataset (Moschoglou et al., 2017). It contains 12.2K images for training, with a minimum age of 0 and a maximum age of 101, and maximum bin density of 353 images and minimum bin density of 1. The validation and test set are balanced with 2.1K images.</li><li>AgeDB-DIRï¼ˆå¹´é¾„ï¼‰ï¼šAgeDB-DIR ä»¥ç±»ä¼¼çš„æ–¹å¼ä» AgeDB æ•°æ®é›†æ„å»ºã€‚å®ƒåŒ…å«ç”¨äºè®­ç»ƒçš„ 12.2K å›¾åƒï¼Œæœ€å°å¹´é¾„ä¸º 0ï¼Œæœ€å¤§å¹´é¾„ä¸º 101ï¼Œæœ€å¤§ bin å¯†åº¦ä¸º 353 ä¸ªå›¾åƒï¼Œæœ€å° bin å¯†åº¦ä¸º 1ã€‚2.1Kå¼ å›¾ç‰‡çš„å¹³è¡¡éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚</li><li>STS-B-DIR (text similarity score): We construct STS-BDIR from the Semantic Textual Similarity Benchmark (Cer et al., 2017; Wang et al., 2018), which is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. Each pair is annotated by multiple annotators with an averaged continuous similarity score from 0 to 5. From the original training set of 7.2K pairs, we create a training set with 5.2K pairs, and balanced validation set and test set of 1K pairs each. The length of each bin is 0.1.</li><li>STS-B-DIRï¼ˆæ–‡æœ¬ç›¸ä¼¼åº¦å¾—åˆ†ï¼‰ï¼šæˆ‘ä»¬ä»è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦åŸºå‡†æ•°æ®é›†æ„å»º STS-BDIRï¼Œå®ƒæ˜¯ä»æ–°é—»æ ‡é¢˜ã€è§†é¢‘å’Œå›¾åƒå­—å¹•ï¼Œä»¥åŠè‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®ä¸­æå–çš„å¥å­å¯¹çš„é›†åˆ ã€‚ æ¯å¯¹ç”±å¤šä¸ªæ³¨é‡Šå™¨è¿›è¡Œæ³¨é‡Šï¼Œå¹³å‡è¿ç»­ç›¸ä¼¼åº¦å¾—åˆ†ä» 0 åˆ° 5ã€‚ä» 7.2K å¯¹çš„åŸå§‹è®­ç»ƒé›†ï¼Œæˆ‘ä»¬æ„å»ºä¸€ä¸ªåŒ…å« 5.2K å¯¹çš„è®­ç»ƒé›†ï¼Œä»¥åŠæ¯ä¸ª 1K å¯¹çš„å¹³è¡¡éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚ æ¯ä¸ª bin çš„é•¿åº¦ä¸º 0.1ã€‚</li><li>NYUD2-DIR (depth): We create NYUD2-DIR based on the NYU Depth Dataset V2 (Nathan Silberman & Fergus, 2012), which provides images and depth maps for different indoor scenes. The depth maps have an upper bound of 10 meters and we set the bin length as 0.1 meter. Following standard practices (Bhat et al., 2020; Hu et al., 2019), we use 50K images for training and 654 images for testing. We randomly select 9357 test pixels for each bin to make the test set balanced.</li><li>NYUD2-DIRï¼ˆæ·±åº¦ï¼‰ï¼šæˆ‘ä»¬åŸºäº NYU Depth Dataset V2åˆ›å»º NYUD2-DIRï¼Œå®ƒä¸ºä¸åŒçš„å®¤å†…åœºæ™¯æä¾›å›¾åƒå’Œæ·±åº¦å›¾ã€‚ æ·±åº¦å›¾çš„ä¸Šé™ä¸º 10 ç±³ï¼Œæˆ‘ä»¬å°† bin é•¿åº¦è®¾ç½®ä¸º 0.1 ç±³ã€‚ æŒ‰ç…§æ ‡å‡†åšæ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨ 50K å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨ 654 å›¾åƒè¿›è¡Œæµ‹è¯•ã€‚ æˆ‘ä»¬ä¸ºæ¯ä¸ª bin éšæœºé€‰æ‹© 9357 ä¸ªæµ‹è¯•åƒç´ ï¼Œä»¥ä½¿æµ‹è¯•é›†å¹³è¡¡ã€‚</li><li>SHHS-DIR (health condition score): We create SHHSDIR based on the SHHS dataset (Quan et al., 1997), which contains full-night Polysomnography (PSG) from 2651 subjects. Available PSG signals include Electroencephalography (EEG), Electrocardiography (ECG), and breathing signals (airflow, abdomen, and thorax), which are used as inputs. The dataset also includes the 36- Item Short Form Health Survey (SF-36) (Ware Jr & Sherbourne, 1992) for each subject, where a General Health score is extracted. The score is used as the target value with a minimum score of 0 and maximum of 100.</li><li>SHHS-DIRï¼ˆå¥åº·çŠ¶å†µè¯„åˆ†ï¼‰ï¼šæˆ‘ä»¬åŸºäº SHHS æ•°æ®é›†åˆ›å»º SHHSDIRï¼Œå…¶ä¸­åŒ…å«æ¥è‡ª 2651 åå—è¯•è€…çš„æ•´æ™šå¤šå¯¼ç¡çœ å›¾ï¼ˆPSGï¼‰ã€‚ å¯ç”¨çš„ PSG ä¿¡å·åŒ…æ‹¬ç”¨ä½œè¾“å…¥çš„è„‘ç”µå›¾ (EEG)ã€å¿ƒç”µå›¾ (ECG) å’Œå‘¼å¸ä¿¡å·ï¼ˆæ°”æµã€è…¹éƒ¨å’Œèƒ¸éƒ¨ï¼‰ã€‚ è¯¥æ•°æ®é›†è¿˜åŒ…æ‹¬æ¯ä¸ªå—è¯•è€…çš„ 36 é¡¹ç®€çŸ­å¥åº·è°ƒæŸ¥ (SF-36) ï¼Œå…¶ä¸­æå–äº†General Healthåˆ†æ•°ã€‚ åˆ†æ•°ç”¨ä½œç›®æ ‡å€¼ï¼Œæœ€ä½åˆ†æ•°ä¸º 0ï¼Œæœ€é«˜åˆ†æ•°ä¸º 100ã€‚</li></ul><p>â€ƒâ€ƒ<strong>Network Architectures.</strong> We employ ResNet-50 (He et al., 2016) as our backbone network for IMDB-WIKI-DIR and AgeDB-DIR. Following (Wang et al., 2018), we adopt the same BiLSTM + GloVe word embeddings baseline for STSB-DIR. For NYUD2-DIR, we use ResNet-50-based encoderdecoder architecture introduced in (Hu et al., 2019). Finally, for SHHS-DIR, we use the same CNN-RNN architecture with ResNet block for PSG signals as in (Wang et al., 2019).
â€ƒâ€ƒç½‘ç»œæ¶æ„ã€‚æˆ‘ä»¬é‡‡ç”¨ResNet-50ä½œä¸ºIMDB-WIKI-DIRå’ŒAgeDB-DIRçš„éª¨å¹²ç½‘ç»œã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¹STSB-DIRé‡‡ç”¨äº†ç›¸åŒçš„BiLSTM+GloVeè¯åµŒå…¥åŸºçº¿ã€‚å¯¹äºNYUD2-DIRï¼Œæˆ‘ä»¬ä½¿ç”¨(Hu et al., 2019)ä¸­ä»‹ç»çš„åŸºäºResNet-50çš„encoderdecoderæ¶æ„ã€‚æœ€åï¼Œå¯¹äº SHHS-DIRï¼Œæˆ‘ä»¬ä½¿ç”¨åŒæ ·çš„å¸¦ResNet å—çš„CNN-RNN æ¶æ„æ¥å¤„ç†PSGä¿¡å·ã€‚</p><p>â€ƒâ€ƒ<strong>Baselines.</strong> Since the literature has only a few proposals for DIR, in addition to past work on imbalanced regression (Branco et al., 2017; Torgo et al., 2013), we adapt a few imbalanced classification methods for regression, and propose a strong set of baselines. Below, we describe the baselines, and how we can combine LDS with each method. For FDS, it can be directly integrated with any baseline as a calibration layer, as described in Sec. 3.2.<br>â€ƒâ€ƒåŸºçº¿ã€‚ç”±äºæ–‡çŒ®ä¸­åªæœ‰å°‘æ•°å…³äºDIRçš„å»ºè®®ï¼Œé™¤äº†è¿‡å»å…³äºä¸å¹³è¡¡å›å½’çš„å·¥ä½œï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€äº›ä¸å¹³è¡¡å›å½’åˆ†ç±»æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç»„å¼ºå¤§çš„åŸºçº¿ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å°†æè¿°åŸºçº¿ï¼Œä»¥åŠå¦‚ä½•å°†LDSä¸æ¯ç§æ–¹æ³•ç›¸ç»“åˆã€‚å¯¹äºFDSï¼Œå®ƒå¯ä»¥ç›´æ¥ä¸ä»»ä½•åŸºçº¿é›†æˆä½œä¸ºæ ¡å‡†å±‚ï¼Œå¦‚ç¬¬3.2èŠ‚æ‰€è¿°ã€‚</p><ul><li><p>$Vanilla model$: We use term <strong>VANILLA</strong> to denote a model that does not include any technique for dealing with imbalanced data. To combine the vanilla model with LDS, we re-weight the loss function by multiplying it by the inverse of the LDS estimated density for each target bin.</p></li><li><p>$Synthetic samples$: We choose existing methods for imbalanced regression, including <strong>SMOTER</strong> (Torgo et al., 2013) and <strong>SMOGN</strong> (Branco et al., 2017). SMOTER first defines frequent and rare regions using the original label density, and creates synthetic samples for pre-defined rare regions by linearly interpolating both inputs and targets. SMOGN further adds Gaussian noise to SMOTER. We note that LDS can be directly used for a better estimation of label density when dividing the target space.</p></li><li><p>$Error-aware loss$: Inspired by the Focal loss (Lin et al., 2017) for classification, we propose a regression version called Focal-R, where the scaling factor is replaced by a continuous function that maps the absolute error into [0, 1]. Precisely, Focal-R loss based on L1 distance can be written as$\frac{1}{n} \sum_{i=1}^{n} \sigma\left(\left|\beta e_{i}\right|\right)^{\gamma} e_{i}$, where $e_i$ is the L1 error for i-th sample, $Ïƒ(\cdot)$ is the $Sigmoid$ function, and $Î², Î³$ are hyper-parameters. To combine Focal-R with LDS, we multiply the loss with the inverse frequency of the estimated label density.</p></li><li><p>$Two-stage training$: Following (Kang et al., 2020) where feature and classifier are decoupled and trained in two stages, we propose a regression version called regressor re-training (RRT), where in the first stage we train the encoder normally, and in the second stage freeze the encoder and re-train the regressor $g(\cdot)$ with inverse re-weighting. When adding LDS, the re-weighting in the second stage is based on the label density estimated through LDS.</p></li><li><p>$Cost-sensitive re-weighting$: Since we divide the target space into finite bins, classic re-weighting methods can be directly plugged in. We adopt two re-weighting schemes based on the label distribution: inverse-frequency weighting (INV) and its square-root weighting variant (SQINV). When combining with LDS, instead of using the original label density, we use the LDS estimated target density.</p></li><li><p>Vanillamodelï¼šæˆ‘ä»¬ä½¿ç”¨VANILLAè¡¨ç¤ºä¸åŒ…å«å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„ä»»ä½•æŠ€æœ¯çš„æ¨¡å‹ã€‚ä¸ºäº†å°†æ¨¡å‹ä¸LDSç»“åˆï¼Œæˆ‘ä»¬é€šè¿‡å°†æŸå¤±å‡½æ•°ä¹˜æ¯ä¸ªç›®æ ‡binçš„LDSä¼°è®¡å¯†åº¦çš„å€’æ•°æ¥é‡åŠ æƒæŸå¤±å‡½æ•°ã€‚</p></li><li><p>Syntheticsampleï¼šæˆ‘ä»¬é€‰æ‹©ç°å­˜çš„ä¸å¹³è¡¡å›å½’æ–¹æ³•ï¼ŒåŒ…æ‹¬SMOTERå’ŒSMOGNã€‚SMOTERé¦–å…ˆä½¿ç”¨åˆå§‹æ ‡ç­¾å¯†åº¦å®šä¹‰frequentåŒºåŸŸå’ŒrareåŒºåŸŸï¼Œå¹¶é€šè¿‡å¯¹è¾“å…¥å’Œç›®æ ‡è¿›è¡Œçº¿æ€§æ’å€¼æ¥ä¸ºé¢„å®šä¹‰çš„rareåŒºåŸŸåˆ›å»ºæ ·æœ¬ã€‚SMOGNå°†é«˜æ–¯å™ªå£°åŠ åˆ°SMOTERã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨åˆ’åˆ†ç›®æ ‡ç©ºé—´æ—¶ï¼ŒLDSå¯ä»¥ç›´æ¥ä½¿ç”¨æ¥æ›´å¥½åœ°ä¼°è®¡ä¼°è®¡æ ‡ç­¾å¯†åº¦ã€‚</p></li><li><p>Error-awarelossï¼šå—ç”¨äºåˆ†ç±»çš„Focal losså¯å‘ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªFocal-Rçš„å›å½’ç‰ˆæœ¬ï¼Œå…¶ä¸­æ¯”ä¾‹å› å­è¢«ä¸€ä¸ªå°†ç»å¯¹è¯¯å·®æ˜ å°„åˆ°[0, 1]çš„è¿ç»­å‡½æ•°æ›¿ä»£ã€‚å‡†ç¡®çš„è¯´ï¼ŒåŸºäºL1è·ç¦»çš„Focal-RæŸå¤±å¯ä»¥å†™ä¸º$\frac{1}{n} \sum_{i=1}^{n} \sigma\left(\left|\beta e_{i}\right|\right)^{\gamma} e_{i}$ï¼Œå…¶ä¸­$e_i$æ—¶ç¬¬iä¸ªæ ·æœ¬çš„L1è¯¯å·®ï¼Œ$Ïƒ(\cdot)$æ˜¯sigmoidå‡½æ•°ï¼Œ$Î², Î³$æ˜¯è¶…å‚æ•°ã€‚ä¸ºäº†å°†Focal-Rå’ŒLDSç»“åˆï¼Œå°†æŸå¤±ä¹˜ä»¥ä¼°è®¡æ ‡ç­¾å¯†åº¦çš„é¢‘ç‡çš„å€’æ•°ã€‚</p></li><li><p>Two-stagetrainingï¼šç»§ (Kang et al., 2020) å°†ç‰¹å¾å’Œåˆ†ç±»å™¨è§£è€¦å¹¶åˆ†ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œè®­ç»ƒä¹‹åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç§°ä¸ºå›å½’å™¨é‡è®­ç»ƒ (RRT) çš„å›å½’ç‰ˆæœ¬ï¼Œåœ¨ç¬¬ä¸€é˜¶æ®µæˆ‘ä»¬æ­£å¸¸è®­ç»ƒç¼–ç å™¨ï¼Œè€Œåœ¨ç¬¬äºŒé˜¶æ®µå†»ç»“ç¼–ç å™¨å¹¶ä½¿ç”¨åå‘é‡åŠ æƒé‡æ–°è®­ç»ƒå›å½’å™¨$g(\cdot)$ã€‚æ·»åŠ LDSåï¼Œç¬¬äºŒé˜¶æ®µçš„é‡æ–°åŠ æƒæ˜¯åŸºäºé€šè¿‡LDSä¼°è®¡çš„æ ‡ç­¾å¯†åº¦ã€‚</p></li><li><p>Cost-sensitive re-weightingï¼šç”±äºæˆ‘ä»¬å°†ç›®æ ‡ç©ºé—´åˆ†ä¸ºæœ‰é™çš„binï¼Œå› æ­¤å¯ä»¥ç›´æ¥æ’å…¥é‚£äº›ç»å…¸çš„é‡åŠ æƒæ–¹æ³•ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸¤ç§åŸºäºæ ‡ç­¾åˆ†å¸ƒçš„é‡åŠ æƒæ–¹æ¡ˆï¼šINVå’ŒSQINVã€‚ä¸LDSç»“åˆæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨LDSä¼°è®¡çš„ç›®æ ‡å¯†åº¦ï¼Œè€Œä¸æ˜¯åŸå§‹çš„ç›®æ ‡å¯†åº¦ã€‚</p></li></ul><p>â€ƒâ€ƒ<strong>Evaluation Process and Metrics.</strong> Following (Liu et al., 2019), we divide the target space into three disjoint subsets: many-shot region (bins with over 100 training samples), medium-shot region (bins with 20âˆ¼100 training samples), and few-shot region (bins with under 20 training samples), and report results on these subsets, as well as overall performance. We also refer to regions with no training samples as zero-shot, and investigate the ability of our techniques to generalize to zero-shot regions in Sec. 4.2. For metrics, we use common metrics for regression, such as the meanaverage-error (MAE), mean-squared-error (MSE), and Pearson correlation. We further propose another metric, called error Geometric Mean (<strong>GM</strong>), and is defined as$\left(\prod_{i=1}^{n} e_{i}\right)^{\frac{1}{n}}$for better prediction fairness.<br>â€ƒâ€ƒè¯„ä¼°è¿‡ç¨‹å’ŒæŒ‡æ ‡ã€‚åœ¨ (Liu et al., 2019) ä¹‹åï¼Œæˆ‘ä»¬å°†ç›®æ ‡ç©ºé—´åˆ’åˆ†ä¸ºä¸‰ä¸ªä¸ç›¸äº¤çš„å­é›†ï¼šmany-shot åŒºåŸŸï¼ˆå…·æœ‰è¶…è¿‡ 100 ä¸ªè®­ç»ƒæ ·æœ¬çš„ binï¼‰ã€medium-shot åŒºåŸŸï¼ˆå…·æœ‰ 20âˆ¼100 ä¸ªè®­ç»ƒæ ·æœ¬çš„ binï¼‰å’Œfew-shotåŒºåŸŸï¼ˆè®­ç»ƒæ ·æœ¬å°‘äº 20 ä¸ªçš„ binï¼‰ï¼Œå¹¶è¯´æ˜è¿™äº›å­é›†çš„ç»“æœä»¥åŠæ•´ä½“æ€§èƒ½ã€‚ æˆ‘ä»¬å°†æ²¡æœ‰è®­ç»ƒæ ·æœ¬çš„åŒºåŸŸç§°ä¸ºzero-shotï¼Œå¹¶åœ¨ Sec.4.2ä¸­ç ”ç©¶æˆ‘ä»¬çš„æŠ€æœ¯æ³›åŒ–åˆ°è¯¥åŒºåŸŸçš„èƒ½åŠ›ã€‚ å¯¹äºæŒ‡æ ‡ï¼Œæˆ‘ä»¬ä½¿ç”¨å¸¸ç”¨çš„å›å½’æŒ‡æ ‡ï¼Œä¾‹å¦‚å‡å€¼è¯¯å·® (MAE)ã€å‡æ–¹è¯¯å·® (MSE) å’Œçš®å°”é€Šç›¸å…³æ€§ã€‚ æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºå¦ä¸€ä¸ªåº¦é‡ï¼Œç§°ä¸ºè¯¯å·®å‡ ä½•å¹³å‡å€¼ (GM)ï¼Œå®šä¹‰ä¸º $\left(\prod_{i=1}^{n} e_{i}\right)^{\frac{1}{n} }$ä»¥è·å¾—æ›´å¥½çš„é¢„æµ‹å…¬å¹³æ€§ã€‚
<a name=qCLS0></a></p><h3 id=41-main-results><a href=#41-main-results class=anchor-link>#</a><a href=#contents:41-main-results class=headings>4.1. Main Results</a></h3><p>â€ƒâ€ƒWe report the main results in this section for all DIR datasets. All training details, hyper-parameter settings, and additional results are provided in Appendix C and D.
<img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-5.png alt=image.png>
<img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-6.png alt=image.png>
<strong>Inferring Age from Images: IMDB-WIKI-DIR & AgeDB-DIR.</strong> We report the performance of different methods in Table 1 and 2, respectively. For each dataset, we group the baselines into four sections to reflect their different strategies. First, as both tables indicate, when applied to modern high-dimensional data like images, SMOTER and SMOGN can actually degrade the performance in comparison to the vanilla model. Moreover, within each group, adding either LDS, FDS, or both leads to performance gains, while LDS + FDS often achieves the best results. Finally, when compared to the vanilla model, using our LDS and FDS maintains or slightly improves the performance overall and on the many-shot regions, while substantially boosting the performance for the medium-shot and few-shot regions.
â€ƒâ€ƒä»å›¾åƒæ¨æ–­å¹´é¾„ï¼šIMDB-WIKI-DIR å’Œ AgeDB-DIRã€‚ æˆ‘ä»¬åˆ†åˆ«åœ¨è¡¨ 1 å’Œè¡¨ 2 ä¸­æŠ¥å‘Šäº†ä¸åŒæ–¹æ³•çš„æ€§èƒ½ã€‚ å¯¹äºæ¯ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†åŸºçº¿åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ä»¥åæ˜ å®ƒä»¬çš„ä¸åŒç­–ç•¥ã€‚ é¦–å…ˆï¼Œå¦‚ä¸¤ä¸ªè¡¨æ‰€ç¤ºï¼Œå½“åº”ç”¨äºç°ä»£çš„é«˜ç»´æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰æ—¶ï¼Œä¸æ™®é€šæ¨¡å‹ç›¸æ¯”ï¼ŒSMOTER å’Œ SMOGN å®é™…ä¸Šä¼šé™ä½æ€§èƒ½ã€‚ æ­¤å¤–ï¼Œåœ¨æ¯ä¸ªç»„ä¸­ï¼Œæ·»åŠ  LDSã€FDS æˆ–ä¸¤è€…éƒ½ä¼šå¯¼è‡´æ€§èƒ½æå‡ï¼Œè€Œ LDS + FDS é€šå¸¸ä¼šè·å¾—æœ€ä½³ç»“æœã€‚ æœ€åï¼Œä¸æ™®é€šæ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨æˆ‘ä»¬çš„ LDS å’Œ FDS ä¿æŒæˆ–ç•¥å¾®æé«˜äº†æ•´ä½“å’Œmany-shotåŒºåŸŸçš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†medium-å’Œfew-shot åŒºåŸŸçš„æ€§èƒ½ã€‚</p><p><img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-7.png alt=image.png>
â€ƒâ€ƒ<strong>Inferring Text Similarity Score: STS-B-DIR.</strong> Table 3 shows the results, where similar observations can be made on STS-B-DIR. Again, both SMOTER and SMOGN perform worse than the vanilla model. In contrast, both LDS and FDS consistently and substantially improve the results for various methods, especially in medium- and few-shot regions. The advantage is even more profound under Pearson correlation, which is commonly used for this NLP task.
â€ƒâ€ƒæ¨æ–­æ–‡æœ¬ç›¸ä¼¼åº¦å¾—åˆ†ï¼šSTS-B-DIRã€‚ è¡¨ 3 å±•ç¤ºäº†ç»“æœï¼Œå…¶ä¸­å¯ä»¥å¯¹ STS-B-DIR è¿›è¡Œç±»ä¼¼çš„è§‚å¯Ÿã€‚ åŒæ ·ï¼ŒSMOTER å’Œ SMOGN çš„è¡¨ç°éƒ½æ¯”vanillaæ¨¡å‹å·®ã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLDS å’Œ FDS éƒ½æ”¹å–„äº†å„ç§æ–¹æ³•çš„ç»“æœï¼Œå°¤å…¶æ˜¯åœ¨ä¸­ç­‰å’Œå°‘æ ·æœ¬åŒºåŸŸã€‚ åœ¨NLP ä»»åŠ¡å¸¸ç”¨çš„Pearsonç›¸å…³æ€§ä¸‹ï¼Œä¼˜åŠ¿æ›´åŠ æ˜¾è‘—ã€‚</p><p><img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-8.png alt=image.png>
â€ƒâ€ƒ<strong>Inferring Depth: NYUD2-DIR.</strong> For NYUD2-DIR, which is a dense regression task, we verify from Table 4 that adding LDS and FDS significantly improves the results. We note that the vanilla model can inevitably overfit to the manyshot regions during training. FDS and LDS help alleviate this effect, and generalize better to all regions, with minor degradation in the many-shot region but significant boosts for other regions.
â€ƒâ€ƒæ¨æ–­æ·±åº¦ï¼šNYUD2-DIRã€‚ å¯¹äºå¯†é›†å›å½’ä»»åŠ¡ NYUD2-DIRï¼Œæˆ‘ä»¬ä»è¡¨ 4 ä¸­éªŒè¯äº†æ·»åŠ  LDS å’Œ FDS å¯ä»¥æ˜¾è‘—æ”¹å–„ç»“æœã€‚ æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œvanilla æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´ä¸å¯é¿å…åœ°ä¼šè¿‡åº¦æ‹Ÿåˆåˆ°manyshotåŒºåŸŸã€‚ FDS å’Œ LDS æœ‰åŠ©äºå‡è½»è¿™ç§å½±å“ï¼Œå¹¶æ›´å¥½åœ°æ³›åŒ–åˆ°æ‰€æœ‰åŒºåŸŸï¼Œåœ¨manyshotåŒºåŸŸæœ‰è½»å¾®çš„é€€åŒ–ï¼Œä½†åœ¨å…¶ä»–åŒºåŸŸæœ‰æ˜¾è‘—æå‡ã€‚</p><p><img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-9.png alt=image.png>
â€ƒâ€ƒ<strong>Inferring Health Score: SHHS-DIR.</strong> Table 5 reports the results on SHHS-DIR. Since SMOTER and SMOGN are not directly applicable to this medical data, we skip them for this dataset. The results again confirm the effectiveness of both FDS and LDS when applied for real-world imbalanced regression tasks, where by combining FDS and LDS we often get the highest gains over all tested regions.
â€ƒâ€ƒæ¨æ–­å¥åº·è¯„åˆ†ï¼šSHHS-DIRã€‚ è¡¨ 5 è¯´æ˜äº† SHHS-DIR çš„ç»“æœã€‚ ç”±äº SMOTER å’Œ SMOGN å¹¶ä¸ç›´æ¥é€‚ç”¨äºè¯¥åŒ»å­¦æ•°æ®ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ­¤æ•°æ®é›†ä¸­è·³è¿‡å®ƒä»¬ã€‚ ç»“æœå†æ¬¡è¯å®äº† FDS å’Œ LDS åœ¨åº”ç”¨äºç°å®ä¸–ç•Œçš„ä¸å¹³è¡¡å›å½’ä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ç»“åˆ FDS å’Œ LDSï¼Œæˆ‘ä»¬é€šå¸¸åœ¨æ‰€æœ‰æµ‹è¯•åŒºåŸŸä¸­è·å¾—æœ€é«˜æ”¶ç›Šã€‚
<img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-10.png alt=image.png><span class=caption>â— Figure 7. The absolute MAE gains of LDS + FDS over the vanilla model, on a curated subset of IMDB-WIKI-DIR with certain target values having no training data. We establish notable performance gains w.r.t. all regions, especially for extrapolation & interpolation.å›¾ 7. LDS + FDS åœ¨vanillaæ¨¡å‹ä¸Šçš„ç»å¯¹ MAE å¢ç›Šï¼Œåœ¨ IMDB-WIKI-DIR çš„ç²¾é€‰å­é›†ä¸Šï¼Œå…¶ä¸­æŸäº›ç›®æ ‡å€¼æ²¡æœ‰è®­ç»ƒæ•°æ®ã€‚ åœ¨æ‰€æœ‰åŒºåŸŸæœ‰ç€æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤–æ’å’Œå†…æ’ã€‚</span></p><figure>Figure 7. The absolute MAE gains of LDS + FDS over the vanilla model, on a curated subset of IMDB-WIKI-DIR with certain target values having no training data. We establish notable performance gains w.r.t. all regions, especially for extrapolation & interpolation.<p>å›¾ 7. LDS + FDS åœ¨vanillaæ¨¡å‹ä¸Šçš„ç»å¯¹ MAE å¢ç›Šï¼Œåœ¨ IMDB-WIKI-DIR çš„ç²¾é€‰å­é›†ä¸Šï¼Œå…¶ä¸­æŸäº›ç›®æ ‡å€¼æ²¡æœ‰è®­ç»ƒæ•°æ®ã€‚ åœ¨æ‰€æœ‰åŒºåŸŸæœ‰ç€æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤–æ’å’Œå†…æ’ã€‚</figure></p><p><a name=f4V3z></a></p><h3 id=42-further-analysis><a href=#42-further-analysis class=anchor-link>#</a><a href=#contents:42-further-analysis class=headings>4.2. Further Analysis</a></h3><p>â€ƒâ€ƒ<strong>Extrapolation & Interpolation.</strong> In real-world DIR tasks, certain target values can have no data at all (e.g., see SHHSDIR and STS-B-DIR in Fig. 6). This motivates the need for target extrapolation and interpolation. We curate a subset from the training set of IMDB-WIKI-DIR, which has no training data in certain regions (Fig. 7), but evaluate on the original testset for zero-shot generalization analysis.
â€ƒâ€ƒå¤–æ’å’Œå†…æ’ã€‚åœ¨ç°å®ä¸–ç•Œçš„DIRä»»åŠ¡ä¸­ï¼ŒæŸäº›ç›®æ ‡å€¼æ²¡æœ‰æ•°æ®ï¼ˆçœ‹å›¾6çš„SHHSDIRå’ŒSRS-B-DIRï¼‰ã€‚è¿™æ¿€å‘å¯¹ç›®æ ‡å¤–æ’å’Œå†…æ’çš„éœ€æ±‚ã€‚æˆ‘ä»¬æŒ‘é€‰äº†IMDB-WIKIè®­ç»ƒé›†ä¸­çš„ä¸€ä¸ªå­é›†ï¼Œè¯¥å­é›†åœ¨æŸäº›åŒºåŸŸæ²¡æœ‰è®­ç»ƒæ•°æ®ï¼ˆå›¾7ï¼‰è¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨åŸå§‹æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¥è¿›è¡Œé›¶æ ·æœ¬æ³›åŒ–åˆ†æã€‚
<img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-11.png alt=image.png>
As Table 6 shows, compared to the vanilla model, LDS and FDS can both improve the results not only on regions that have data, but also achieve larger gains on those without data. Specifically, substantial improvements are established for both target interpolation and extrapolation, where interpolation enjoys larger boosts.
å¦‚è¡¨6æ‰€ç¤ºï¼Œè·Ÿvanillaæ¨¡å‹æ¯”è¾ƒï¼ŒLDSå’ŒFDSä¸ä»…åœ¨æœ‰æ•°æ®çš„åŒºåŸŸæé«˜æ€§èƒ½è¿˜èƒ½åœ¨æ²¡æœ‰æ•°æ®çš„åŒºåŸŸå–å¾—æ›´å¤§çš„æ”¶ç›Šã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ç›®æ ‡å†…æ’å’Œå¤–æ’éƒ½æœ‰å®è´¨ä¸Šçš„æå‡ï¼Œå°¤å…¶åœ¨å†…æ’ä¸­ã€‚
We further visualize the absolute MAE gains of our method over vanilla model in Fig. 7. Our method provides a comprehensive treatment to the many, medium, few, as well as zero-shot regions, achieving remarkable performance gains.
æˆ‘ä»¬è¿›ä¸€æ­¥å¯è§†åŒ–æˆ‘ä»¬æ–¹æ³•çš„ç»å¯¹MAEæ”¶ç›Šåœ¨å›¾7ä¸­çš„vanillaæ¨¡å‹ä¸Šã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯¹å¤šã€ä¸­ï¼Œå°‘ï¼Œé›¶åŒºåŸŸæä¾›å…¨é¢ç»¼åˆçš„å¤„ç†ï¼Œå®ç°äº†å·¨å¤§çš„æ”¶ç›Šã€‚
<img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-12.png alt=image.png><span class=caption>â— (a) Feature statistics similarity for age 0, without FDS</span>
<img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-13.png alt=image.png><span class=caption>â— (b) Feature statistics similarity for age 0, with FDS</span>
<img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-14.png alt=image.png><span class=caption>â— (c) Statistics change</span></p><blockquote><p>â€ƒâ€ƒFigure 8. Analysis on how FDS works.(a) & (b)Feature statistics similarity for anchor age 0, using model trained without and with FDS. (c) L1 distance between the running statistics$\left{\boldsymbol{\mu}<em>{b}, \boldsymbol{\Sigma}</em>{b}\right}$and the smoothed statistics$\left{\tilde{\boldsymbol{\mu}}<em>{b}, \widetilde{\boldsymbol{\Sigma}}</em>{b}\right}$during training.<br>å›¾8ï¼šå¯¹FDSå¦‚ä½•æœ‰æ•ˆçš„åˆ†æã€‚(a)å’Œ(b)anchor age 0çš„ç‰¹å¾ç»Ÿè®¡ç›¸ä¼¼æ€§ï¼Œä½¿ç”¨å¸¦FDSå’Œä¸å¸¦FDSçš„è®­ç»ƒæ¨¡å‹ã€‚(c)è¿è¡Œç»Ÿè®¡æ•°æ®$\left{\boldsymbol{\mu}<em>{b}ã€\boldsymbol{\Sigma}</em>{b}\right}$ä¸å¹³æ»‘ç»Ÿè®¡æ•°æ®$\left{\tilde{\boldsymbol{\mu}}<em>{b}, \widetilde{\boldsymbol{\Sigma}}</em>{b}\right}$åœ¨è®­ç»ƒæœŸé—´çš„L1è·ç¦»ã€‚</p></blockquote><p>â€ƒâ€ƒ<strong>Understanding FDS.</strong> We investigate how FDS influences the feature statistics. In Fig. 8(a) and 8(b) we plot the similarity of the feature statistics for anchor age 0, using model trained without and with FDS. As the figure indicates, since age 0 lies in the few-shot region, the feature statistics can have a large bias, i.e., age 0 shares large similarity with region 40 âˆ¼ 80 as in Fig. 8(a). In contrast, when FDS is added, the statistics are better calibrated, resulting in a high similarity only in its neighborhood, and a gradually decreasing similarity score as target value becomes larger. We further visualize the L1 distance between the running statistics$\left{\boldsymbol{\mu}<em>{b}, \boldsymbol{\Sigma}</em>{b}\right}$and the smoothed statistics$\left{\tilde{\boldsymbol{\mu}}<em>{b}, \widetilde{\boldsymbol{\Sigma}}</em>{b}\right}$during training in Fig. 8(c). Interestingly, the average$L_1$distance becomes smaller and gradually diminishes as the training evolves, indicating that the model learns to generate features that are more accurate even without smoothing, and finally the smoothing module can be removed during inference. We provide more results for different anchor ages in Appendix E.7, where similar effects can be observed.
â€ƒâ€ƒç†è§£FDSã€‚æˆ‘ä»¬ç ”ç©¶äº†FDSå¦‚ä½•å½±å“ç‰¹å¾ç»Ÿè®¡å€¼ã€‚åœ¨å›¾8(a,b)ä¸­ï¼Œç”»å‡ºäº†ä½¿ç”¨å’Œæ²¡æœ‰ä½¿ç”¨FDSè®­ç»ƒåçš„æ¨¡å‹ä¸­anchor age 0çš„ç‰¹å¾ç»Ÿè®¡æ•°æ®ï¼ˆmeanã€varianceï¼‰çš„ç›¸ä¼¼æ€§ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œç”±äº0å²åœ¨å°‘æ ·æœ¬åŒºé—´ï¼Œå®ƒçš„ç‰¹å¾ç»Ÿè®¡å€¼ç”±å¾ˆå¤§çš„åå·®ï¼Œå³å›¾8(a)çœ‹å‡º0å²ä¸40-80å²çš„åŒºåŸŸæœ‰å¾ˆå¤§çš„ç›¸ä¼¼æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå½“åŠ å…¥FDSåï¼Œç»Ÿè®¡æ•°æ®è¢«å¾ˆå¥½çš„æ ¡å‡†äº†ï¼Œä½¿å¾—ä»…åœ¨å…¶å‘¨å›´å…·æœ‰é«˜ç›¸ä¼¼åº¦ï¼Œå¹¶ä¸”éšç€ç›®æ ‡å€¼å˜å¤§ï¼Œç›¸ä¼¼åº¦éšä¹‹ä¸‹é™ã€‚å›¾8(c)å¯è§†åŒ–äº†è¿è¡Œç»Ÿè®¡æ•°æ®å’Œå¹³æ»‘ç»Ÿè®¡æ•°æ®é—´çš„L1è·ç¦»ã€‚æœ‰è¶£çš„æ˜¯ï¼Œéšç€è®­ç»ƒè¿›è¡Œï¼ŒL1è·ç¦»å˜å¾—æ›´å°å¹¶é€æ¸å¹³ç¨³ä¸‹é™ï¼Œè¿™è¡¨æ˜äº†æ¨¡å‹å³ä½¿åœ¨æ²¡æœ‰å¹³æ»‘çš„æƒ…å†µä¸‹å­¦ä¼šäº†ç”Ÿæˆæ›´å‡†ç¡®çš„ç‰¹å¾ï¼Œæœ€ç»ˆå¹³æ»‘æ¨¡å—å¯ä»¥åœ¨æ¨ç†é˜¶æ®µç§»é™¤ã€‚æˆ‘ä»¬æä¾›äº†æ›´å¤šçš„ç»“æœåœ¨é™„å½•E.7ï¼Œå¯ä»¥çœ‹åˆ°ç›¸ä¼¼çš„æ•ˆæœã€‚</p><p>â€ƒâ€ƒ<strong>Ablation: Kernel type for LDS & FDS (Appendix <strong><a href=#ADrwB><strong>E.1</strong></a></strong>).</strong> We study the effects of different kernel types for LDS and FDS when applying distribution smoothing. We select three different kernel types, i.e., Gaussian, Laplacian, and Triangular kernel, and evaluate their influences on both LDS and FDS. In general, all kernel types lead to notable gains (e.g., 3.7% âˆ¼ 6.2% relative MSE gains on STS-B-DIR), with the Gaussian kernel often delivering the best results.
â€ƒâ€ƒæ¶ˆèå®éªŒï¼šLDSå’ŒFDSæ ¸ç±»å‹(é™„å½•E.1)ã€‚æˆ‘ä»¬ç ”ç©¶äº†åº”ç”¨åˆ†å¸ƒå¹³æ»‘æ—¶ä¸åŒæ ¸ç±»å‹å¯¹LDSå’ŒFDSçš„æ•ˆæœã€‚æˆ‘ä»¬é€‰æ‹©äº†ä¸‰ä¸ªä¸åŒæ ¸ç±»å‹ï¼Œå³Gaussianï¼ŒLaplicaianå’ŒTriangularï¼Œå¹¶ä¸”åœ¨LDSå’ŒFDSä¸Šè¯„ä¼°å®ƒä»¬çš„æ•ˆæœã€‚é€šå¸¸æ‰€æœ‰æ ¸ç±»å‹éƒ½ä¼šå¸¦æ¥æ˜¾è‘—æ•ˆæœï¼ˆå¦‚ï¼Œåœ¨STS-B-DIRä¸Šçš„ç›¸å¯¹MSEæ”¶ç›Šæ˜¯3.7%-6.2%ï¼‰ï¼Œå…¶ä¸­Gaussianæ ¸å¾€å¾€æ˜¯æœ€ä¼˜ç»“æœã€‚</p><p>â€ƒâ€ƒ<strong>Ablation: Different regression loss functions (Appendix E.2).</strong> We investigate the influence of different training loss functions on LDS and FDS. We select three common losses used for regression tasks, i.e., $L_1$ loss, MSE loss, and the Huber loss (also referred to as smoothed $L_1$loss). We find that similar results are obtained for all losses, indicating that both LDS and FDS are robust to different loss functions.
â€ƒâ€ƒæ¶ˆèå®éªŒï¼šä¸åŒçš„å›å½’æŸå¤±å‡½æ•°(é™„å½•E.2)ã€‚æˆ‘ä»¬ç ”ç©¶ äº†ä¸åŒè®­ç»ƒæ—¶çš„æŸå¤±å‡½æ•°å¯¹LDSå’ŒFDSçš„å½±å“ã€‚æˆ‘ä»¬é€‰æ‹©ä¸‰ä¸ªå¸¸è§çš„ç”¨äºå›å½’ä»»åŠ¡çš„æŸå¤±å‡½æ•°ï¼Œå³L1 lossï¼ŒMSE losså’ŒHuber lossï¼ˆä¹Ÿç§°ä¸ºå¹³æ»‘L1 lossï¼‰ã€‚æˆ‘ä»¬å‘ç°å¯¹äºæ‰€æœ‰æŸå¤±å‡½æ•°éƒ½è·å¾—äº†ç›¸ä¼¼çš„ç»“æœï¼Œè¿™è¡¨æ˜ LDS å’Œ FDS å¯¹ä¸åŒçš„æŸå¤±å‡½æ•°éƒ½å…·æœ‰é²æ£’æ€§ã€‚</p><p>â€ƒâ€ƒ<strong>Ablation: Hyper-parameter for LDS & FDS (Appendix E.3).</strong> We investigate the effects of hyper-parameters on both LDS and FDS. As we mainly employ the Gaussian kernel for distribution smoothing, we extensively study different choices of the kernel size $l$ and standard deviation $Ïƒ$. Interestingly, we find LDS and FDS are surprisingly robust to different hyper-parameters in a given range, and obtain similar gains. For example, on STS-B-DIR with$l âˆˆ \left { 5,9,15 \right }$and$Ïƒ âˆˆ \left { 1,2,3 \right }$, overall MSE gains range from 3.3% to 6.2%, with$l = 5$and$Ïƒ = 2$exhibiting the best results.
â€ƒâ€ƒæ¶ˆèå®éªŒï¼šLDSå’ŒFDSçš„è¶…å‚æ•°(é™„å½•E.3)ã€‚æˆ‘ä»¬ç ”ç©¶äº†è¶…å‚æ•°å¯¹ LDS å’Œ FDS çš„å½±å“ã€‚ ç”±äºæˆ‘ä»¬ä¸»è¦ä½¿ç”¨é«˜æ–¯æ ¸è¿›è¡Œåˆ†å¸ƒå¹³æ»‘ï¼Œæˆ‘ä»¬å¹¿æ³›ç ”ç©¶äº†æ ¸å¤§å° l å’Œæ ‡å‡†å·® Ïƒ çš„ä¸åŒé€‰æ‹©ã€‚ æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç° LDS å’Œ FDS å¯¹ç»™å®šèŒƒå›´å†…çš„ä¸åŒè¶…å‚æ•°å…·æœ‰æƒŠäººçš„é²æ£’æ€§ï¼Œå¹¶ä¸”è·å¾—äº†ç›¸ä¼¼çš„å¢ç›Šã€‚ ä¾‹å¦‚ï¼Œåœ¨ $l âˆˆ \left { 5,9,15 \right }$and$Ïƒ âˆˆ \left { 1,2,3 \right }$ çš„ STS-B-DIR ä¸Šï¼Œæ€»ä½“ MSE å¢ç›ŠèŒƒå›´ ä» 3.3% åˆ° 6.2%ï¼Œå…¶ä¸­ $l = 5$ å’Œ $Ïƒ = 2$ è¡¨ç°å‡ºæœ€å¥½çš„ç»“æœã€‚</p><p>â€ƒâ€ƒ<strong>Ablation: Robustness to diverse skewed label densities (Appendix E.4).</strong> We curate different imbalanced distributions for IMDB-WIKI-DIR by combining different number of disjoint skewed Gaussian distributions over the target space, with potential missing data in certain target regions, and evaluate the robustness of FDS and LDS to the distribution change. We verify that even under different imbalanced label distributions, LDS and FDS consistently boost the performance across all regions compared to the vanilla model, with relative MAE gains ranging from 8.8% to 12.4%.
â€ƒâ€ƒæ¶ˆèå®éªŒï¼šå¯¹ä¸åŒskewedæ ‡ç­¾å¯†åº¦çš„é²æ£’æ€§(é™„å½•E.4)ã€‚æˆ‘ä»¬é€šè¿‡å°†ç›®æ ‡ç©ºé—´ä¸Šä¸åŒæ•°é‡çš„ä¸ç›¸äº¤skewedé«˜æ–¯åˆ†å¸ƒä¸æŸäº›ç›®æ ‡åŒºåŸŸä¸­çš„æ½œåœ¨ç¼ºå¤±æ•°æ®ç›¸ç»“åˆï¼Œä¸º IMDB-WIKI-DIR ç®¡ç†ä¸åŒçš„ä¸å¹³è¡¡åˆ†å¸ƒï¼Œå¹¶è¯„ä¼° FDS å’Œ LDS å¯¹åˆ†å¸ƒå˜åŒ–çš„é²æ£’æ€§ã€‚ æˆ‘ä»¬éªŒè¯äº†å³ä½¿åœ¨ä¸åŒçš„ä¸å¹³è¡¡æ ‡ç­¾åˆ†å¸ƒä¸‹ï¼Œä¸æ™®é€šæ¨¡å‹ç›¸æ¯”ï¼ŒLDS å’Œ FDS ä¹Ÿèƒ½æŒç»­æå‡æ‰€æœ‰åŒºåŸŸçš„æ€§èƒ½ï¼Œç›¸å¯¹ MAE å¢ç›Šä» 8.8% åˆ° 12.4% ä¸ç­‰ã€‚</p><p>â€ƒâ€ƒ<strong>Comparisons to imbalanced classification methods (Appendix E.6).</strong> Finally, to gain more insights on the intrinsic difference between imbalanced classification & imbalanced regression problems, we directly apply existing imbalanced classification schemes on several appropriate DIR datasets, and show empirical comparisons with imbalanced regression approaches. We demonstrate in Appendix E.6 that LDS and FDS outperform imbalanced classification schemes by a large margin, where the errors for few-shot regions can be reduced by up to 50% to 60%. Interestingly, the results also show that imbalanced classification schemes often perform worse than even the vanilla regression model, which confirms that regression requires different approaches for data imbalance than simply applying classification methods. We note that imbalanced classification methods could fail on regression problems for several reasons. First, they ignore the similarity between data samples that are close w.r.t. the continuous target. Moreover, classification cannot extrapolate or interpolate in the continuous label space, therefore unable to deal with missing data in certain target regions.
â€ƒâ€ƒä¸ä¸å¹³è¡¡åˆ†ç±»æ–¹æ³•çš„æ¯”è¾ƒ(é™„å½•E.5)ã€‚æœ€åï¼Œä¸ºäº†æ›´æ·±å…¥åœ°äº†è§£ä¸å¹³è¡¡åˆ†ç±»å’Œä¸å¹³è¡¡å›å½’é—®é¢˜ä¹‹é—´çš„å†…åœ¨å·®å¼‚ï¼Œæˆ‘ä»¬ç›´æ¥åœ¨å‡ ä¸ªé€‚å½“çš„ DIR æ•°æ®é›†ä¸Šåº”ç”¨ç°æœ‰çš„ä¸å¹³è¡¡åˆ†ç±»æ–¹æ¡ˆï¼Œå¹¶å±•ç¤ºä¸ä¸å¹³è¡¡å›å½’æ–¹æ³•çš„empirical æ¯”è¾ƒã€‚æˆ‘ä»¬åœ¨é™„å½• E.6 ä¸­è¯æ˜ï¼ŒLDS å’Œ FDS å¤§å¤§ä¼˜äºé‚£äº›ä¸å¹³è¡¡åˆ†ç±»æ–¹æ¡ˆï¼Œå…¶ä¸­å°‘æ ·æœ¬åŒºåŸŸçš„é”™è¯¯å¯ä»¥å‡å°‘å¤šè¾¾ 50% åˆ° 60%ã€‚æœ‰è¶£çš„æ˜¯ï¼Œç»“æœè¿˜è¡¨æ˜ï¼Œä¸å¹³è¡¡çš„åˆ†ç±»æ–¹æ¡ˆé€šå¸¸æ¯”æ™®é€šå›å½’æ¨¡å‹è¡¨ç°æ›´å·®ï¼Œè¿™è¯å®å›å½’éœ€è¦ä¸åŒçš„æ–¹æ³•æ¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œè€Œä¸æ˜¯ç®€å•åœ°åº”ç”¨åˆ†ç±»æ–¹æ³•ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œä¸å¹³è¡¡çš„åˆ†ç±»æ–¹æ¡ˆå¯èƒ½ä¼šå› ä¸ºå‡ ä¸ªåŸå› è€Œåœ¨å›å½’é—®é¢˜ä¸Šå¤±è´¥ã€‚é¦–å…ˆï¼Œä»–ä»¬å¿½ç•¥äº†ä¸è¿ç»­ç›®æ ‡æ¥è¿‘çš„æ•°æ®æ ·æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œåˆ†ç±»æ— æ³•åœ¨è¿ç»­æ ‡ç­¾ç©ºé—´ä¸­è¿›è¡Œå¤–æ¨æˆ–å†…æ’ï¼Œå› æ­¤æ— æ³•å¤„ç†æŸäº›ç›®æ ‡åŒºåŸŸä¸­çš„ç¼ºå¤±æ•°æ®ã€‚
<a name=LsUbt></a></p><h2 id=5-conclusion><a href=#5-conclusion class=anchor-link>#</a><a href=#contents:5-conclusion class=headings>5. Conclusion</a></h2><p>â€ƒâ€ƒWe introduce the DIR task that learns from natural imbalanced data with continuous targets, and generalizes to the entire target range. We propose two simple and effective algorithms for DIR that exploit the similarity between nearby targets in both label and feature spaces. Extensive results on five curated large-scale real-world DIR benchmarks confirm the superior performance of our methods. Our work fills the gap in benchmarks and techniques for practical DIR tasks.<br>â€ƒâ€ƒæˆ‘ä»¬å¼•å…¥äº† DIR ä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡ä»å…·æœ‰è¿ç»­ç›®æ ‡çš„è‡ªç„¶ä¸å¹³è¡¡æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶æ¨å¹¿åˆ°æ•´ä¸ªç›®æ ‡èŒƒå›´ã€‚ æˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç®€å•æœ‰æ•ˆçš„ DIR ç®—æ³•ï¼Œåˆ©ç”¨æ ‡ç­¾å’Œç‰¹å¾ç©ºé—´ä¸­ä¸´è¿‘ç›®æ ‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ äº”ä¸ªæ•´ç†è¿‡çš„å¤§è§„æ¨¡çœŸå®ä¸–ç•Œ DIR åŸºå‡†æµ‹è¯•çš„å¹¿æ³›ç»“æœè¯å®äº†æˆ‘ä»¬æ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚ æˆ‘ä»¬çš„å·¥ä½œå¡«è¡¥äº†å®é™… Deep Imbalanced Regression (DIR) ä»»åŠ¡çš„åŸºå‡†å’ŒæŠ€æœ¯æ–¹é¢çš„ç©ºç™½ã€‚</p><hr><p><a name=GAKZb></a></p><h2 id=supplementary-material><a href=#supplementary-material class=anchor-link>#</a><a href=#contents:supplementary-material class=headings>Supplementary Material</a></h2><p><a name=Nun91></a></p><h3 id=a-pseudo-code-for-lds--fds><a href=#a-pseudo-code-for-lds--fds class=anchor-link>#</a><a href=#contents:a-pseudo-code-for-lds--fds class=headings>A. Pseudo Code for LDS & FDS</a></h3><p><img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-15.png alt=image.png>
<a name=cchrV></a></p><h3 id=b-details-of-dir-datasets><a href=#b-details-of-dir-datasets class=anchor-link>#</a><a href=#contents:b-details-of-dir-datasets class=headings>B. Details of DIR Datasets</a></h3><p>In this section, we provide the detailed information of the five curated DIR datasets we used in our experiments. Table 7 provides an overview of the five datasets.<br><img src=https://halo-1310118673.cos.ap-singapore.myqcloud.com/halo/blog/2022/08/20220816201758-16.png alt=image.png>
<a name=ESOiR></a></p><h3 id=c-experimental-settings><a href=#c-experimental-settings class=anchor-link>#</a><a href=#contents:c-experimental-settings class=headings>C. Experimental Settings</a></h3><p><a name=tQC4e></a></p><h3 id=d-additional-results><a href=#d-additional-results class=anchor-link>#</a><a href=#contents:d-additional-results class=headings>D. Additional Results</a></h3><p>We provide complete evaluation results on the five DIR datasets, where more baselines and evaluation metrics are included in addition to the reported results in the main paper.<br><a name=E4UKC></a></p><h3 id=e-further-analysis-and-ablation-studies><a href=#e-further-analysis-and-ablation-studies class=anchor-link>#</a><a href=#contents:e-further-analysis-and-ablation-studies class=headings>E. Further Analysis and Ablation Studies</a></h3><p><a name=ADrwB></a></p><h4 id=e1-kernel-type-for-lds--fds><a href=#e1-kernel-type-for-lds--fds class=anchor-link>#</a><a href=#contents:e1-kernel-type-for-lds--fds class=headings>E.1. Kernel Type for LDS & FDS</a></h4><p><a name=xato0></a></p><h4 id=e2-training-loss-for-lds--fds><a href=#e2-training-loss-for-lds--fds class=anchor-link>#</a><a href=#contents:e2-training-loss-for-lds--fds class=headings>E.2. Training Loss for LDS & FDS</a></h4><p><a name=d5MnK></a></p><h4 id=e3-hyper-parameters-for-lds--fds><a href=#e3-hyper-parameters-for-lds--fds class=anchor-link>#</a><a href=#contents:e3-hyper-parameters-for-lds--fds class=headings>E.3. Hyper-parameters for LDS & FDS</a></h4><p>ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚å¤ªå¤šäº† è‡ªè¡ŒæŸ¥é˜…å§</p></div></article><footer class=minimal-footer><div class=post-tag><a href=/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/ rel=tag class=post-tag-link>#äººå·¥æ™ºèƒ½</a> <a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ rel=tag class=post-tag-link>#æ·±åº¦å­¦ä¹ </a></div><div class=post-category><a href=/posts/ class="post-category-link active">æ–‡ç« </a> |</div></footer></div></main><div id=back-to-top class=back-to-top><a href=#><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6.0-33.9L207 39c9.4-9.4 24.6-9.4 33.9.0l194.3 194.3c9.4 9.4 9.4 24.6.0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3.0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a></div></div><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js")})</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css><script>if(typeof renderMathInElement=="undefined"){const e=e=>{const t=document.createElement("script");t.defer=!0,t.crossOrigin="anonymous",Object.keys(e).forEach(n=>{t[n]=e[n]}),document.body.appendChild(t)};e({src:"https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js",onload:()=>{e({src:"https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/mhchem.min.js",onload:()=>{e({src:"https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js",onload:()=>{renderKaTex()}})}})}})}else renderKaTex();function renderKaTex(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})}</script><script src=https://cdn.jsdelivr.net/npm/medium-zoom@latest/dist/medium-zoom.min.js></script>
<script>let imgNodes=document.querySelectorAll("div.post-body img");imgNodes=Array.from(imgNodes).filter(e=>e.parentNode.tagName!=="A"),mediumZoom(imgNodes,{background:"hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)"})</script></body></html>